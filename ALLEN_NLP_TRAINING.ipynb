{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ALLEN NLP TRAINING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "478986a2f793407f9e7486f9d964bcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4edf7f4a913d4b5b9ccad0b2992f837a",
              "IPY_MODEL_0eb9c572380a4975ab4657f875dd63c7",
              "IPY_MODEL_3b3d9d11c8914f5f82aebe378d03bc4c"
            ],
            "layout": "IPY_MODEL_6ea636c1dc7149ddb0914893754abff0"
          }
        },
        "4edf7f4a913d4b5b9ccad0b2992f837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e97d90b55444eba726c8cf8930e15c",
            "placeholder": "​",
            "style": "IPY_MODEL_8cce9f9bb66d4cdc947b757d7ed28593",
            "value": "Downloading: 100%"
          }
        },
        "0eb9c572380a4975ab4657f875dd63c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6fd1a84899c4cf8b27e5beee8efd6f5",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4a5268b6a054e55aafcab5332cc3686",
            "value": 231508
          }
        },
        "3b3d9d11c8914f5f82aebe378d03bc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_256ec5e4b7374fc29b2e2340c5275f77",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7ed01a44974ad3ac0faf68e539d4da",
            "value": " 232k/232k [00:00&lt;00:00, 206kB/s]"
          }
        },
        "6ea636c1dc7149ddb0914893754abff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27e97d90b55444eba726c8cf8930e15c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cce9f9bb66d4cdc947b757d7ed28593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6fd1a84899c4cf8b27e5beee8efd6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a5268b6a054e55aafcab5332cc3686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "256ec5e4b7374fc29b2e2340c5275f77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7ed01a44974ad3ac0faf68e539d4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9392e016ddb54941a5c2f1395e4da688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f65a7d0d411945f98471604d938303a3",
              "IPY_MODEL_5d28c66da01b4d5f8a4972306e65e9d2",
              "IPY_MODEL_d9e2f98ed952487c9a7f42422bef471e"
            ],
            "layout": "IPY_MODEL_30b3f7e99a144420b6e74e150b0bdda8"
          }
        },
        "f65a7d0d411945f98471604d938303a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4c558ef28b04b0489d1f5f26e84806d",
            "placeholder": "​",
            "style": "IPY_MODEL_e5f1fd12642543d0af133455182d156b",
            "value": "Downloading: 100%"
          }
        },
        "5d28c66da01b4d5f8a4972306e65e9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3473b35e71944917b266ce6b8e28f678",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4bf074cb42c4ccb8cb66d0290ab13e7",
            "value": 28
          }
        },
        "d9e2f98ed952487c9a7f42422bef471e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c884bfd71ecc4229be3f124a9ed636a8",
            "placeholder": "​",
            "style": "IPY_MODEL_19d3ab9089b844d39944ec440e4e7cc9",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.13kB/s]"
          }
        },
        "30b3f7e99a144420b6e74e150b0bdda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c558ef28b04b0489d1f5f26e84806d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f1fd12642543d0af133455182d156b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3473b35e71944917b266ce6b8e28f678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4bf074cb42c4ccb8cb66d0290ab13e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c884bfd71ecc4229be3f124a9ed636a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d3ab9089b844d39944ec440e4e7cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bade0270d7642288e6021d508edcbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_803afacd2c4d40c3894a900e52b216a2",
              "IPY_MODEL_2deb768979be4391af174b041d622d21",
              "IPY_MODEL_9a5cfcce65ba4fc4be389f055f1d51f8"
            ],
            "layout": "IPY_MODEL_13b7c7e2cb694609b541b2cadfe983b8"
          }
        },
        "803afacd2c4d40c3894a900e52b216a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed46b7773da4f37862ab1308ae4708a",
            "placeholder": "​",
            "style": "IPY_MODEL_b08be842791244d19285ddf8805d967b",
            "value": "Downloading: 100%"
          }
        },
        "2deb768979be4391af174b041d622d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f26b7f297a4af291ed155b8fe94392",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_addd5f33fa094cfa973bfeba9fdaf78e",
            "value": 466062
          }
        },
        "9a5cfcce65ba4fc4be389f055f1d51f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1aa1d4949c14fffa234cd6ce3e72bff",
            "placeholder": "​",
            "style": "IPY_MODEL_832e1f25c2c64827bbc01a5fdd441e27",
            "value": " 466k/466k [00:00&lt;00:00, 408kB/s]"
          }
        },
        "13b7c7e2cb694609b541b2cadfe983b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bed46b7773da4f37862ab1308ae4708a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08be842791244d19285ddf8805d967b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f26b7f297a4af291ed155b8fe94392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "addd5f33fa094cfa973bfeba9fdaf78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1aa1d4949c14fffa234cd6ce3e72bff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832e1f25c2c64827bbc01a5fdd441e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqXG6NPrUbW_"
      },
      "source": [
        "example code to use the allennlp library to make predictions: https://github.com/NeuroSYS-pl/coreference-resolution/blob/main/allennlp_coreference_resolution.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rM27FVAGg1Xz",
        "outputId": "a13ca0b1-1225-48b8-cb01-d72f30be26f9"
      },
      "source": [
        "!pip install torch\n",
        "!pip install allennlp\n",
        "!pip install allennlp_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu111)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu111)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 73.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 72.2 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.4-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.18.58-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 78.3 MB/s \n",
            "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 93.7 MB/s \n",
            "\u001b[?25hCollecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.10.0)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 75.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 82.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Collecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.58\n",
            "  Downloading botocore-1.21.58-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.58->boto3<2.0,>=1.14->allennlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 84.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.17.3)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.1.0-py2.py3-none-any.whl (27 kB)\n",
            "Collecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.4)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.13)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 87.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 86.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.4 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 58.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 80.8 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 76.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 76.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 72.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 78.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 82.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 74.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 72.0 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 85.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 82.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 85.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.0 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 76.7 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 67.5 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.0.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.5.1-py3-none-any.whl (8.1 kB)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=d3f3421b05fd08e7e893a8c43ee1860c4ac8832d5a4bbbcdbed66dc0aab6cf4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=685cb801313b086c0f7a068efa368c1b18f3df2bcda08f24a0f94674dabbbd01\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=8b6df2d895c0dd78883655bafc4454043ad4015eb94cd052da809bb334bd2b87\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388706 sha256=7662d70800e435b5b40288ffc8dfe81e0e09692ff09b9b0678389f28d6330df2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=b1536d6cea723d37d5a3d20e6bf314ed31f4ae38ae5f27a8596c380985628c7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=b8a0ebc5a9f4c429469187bb34220882ea182cfc3615094d57811bbb3c7a4c6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=627641815c8d0c3dc4e56e862e5998b8ad2927362c11368aaf192f9cb6688585\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=c22508613cc3b225e653555225a1b1515db27a32d47cfeec96de19302e6a79aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=7da0e4566d82df35c033b4054fdc666a8057c91a15f628ebc47dfb64870ff38d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=c4d1b2aaa4db2362bc779dad14074f3138910639a2297b2f4753ebcaa7238bd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=453ede2f83f3a7067dddc4fb6215e6c5bc8f33db237e8a5c906d0bf4c54a30e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: jaraco.functools, urllib3, tempora, multidict, jaraco.text, jaraco.classes, zc.lockfile, yarl, smmap, sgmllib3k, portend, jmespath, jaraco.collections, cryptography, cheroot, async-timeout, tokenizers, sacremoses, python-docx, pdfminer.six, google-crc32c, google-api-core, gitdb, fsspec, filelock, feedparser, cherrypy, botocore, backports.csv, aiohttp, yaspin, xxhash, transformers, subprocess32, shortuuid, sentry-sdk, s3transfer, patternfork-nosql, pathtools, munch, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, configparser, wandb, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.3.0\n",
            "    Uninstalling filelock-3.3.0:\n",
            "      Successfully uninstalled filelock-3.3.0\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.1.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 allennlp-2.7.0 async-timeout-3.0.1 backports.csv-1.0.7 base58-2.1.0 boto3-1.18.58 botocore-1.21.58 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 cryptography-35.0.0 datasets-1.12.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 filelock-3.0.12 fsspec-2021.10.0 gitdb-4.0.7 google-api-core-2.1.0 google-cloud-core-2.1.0 google-cloud-storage-1.42.3 google-crc32c-1.3.0 google-resumable-media-2.0.3 huggingface-hub-0.0.19 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.3.0 jaraco.text-3.5.1 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.2.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-3.0.0 python-docx-0.8.11 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.3 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.2 tensorboardX-2.4 tokenizers-0.10.3 transformers-4.5.1 urllib3-1.25.11 wandb-0.12.4 xxhash-2.0.2 yarl-1.7.0 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp_models\n",
            "  Downloading allennlp_models-2.7.0-py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: allennlp<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (2.7.0)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (1.9.0+cu111)\n",
            "Collecting conllu==4.4.1\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp_models) (3.2.5)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (3.0.12)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.12.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (3.1.0)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.12.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.1.96)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.17.0)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.7.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (4.62.3)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.4.0)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (3.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.0.19)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (8.10.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.18.58)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (3.6.4)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (4.5.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.99)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (2.2.4)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (2.4)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.10.0+cu111)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.0.11)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (2.1.0)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.42.3)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.4.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp_models) (1.19.5)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (7.6.5)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.4.5)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.6)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.5.0)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.58 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp_models) (1.21.58)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp_models) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp_models) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.58->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp_models) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.58->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp_models) (2.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (4.8.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (2021.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (3.7.4.post0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (21.0)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.15.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.0.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.1.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.35.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (4.2.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp_models) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp_models) (3.7.4.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.10.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.1.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.1.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.2)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.3.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.8.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp_models) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp_models) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp_models) (3.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp_models) (7.1.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp_models) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp_models) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp_models) (0.10.3)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.1.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (2.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.4.3)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (5.0.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (3.1.24)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (0.1.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (3.5.4)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp_models) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (3.0.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp_models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp_models) (2018.9)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.6.3)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (18.6.1)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.7)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (6.0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.16.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (0.8.11)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (20201018)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.0)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.4.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.0.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (8.5.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (4.1.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (3.5.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.4.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (35.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (2.20)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp_models) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp_models) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp_models) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp_models) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp_models) (1.0.1)\n",
            "Building wheels for collected packages: word2number, ftfy\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=48482446a31eee93a6fd76353a10318d181fa646935773c91232a1d9b87df3d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=b826bd1387bcd45e17c9f1c6835690f8e3f896493b5a481f1dc63e23bc3730f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built word2number ftfy\n",
            "Installing collected packages: word2number, py-rouge, ftfy, conllu, allennlp-models\n",
            "Successfully installed allennlp-models-2.7.0 conllu-4.4.1 ftfy-6.0.3 py-rouge-1.1 word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIIGCrW7sxRS"
      },
      "source": [
        "# **Using the CLI to train model.** \n",
        "\n",
        "### **1. Installing libraries and mounting drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uonn87w-tsAU",
        "outputId": "c6de46e4-54b7-4f78-b8c6-2d48e7d49e43"
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install allennlp-models\n",
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.18.49-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Collecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu102)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 56.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu102)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 51.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.9.0)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 38.0 MB/s \n",
            "\u001b[?25hCollecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.2)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 55.2 MB/s \n",
            "\u001b[?25hCollecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 28.3 MB/s \n",
            "\u001b[?25hCollecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 52.8 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.49\n",
            "  Downloading botocore-1.21.49-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.49->boto3<2.0,>=1.14->allennlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.15.0)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.0.0-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 435 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.2)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 37.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 32.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 50.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 44.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 47.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 57.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 36.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 39.6 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.2-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.13)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 48.5 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-2.7.1-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.1-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.5.1-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.4.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=fe3fa409dc257e828826e7d0f2842767afceaacded1a36a06d2e943c2a635fae\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=e99189c31327370b1ec15076cf22c424659a337d437d54d30577f380d148b6ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=e8417c3432daff397842177cc0b30b67b8d6107130afdc47757b67177aefe5d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388683 sha256=394147a697875f64be8e84713a0a44e8d274c35e6482962ccfe85e6cfa6d0042\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=a7d7bd1bd993eb94a57413374521ef95705e295d8b62a533b5f2ed8251021630\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=cf61c351ecb15443810af7ed16ac665609a56933364c999da30288210d4b4fea\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b151d5ca1bc87b4904e815e607152108f5313866490edcce2d16e73c9cea76c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=6aad74e586a0562253127551aada18f1250cba9235d2624897557b671ab79dea\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=79bd70d1e664163715ef21f5df422846e37cc4987d50e17bc32ffe009989b88d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=04d75c236997d4fe5c100e550c3ad12d945d69384246eb713af8ed0c5658ff3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=0b390c5ca5432c3763ab123fb6ebb60f790d0b7b0283bb9d0660cc2a3ff16d52\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: jaraco.functools, urllib3, tempora, multidict, jaraco.text, jaraco.classes, zc.lockfile, yarl, smmap, sgmllib3k, portend, jmespath, jaraco.collections, cryptography, cheroot, async-timeout, tokenizers, sacremoses, python-docx, pdfminer.six, google-crc32c, google-api-core, gitdb, fsspec, feedparser, cherrypy, botocore, backports.csv, aiohttp, yaspin, xxhash, transformers, subprocess32, shortuuid, sentry-sdk, s3transfer, patternfork-nosql, pathtools, munch, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, configparser, wandb, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 allennlp-2.7.0 async-timeout-3.0.1 backports.csv-1.0.7 base58-2.1.0 boto3-1.18.49 botocore-1.21.49 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 cryptography-3.4.8 datasets-1.12.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 fsspec-2021.9.0 gitdb-4.0.7 google-api-core-2.0.1 google-cloud-core-2.0.0 google-cloud-storage-1.42.2 google-crc32c-1.2.0 google-resumable-media-2.0.3 huggingface-hub-0.0.17 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.3.0 jaraco.text-3.5.1 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.1.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.11 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.2 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.1 tensorboardX-2.4 tokenizers-0.10.3 transformers-4.5.1 urllib3-1.25.11 wandb-0.12.2 xxhash-2.0.2 yarl-1.6.3 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp-models\n",
            "  Downloading allennlp_models-2.7.0-py3-none-any.whl (461 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 461 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.9.0+cu102)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: allennlp<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.7.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Collecting conllu==4.4.1\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0+cu102)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.42.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.3.4)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.1.96)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.17)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.62.2)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.12.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.18.49)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.12.2)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (8.9.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.4)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (7.6.5)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.5.0)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.5)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.6)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.49 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.21.49)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.49->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.49->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.25.11)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2021.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.post0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.35.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.0.46)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.1.24)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2018.9)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (20201018)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.6.3)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (18.6.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (6.0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.16.0)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.7)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.11)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.7.1)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (8.5.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.2.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.20)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Building wheels for collected packages: word2number, ftfy\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=35d7326e966e133d00c08ee111ec0ec5fee906391d643a7cf2a94186d4e25f94\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=7d0d8931eb2e15d01e088ea96301dfe04ca6fb8f3e395dc76da52d2e01738b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built word2number ftfy\n",
            "Installing collected packages: word2number, py-rouge, ftfy, conllu, allennlp-models\n",
            "Successfully installed allennlp-models-2.7.0 conllu-4.4.1 ftfy-6.0.3 py-rouge-1.1 word2number-1.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC2N3O1BvPAm",
        "outputId": "c2ce4b53-0e99-481f-8005-0ac805ddebc7"
      },
      "source": [
        "!allennlp test-install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2021-10-10 15:49:53,330 - ERROR - allennlp.common.plugins - Plugin allennlp_models could not be loaded: No module named 'nltk.translate.meteor_score'\n",
            "2021-10-10 15:49:53,491 - INFO - allennlp.commands.test_install - AllenNLP version 2.7.0 installed to /usr/local/lib/python3.7/dist-packages/allennlp\n",
            "2021-10-10 15:49:53,491 - INFO - allennlp.commands.test_install - Cuda devices available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6SLm79ZwSca",
        "outputId": "c67d141f-9c3d-4840-cc9f-6989550d0231"
      },
      "source": [
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVQwwqnSk4w"
      },
      "source": [
        "### **1.1 Uploading Necessary Files to Directory**\n",
        "\n",
        "Need to: \n",
        "\n",
        "1. upload OntoNotes data to Content (done using script below)\n",
        "2. upload jsonnet files to /content/\n",
        "3. upload coref_adv.py to /content/ (for step 3)\n",
        "4. upload coref_model to /content/ (for step 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQjQPw7XRLwt",
        "outputId": "1c306b9f-c746-419f-e1ab-c5e9ba0cb6ed"
      },
      "source": [
        "!ls \"/content/gdrive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev\t\t\t   test.english.v4_gold_conll\n",
            "dev.english.v4_gold_conll  train\n",
            "test\t\t\t   train.english.v4_gold_conll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKLBFNLiRbaP"
      },
      "source": [
        "#!mv \"/content/gdrive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/dev.english.v4_gold_conll\" \"/content/dev\"\n",
        "\n",
        "!cp \"/content/gdrive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/dev.english.v4_gold_conll\" \"/content/dev\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5BcxK-XUUhW"
      },
      "source": [
        "#!mv \"/content/drive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/train.english.v4_gold_conll\" \"/content/train\"\n",
        "!cp \"/content/gdrive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/train.english.v4_gold_conll\" \"/content/train\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9jMCH5mUdLT"
      },
      "source": [
        "#!mv \"/content/drive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/test.english.v4_gold_conll\" \"/content/test\"\n",
        "!cp \"/content/gdrive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/test.english.v4_gold_conll\" \"/content/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23p_pBGyTSvp"
      },
      "source": [
        "## **2. DEBIASING CONTEXTUALISED WORD EMBEDDINGS - BiasMitigatorApplicator**\n",
        "\n",
        "\n",
        "\n",
        "### **2.1 Training the model**\n",
        "\n",
        "Using span-bert-base as the model to train with. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw11y3F9sQJS",
        "outputId": "b8132a08-5527-4141-fe8a-34e596627ac9"
      },
      "source": [
        "!allennlp train SpanBERT_BiasMA_new.jsonnet.txt --include-package coref_adv -s /content/BiasMitigatorApplicator4 #allennlp_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-09-11 17:21:22,373 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2021-09-11 17:21:22,411 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2021-09-11 17:21:22,412 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2021-09-11 17:21:22,412 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2021-09-11 17:21:22,412 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2021-09-11 17:21:22,413 - INFO - allennlp.common.checks - Pytorch version: 1.9.0+cu102\n",
            "2021-09-11 17:21:22,413 - INFO - allennlp.common.params - type = default\n",
            "2021-09-11 17:21:22,414 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-11 17:21:22,414 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-11 17:21:22,415 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-09-11 17:21:22,415 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-11 17:21:22,415 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-11 17:21:22,415 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-09-11 17:21:22,415 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-09-11 17:21:22,417 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:21:22,418 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:21:22,418 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-11 17:21:22,418 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:21:22,418 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:21:22,418 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-11 17:21:22,419 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-11 17:21:22,419 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-11 17:21:28,332 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-11 17:21:28,332 - INFO - allennlp.common.params - dataset_reader.max_sentences = 20\n",
            "2021-09-11 17:21:28,333 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-11 17:21:28,333 - INFO - allennlp.common.params - train_data_path = /content/train\n",
            "2021-09-11 17:21:28,333 - INFO - allennlp.common.params - type = /content/train\n",
            "2021-09-11 17:21:28,334 - INFO - allennlp.common.params - model.type = allennlp.fairness.bias_mitigator_applicator.BiasMitigatorApplicator\n",
            "2021-09-11 17:21:28,334 - INFO - allennlp.common.params - data_loader.type = ref\n",
            "2021-09-11 17:21:28,337 - INFO - allennlp.common.params - trainer.type = ref\n",
            "2021-09-11 17:21:28,338 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f3fa89822d0>\n",
            "2021-09-11 17:21:28,338 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2021-09-11 17:21:28,339 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-11 17:21:28,339 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-11 17:21:28,339 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-09-11 17:21:28,339 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-11 17:21:28,340 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-11 17:21:28,340 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-09-11 17:21:28,340 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-09-11 17:21:28,341 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:21:28,342 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:21:28,342 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-11 17:21:28,342 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:21:28,342 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:21:28,343 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-11 17:21:28,343 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-11 17:21:28,343 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-11 17:21:28,344 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-11 17:21:28,344 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - validation_data_path = /content/dev\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - type = /content/dev\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - test_data_path = None\n",
            "2021-09-11 17:21:28,345 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2021-09-11 17:21:28,346 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2021-09-11 17:21:28,346 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-11 17:21:28,346 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-11 17:21:28,346 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-11 17:21:28,346 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-11 17:21:28,347 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-11 17:21:28,347 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-11 17:21:28,347 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-11 17:21:28,348 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-11 17:21:28,348 - INFO - allennlp.common.params - type = text\n",
            "2021-09-11 17:21:28,348 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-11 17:21:28,348 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-11 17:21:28,348 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-11 17:21:28,349 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-11 17:21:28,350 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f3fb0b29310>\n",
            "loading instances: 2802it [01:14, 37.50it/s]\n",
            "2021-09-11 17:22:43,065 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-11 17:22:43,066 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-11 17:22:43,066 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-11 17:22:43,066 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-11 17:22:43,066 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-11 17:22:43,067 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-11 17:22:43,067 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-11 17:22:43,067 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - type = text\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-11 17:22:43,068 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-11 17:22:43,069 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-11 17:22:43,069 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-11 17:22:43,069 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f3fb0b29310>\n",
            "loading instances: 343it [00:12, 27.05it/s]\n",
            "2021-09-11 17:22:55,748 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-09-11 17:22:55,748 - INFO - allennlp.common.params - min_count = None\n",
            "2021-09-11 17:22:55,748 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-09-11 17:22:55,748 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-09-11 17:22:55,748 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-09-11 17:22:55,749 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 3145it [00:01, 1662.60it/s]\n",
            "2021-09-11 17:22:57,642 - INFO - allennlp.common.params - model.type = allennlp.fairness.bias_mitigator_applicator.BiasMitigatorApplicator\n",
            "2021-09-11 17:22:57,642 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-09-11 17:22:57,642 - INFO - allennlp.common.params - model.base_model._pretrained.archive_file = /content/coref_model5/model.tar.gz\n",
            "2021-09-11 17:22:57,642 - INFO - allennlp.common.params - model.base_model._pretrained.module_path = \n",
            "2021-09-11 17:22:57,643 - INFO - allennlp.common.params - model.base_model._pretrained.freeze = False\n",
            "2021-09-11 17:22:57,643 - INFO - allennlp.models.archival - loading archive file /content/coref_model5/model.tar.gz\n",
            "2021-09-11 17:22:57,643 - INFO - allennlp.models.archival - extracting archive file /content/coref_model5/model.tar.gz to temp dir /tmp/tmptynql4hs\n",
            "2021-09-11 17:23:03,026 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-11 17:23:03,027 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-09-11 17:23:03,027 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-11 17:23:03,027 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-11 17:23:03,027 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-09-11 17:23:03,028 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-09-11 17:23:03,029 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,030 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,030 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-11 17:23:03,030 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,030 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,030 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-11 17:23:03,031 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-11 17:23:03,031 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-11 17:23:03,032 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-11 17:23:03,033 - INFO - allennlp.common.params - dataset_reader.max_sentences = 20\n",
            "2021-09-11 17:23:03,033 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-11 17:23:03,033 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-11 17:23:03,033 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-09-11 17:23:03,033 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-11 17:23:03,034 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-11 17:23:03,034 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-09-11 17:23:03,034 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,036 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-11 17:23:03,037 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-11 17:23:03,037 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-11 17:23:03,039 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-11 17:23:03,039 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-09-11 17:23:03,039 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-11 17:23:03,039 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-09-11 17:23:03,039 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmptynql4hs/vocabulary.\n",
            "2021-09-11 17:23:03,040 - INFO - allennlp.common.params - model.type = coref_adv\n",
            "2021-09-11 17:23:03,040 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-09-11 17:23:03,041 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2021-09-11 17:23:03,041 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-09-11 17:23:03,042 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-09-11 17:23:03,043 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-09-11 17:23:03,044 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-09-11 17:23:03,045 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-09-11 17:23:03,046 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2021-09-11 17:23:04,938 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-11 17:23:04,938 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-11 17:23:04,938 - INFO - allennlp.common.params - model.context_layer.input_dim = 768\n",
            "2021-09-11 17:23:04,939 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
            "2021-09-11 17:23:04,941 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 2324\n",
            "2021-09-11 17:23:04,941 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
            "2021-09-11 17:23:04,942 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
            "2021-09-11 17:23:04,942 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
            "2021-09-11 17:23:04,942 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,943 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,943 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,943 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
            "2021-09-11 17:23:04,994 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
            "2021-09-11 17:23:04,996 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 6992\n",
            "2021-09-11 17:23:04,997 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
            "2021-09-11 17:23:04,997 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
            "2021-09-11 17:23:04,997 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
            "2021-09-11 17:23:04,997 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,998 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,998 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-11 17:23:04,998 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
            "2021-09-11 17:23:05,112 - INFO - allennlp.common.params - model.feature_size = 20\n",
            "2021-09-11 17:23:05,113 - INFO - allennlp.common.params - model.max_span_width = 30\n",
            "2021-09-11 17:23:05,113 - INFO - allennlp.common.params - model.spans_per_word = 0.08\n",
            "2021-09-11 17:23:05,113 - INFO - allennlp.common.params - model.max_antecedents = 5\n",
            "2021-09-11 17:23:05,113 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
            "2021-09-11 17:23:05,114 - INFO - allennlp.common.params - model.inference_order = 2\n",
            "2021-09-11 17:23:05,114 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2021-09-11 17:23:05,114 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f3f8cf9b610>\n",
            "2021-09-11 17:23:05,163 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-09-11 17:23:05,164 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-09-11 17:23:05,164 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-09-11 17:23:05,164 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-09-11 17:23:05,164 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-09-11 17:23:05,164 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _distance_embedding.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
            "2021-09-11 17:23:05,165 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _mention_scorer._module.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,166 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,167 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,168 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-09-11 17:23:05,169 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-09-11 17:23:05,170 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,171 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,249 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,249 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-09-11 17:23:05,249 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,250 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,251 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,252 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,253 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-09-11 17:23:05,254 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-09-11 17:23:05,255 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-09-11 17:23:05,256 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,257 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,258 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-09-11 17:23:05,259 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-09-11 17:23:05,260 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-09-11 17:23:05,261 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,262 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-09-11 17:23:05,263 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,264 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-09-11 17:23:05,265 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-09-11 17:23:05,266 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-09-11 17:23:05,267 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-09-11 17:23:05,276 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-09-11 17:23:05,276 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-09-11 17:23:05,815 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmptynql4hs\n",
            "2021-09-11 17:23:05,880 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-09-11 17:23:05,880 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-09-11 17:23:05,881 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-09-11 17:23:05,882 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-09-11 17:23:05,882 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.seed_word_pairs_file = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-09-11 17:23:05,883 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-09-11 17:23:05,884 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-09-11 17:23:05,885 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-09-11 17:23:05,885 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-09-11 17:23:05,885 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:05,885 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-11 17:23:05,886 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.add_special_tokens = True\n",
            "2021-09-11 17:23:05,886 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.max_length = 512\n",
            "2021-09-11 17:23:05,886 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.tokenizer_kwargs = None\n",
            "2021-09-11 17:23:05,887 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.direction_vocab = None\n",
            "2021-09-11 17:23:05,888 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.namespace = tokens\n",
            "2021-09-11 17:23:05,888 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.requires_grad = False\n",
            "2021-09-11 17:23:05,888 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.noise = 1e-10\n",
            "2021-09-11 17:23:06,059 - INFO - allennlp.common.file_utils - cache of https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json is up-to-date\n",
            "2021-09-11 17:23:06,067 - INFO - allennlp.common.params - model.bias_mitigator.requires_grad = True\n",
            "2021-09-11 17:46:47,141 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2021-09-11 17:46:47,142 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2021-09-11 17:46:47,142 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2021-09-11 17:46:47,142 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2021-09-11 17:46:47,143 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2021-09-11 17:46:47,143 - INFO - allennlp.common.params - trainer.validation_metric = +coref_f1\n",
            "2021-09-11 17:46:47,143 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-09-11 17:46:47,144 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-09-11 17:46:47,144 - INFO - allennlp.common.params - trainer.num_epochs = 10\n",
            "2021-09-11 17:46:47,144 - INFO - allennlp.common.params - trainer.grad_norm = False\n",
            "2021-09-11 17:46:47,144 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2021-09-11 17:46:47,144 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2021-09-11 17:46:47,145 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2021-09-11 17:46:47,145 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2021-09-11 17:46:47,145 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-09-11 17:46:47,146 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-09-11 17:46:47,146 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2021-09-11 17:46:47,146 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2021-09-11 17:46:47,146 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f3fa89c8710>\n",
            "2021-09-11 17:46:47,146 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2021-09-11 17:46:47,147 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2021-09-11 17:46:47,147 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2021-09-11 17:46:47,147 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2021-09-11 17:46:50,069 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-09-11 17:46:50,069 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2021-09-11 17:46:50,070 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05\n",
            "2021-09-11 17:46:50,070 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2021-09-11 17:46:50,070 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2021-09-11 17:46:50,070 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2021-09-11 17:46:50,071 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n",
            "2021-09-11 17:46:50,071 - INFO - allennlp.training.optimizers - Number of trainable parameters: 132202792\n",
            "2021-09-11 17:46:50,081 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-09-11 17:46:50,082 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,083 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-09-11 17:46:50,084 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,085 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,086 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,087 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-09-11 17:46:50,088 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,089 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-09-11 17:46:50,090 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,091 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,092 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,092 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,110 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,110 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-09-11 17:46:50,111 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,112 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,113 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,114 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-09-11 17:46:50,115 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-09-11 17:46:50,116 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,117 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-09-11 17:46:50,118 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,119 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,120 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-09-11 17:46:50,121 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,122 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-09-11 17:46:50,123 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,124 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-09-11 17:46:50,125 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-09-11 17:46:50,126 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-09-11 17:46:50,127 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.0.weight\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.0.bias\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.1.weight\n",
            "2021-09-11 17:46:50,128 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.1.bias\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._mention_scorer._module.weight\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._mention_scorer._module.bias\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-09-11 17:46:50,129 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._antecedent_scorer._module.weight\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._antecedent_scorer._module.bias\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._attentive_span_extractor._global_attention._module.weight\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._attentive_span_extractor._global_attention._module.bias\n",
            "2021-09-11 17:46:50,130 - INFO - allennlp.common.util - base_model._distance_embedding.weight\n",
            "2021-09-11 17:46:50,131 - INFO - allennlp.common.util - base_model._coarse2fine_scorer.weight\n",
            "2021-09-11 17:46:50,131 - INFO - allennlp.common.util - base_model._coarse2fine_scorer.bias\n",
            "2021-09-11 17:46:50,131 - INFO - allennlp.common.util - base_model._span_updating_gated_sum._gate.weight\n",
            "2021-09-11 17:46:50,131 - INFO - allennlp.common.util - base_model._span_updating_gated_sum._gate.bias\n",
            "2021-09-11 17:46:50,131 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-09-11 17:46:50,132 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n",
            "2021-09-11 17:46:50,132 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2021-09-11 17:46:50,132 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2021-09-11 17:46:50,132 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2021-09-11 17:46:50,132 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - type = default\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - save_completed_epochs = True\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - save_every_num_batches = None\n",
            "2021-09-11 17:46:50,133 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
            "2021-09-11 17:46:50,134 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
            "2021-09-11 17:46:50,136 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2021-09-11 17:46:50,136 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/9\n",
            "2021-09-11 17:46:50,136 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  FutureWarning)\n",
            "2021-09-11 17:46:50,137 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 505M\n",
            "2021-09-11 17:46:50,139 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "  0%|          | 0/2802 [00:00<?, ?it/s]2021-09-11 17:46:50,340 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
            "2021-09-11 17:46:50,341 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/token_ids (Shape: 1 x 264)\n",
            "tensor([[  101,   174, 15792,  ...,  3612,   119,   102]], device='cuda:0')\n",
            "2021-09-11 17:46:50,342 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/mask (Shape: 1 x 222)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:46:50,343 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/type_ids (Shape: 1 x 264)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "2021-09-11 17:46:50,345 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/wordpiece_mask (Shape: 1 x 264)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:46:50,346 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/segment_concat_mask (Shape: 1 x 264)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:46:50,347 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/offsets (Shape: 1 x 222 x 2)\n",
            "tensor([[[  1,   3],\n",
            "         [  4,   4],\n",
            "         [  5,   5],\n",
            "         ...,\n",
            "         [260, 260],\n",
            "         [261, 261],\n",
            "         [262, 262]]], device='cuda:0')\n",
            "2021-09-11 17:46:50,349 - INFO - allennlp.training.callbacks.console_logger - batch_input/spans (Shape: 1 x 3441 x 2)\n",
            "tensor([[[  0,   0],\n",
            "         [  0,   1],\n",
            "         [  0,   2],\n",
            "         ...,\n",
            "         [220, 220],\n",
            "         [220, 221],\n",
            "         [221, 221]]], device='cuda:0')\n",
            "2021-09-11 17:46:50,351 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/metadata\" : (Length 1 of type \"<class 'dict'>\")\n",
            "2021-09-11 17:46:50,351 - INFO - allennlp.training.callbacks.console_logger - batch_input/span_labels (Shape: 1 x 3441)\n",
            "tensor([[ 1,  0, -1,  ..., -1, -1, -1]], device='cuda:0')\n",
            "coref_precision: 0.7963, coref_recall: 0.3241, coref_f1: 0.4521, mention_recall: 0.3793, batch_loss: 3.0059, loss: 3.0059 ||:   0%|          | 1/2802 [00:00<09:58,  4.68it/s]/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "coref_precision: 0.8745, coref_recall: 0.3477, coref_f1: 0.4956, mention_recall: 0.5042, batch_loss: 1.3834, loss: 4.4066 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 17:55:02,279 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "  0%|          | 0/343 [00:00<?, ?it/s]2021-09-11 17:55:02,404 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
            "2021-09-11 17:55:02,404 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/token_ids (Shape: 1 x 682)\n",
            "tensor([[  101,  1165, 22572,  ..., 10242,   119,   102]], device='cuda:0')\n",
            "2021-09-11 17:55:02,405 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/mask (Shape: 1 x 549)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:55:02,406 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/type_ids (Shape: 1 x 682)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "2021-09-11 17:55:02,407 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/wordpiece_mask (Shape: 1 x 680)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:55:02,408 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/segment_concat_mask (Shape: 1 x 682)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-09-11 17:55:02,409 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/offsets (Shape: 1 x 549 x 2)\n",
            "tensor([[[  1,   1],\n",
            "         [  2,   3],\n",
            "         [  4,   4],\n",
            "         ...,\n",
            "         [674, 674],\n",
            "         [675, 677],\n",
            "         [678, 678]]], device='cuda:0')\n",
            "2021-09-11 17:55:02,411 - INFO - allennlp.training.callbacks.console_logger - batch_input/spans (Shape: 1 x 8410 x 2)\n",
            "tensor([[[  0,   0],\n",
            "         [  0,   1],\n",
            "         [  0,   2],\n",
            "         ...,\n",
            "         [547, 547],\n",
            "         [547, 548],\n",
            "         [548, 548]]], device='cuda:0')\n",
            "2021-09-11 17:55:02,412 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/metadata\" : (Length 1 of type \"<class 'dict'>\")\n",
            "2021-09-11 17:55:02,412 - INFO - allennlp.training.callbacks.console_logger - batch_input/span_labels (Shape: 1 x 8410)\n",
            "tensor([[-1, -1, -1,  ...,  9, -1, -1]], device='cuda:0')\n",
            "coref_precision: 0.7185, coref_recall: 0.3166, coref_f1: 0.4375, mention_recall: 0.4917, batch_loss: 4.2002, loss: 19.9465 ||: 100%|##########| 343/343 [00:30<00:00, 11.10it/s]\n",
            "2021-09-11 17:55:33,191 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.496  |     0.438\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.875  |     0.718\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.348  |     0.317\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |   504.827  |       N/A\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - loss               |     4.407  |    19.946\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.504  |     0.492\n",
            "2021-09-11 17:55:33,192 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6017.777  |       N/A\n",
            "2021-09-11 17:55:38,154 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:48.018073\n",
            "2021-09-11 17:55:38,155 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 1:18:27\n",
            "2021-09-11 17:55:38,155 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/9\n",
            "2021-09-11 17:55:38,155 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 17:55:38,155 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 17:55:38,157 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.8819, coref_recall: 0.3583, coref_f1: 0.5078, mention_recall: 0.5125, batch_loss: 14.0513, loss: 4.0537 ||: 100%|##########| 2802/2802 [08:11<00:00,  5.70it/s]\n",
            "2021-09-11 18:03:49,892 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7229, coref_recall: 0.3201, coref_f1: 0.4424, mention_recall: 0.5013, batch_loss: 10.6170, loss: 19.3201 ||: 100%|##########| 343/343 [00:30<00:00, 11.20it/s]\n",
            "2021-09-11 18:04:20,521 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:04:20,521 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.508  |     0.442\n",
            "2021-09-11 18:04:20,521 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.882  |     0.723\n",
            "2021-09-11 18:04:20,521 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.358  |     0.320\n",
            "2021-09-11 18:04:20,521 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8973.538  |       N/A\n",
            "2021-09-11 18:04:20,522 - INFO - allennlp.training.callbacks.console_logger - loss               |     4.054  |    19.320\n",
            "2021-09-11 18:04:20,522 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.513  |     0.501\n",
            "2021-09-11 18:04:20,522 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6043.211  |       N/A\n",
            "2021-09-11 18:04:25,695 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:47.540731\n",
            "2021-09-11 18:04:25,696 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 1:10:01\n",
            "2021-09-11 18:04:25,696 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/9\n",
            "2021-09-11 18:04:25,696 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:04:25,696 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:04:25,699 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.8980, coref_recall: 0.3577, coref_f1: 0.5099, mention_recall: 0.5065, batch_loss: 4.9440, loss: 3.3066 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 18:12:38,245 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7049, coref_recall: 0.3278, coref_f1: 0.4454, mention_recall: 0.4952, batch_loss: 53.1319, loss: 21.5048 ||: 100%|##########| 343/343 [00:33<00:00, 10.25it/s]\n",
            "2021-09-11 18:13:11,721 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.510  |     0.445\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.898  |     0.705\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.358  |     0.328\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - loss               |     3.307  |    21.505\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.506  |     0.495\n",
            "2021-09-11 18:13:11,722 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6043.883  |       N/A\n",
            "2021-09-11 18:13:16,987 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:51.291123\n",
            "2021-09-11 18:13:16,987 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 1:01:30\n",
            "2021-09-11 18:13:16,987 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/9\n",
            "2021-09-11 18:13:16,988 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:13:16,988 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:13:16,990 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9025, coref_recall: 0.3504, coref_f1: 0.5033, mention_recall: 0.4961, batch_loss: 0.2729, loss: 2.8635 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 18:21:29,231 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7391, coref_recall: 0.3180, coref_f1: 0.4436, mention_recall: 0.4895, batch_loss: 1.4310, loss: 21.1875 ||: 100%|##########| 343/343 [00:30<00:00, 11.19it/s]\n",
            "2021-09-11 18:21:59,886 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:21:59,886 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.503  |     0.444\n",
            "2021-09-11 18:21:59,886 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.902  |     0.739\n",
            "2021-09-11 18:21:59,887 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.350  |     0.318\n",
            "2021-09-11 18:21:59,887 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:21:59,887 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.864  |    21.187\n",
            "2021-09-11 18:21:59,887 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.496  |     0.489\n",
            "2021-09-11 18:21:59,887 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 18:22:05,185 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:48.197077\n",
            "2021-09-11 18:22:05,185 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:52:44\n",
            "2021-09-11 18:22:05,185 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/9\n",
            "2021-09-11 18:22:05,185 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:22:05,186 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:22:05,188 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9022, coref_recall: 0.3571, coref_f1: 0.5101, mention_recall: 0.5041, batch_loss: 7.6630, loss: 2.8676 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 18:30:17,938 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7429, coref_recall: 0.3123, coref_f1: 0.4386, mention_recall: 0.4901, batch_loss: 0.0024, loss: 19.1904 ||: 100%|##########| 343/343 [00:30<00:00, 11.20it/s]\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.510  |     0.439\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.902  |     0.743\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.357  |     0.312\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.868  |    19.190\n",
            "2021-09-11 18:30:48,555 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.504  |     0.490\n",
            "2021-09-11 18:30:48,556 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 18:30:54,386 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:49.200543\n",
            "2021-09-11 18:30:54,386 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:43:58\n",
            "2021-09-11 18:30:54,386 - INFO - allennlp.training.gradient_descent_trainer - Epoch 5/9\n",
            "2021-09-11 18:30:54,386 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:30:54,387 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:30:54,389 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9180, coref_recall: 0.3713, coref_f1: 0.5272, mention_recall: 0.5069, batch_loss: 16.0539, loss: 2.1915 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 18:39:06,934 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7244, coref_recall: 0.3170, coref_f1: 0.4399, mention_recall: 0.4844, batch_loss: 17.3985, loss: 20.9007 ||: 100%|##########| 343/343 [00:33<00:00, 10.20it/s]\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.527  |     0.440\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.918  |     0.724\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.371  |     0.317\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.191  |    20.901\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.507  |     0.484\n",
            "2021-09-11 18:39:40,560 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 18:39:45,772 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:51.386402\n",
            "2021-09-11 18:39:45,773 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:35:13\n",
            "2021-09-11 18:39:45,773 - INFO - allennlp.training.gradient_descent_trainer - Epoch 6/9\n",
            "2021-09-11 18:39:45,773 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:39:45,774 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:39:45,776 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9246, coref_recall: 0.3725, coref_f1: 0.5296, mention_recall: 0.5059, batch_loss: 2.1431, loss: 1.8200 ||: 100%|##########| 2802/2802 [08:13<00:00,  5.68it/s]\n",
            "2021-09-11 18:47:58,834 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7353, coref_recall: 0.3294, coref_f1: 0.4539, mention_recall: 0.4969, batch_loss: 34.8008, loss: 22.8756 ||: 100%|##########| 343/343 [00:30<00:00, 11.15it/s]\n",
            "2021-09-11 18:48:29,602 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:48:29,602 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.530  |     0.454\n",
            "2021-09-11 18:48:29,602 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.925  |     0.735\n",
            "2021-09-11 18:48:29,602 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.372  |     0.329\n",
            "2021-09-11 18:48:29,603 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:48:29,603 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.820  |    22.876\n",
            "2021-09-11 18:48:29,603 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.506  |     0.497\n",
            "2021-09-11 18:48:29,603 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 18:48:35,036 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:49.263117\n",
            "2021-09-11 18:48:35,036 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:26:25\n",
            "2021-09-11 18:48:35,037 - INFO - allennlp.training.gradient_descent_trainer - Epoch 7/9\n",
            "2021-09-11 18:48:35,037 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:48:35,037 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:48:35,040 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9288, coref_recall: 0.3691, coref_f1: 0.5269, mention_recall: 0.5003, batch_loss: 1.4797, loss: 1.5289 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 18:56:47,908 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7344, coref_recall: 0.3221, coref_f1: 0.4469, mention_recall: 0.4888, batch_loss: 15.7016, loss: 22.8478 ||: 100%|##########| 343/343 [00:30<00:00, 11.25it/s]\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.527  |     0.447\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.929  |     0.734\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.369  |     0.322\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.529  |    22.848\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.500  |     0.489\n",
            "2021-09-11 18:57:18,408 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 18:57:23,680 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:48.643410\n",
            "2021-09-11 18:57:23,680 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:17:37\n",
            "2021-09-11 18:57:23,680 - INFO - allennlp.training.gradient_descent_trainer - Epoch 8/9\n",
            "2021-09-11 18:57:23,681 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 18:57:23,681 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 18:57:23,683 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9322, coref_recall: 0.3692, coref_f1: 0.5276, mention_recall: 0.4987, batch_loss: 4.7491, loss: 1.3118 ||: 100%|##########| 2802/2802 [08:11<00:00,  5.70it/s]\n",
            "2021-09-11 19:05:35,293 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7292, coref_recall: 0.3130, coref_f1: 0.4371, mention_recall: 0.4802, batch_loss: 5.8271, loss: 23.4248 ||: 100%|##########| 343/343 [00:33<00:00, 10.28it/s]\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.528  |     0.437\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.932  |     0.729\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.369  |     0.313\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.312  |    23.425\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.499  |     0.480\n",
            "2021-09-11 19:06:08,665 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 19:06:13,764 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:50.083482\n",
            "2021-09-11 19:06:13,764 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:08:48\n",
            "2021-09-11 19:06:13,764 - INFO - allennlp.training.gradient_descent_trainer - Epoch 9/9\n",
            "2021-09-11 19:06:13,765 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 5.9G\n",
            "2021-09-11 19:06:13,765 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 8.8G\n",
            "2021-09-11 19:06:13,768 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9349, coref_recall: 0.3650, coref_f1: 0.5237, mention_recall: 0.4929, batch_loss: 0.1351, loss: 1.1583 ||: 100%|##########| 2802/2802 [08:12<00:00,  5.69it/s]\n",
            "2021-09-11 19:14:26,388 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7390, coref_recall: 0.3040, coref_f1: 0.4300, mention_recall: 0.4741, batch_loss: 3.9620, loss: 22.5926 ||: 100%|##########| 343/343 [00:30<00:00, 11.13it/s]\n",
            "2021-09-11 19:14:57,213 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-11 19:14:57,213 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.524  |     0.430\n",
            "2021-09-11 19:14:57,213 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.935  |     0.739\n",
            "2021-09-11 19:14:57,213 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.365  |     0.304\n",
            "2021-09-11 19:14:57,213 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.038  |       N/A\n",
            "2021-09-11 19:14:57,214 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.158  |    22.593\n",
            "2021-09-11 19:14:57,214 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.493  |     0.474\n",
            "2021-09-11 19:14:57,214 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6044.125  |       N/A\n",
            "2021-09-11 19:15:02,639 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:08:48.874978\n",
            "2021-09-11 19:15:11,287 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 6,\n",
            "  \"peak_worker_0_memory_MB\": 6044.125,\n",
            "  \"peak_gpu_0_memory_MB\": 8978.03759765625,\n",
            "  \"training_duration\": \"1:28:07.076605\",\n",
            "  \"epoch\": 9,\n",
            "  \"training_coref_precision\": 0.9349386250175288,\n",
            "  \"training_coref_recall\": 0.36500187201627693,\n",
            "  \"training_coref_f1\": 0.5237167995045483,\n",
            "  \"training_mention_recall\": 0.49291488883898493,\n",
            "  \"training_loss\": 1.158346459855922,\n",
            "  \"training_worker_0_memory_MB\": 6044.125,\n",
            "  \"training_gpu_0_memory_MB\": 8978.03759765625,\n",
            "  \"validation_coref_precision\": 0.7389920687981425,\n",
            "  \"validation_coref_recall\": 0.30404214425053155,\n",
            "  \"validation_coref_f1\": 0.43001454067850986,\n",
            "  \"validation_mention_recall\": 0.47413208039676324,\n",
            "  \"validation_loss\": 22.59258615794402,\n",
            "  \"best_validation_coref_precision\": 0.7352899474864617,\n",
            "  \"best_validation_coref_recall\": 0.32939224056053024,\n",
            "  \"best_validation_coref_f1\": 0.4538821151584808,\n",
            "  \"best_validation_mention_recall\": 0.4968937614199948,\n",
            "  \"best_validation_loss\": 22.87557198545486\n",
            "}\n",
            "2021-09-11 19:15:11,287 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/BiasMitigatorApplicator4/model.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otMjEyJ97GJF",
        "outputId": "1d5901c6-b4fb-4234-e223-078b0f1ecfa8"
      },
      "source": [
        "# using span-bert-large \n",
        "!allennlp train -f SpanBERT_BiasMitigatorApplicator.jsonnet.txt -s /content/BiasMitigatorApplicator2 #allennlp_models\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-10-10 16:10:43,232 - ERROR - allennlp.common.plugins - Plugin allennlp_models could not be loaded: No module named 'nltk.translate.meteor_score'\n",
            "2021-10-10 16:10:43,264 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2021-10-10 16:10:43,265 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2021-10-10 16:10:43,265 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2021-10-10 16:10:43,265 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2021-10-10 16:10:43,266 - INFO - allennlp.common.checks - Pytorch version: 1.9.0+cu111\n",
            "2021-10-10 16:10:43,266 - INFO - allennlp.common.params - type = default\n",
            "2021-10-10 16:10:43,267 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-10-10 16:10:43,267 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-10-10 16:10:43,268 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-10-10 16:10:43,268 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 16:10:43,268 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 16:10:43,268 - INFO - allennlp.common.params - dataset_reader.max_span_width = 10\n",
            "2021-10-10 16:10:43,268 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 16:10:43,270 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:10:43,270 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:10:43,270 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 16:10:43,270 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:10:43,271 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:10:43,271 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 16:10:43,271 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 16:10:43,271 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 16:10:47,687 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 16:10:47,687 - INFO - allennlp.common.params - dataset_reader.max_sentences = 10\n",
            "2021-10-10 16:10:47,687 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 16:10:47,687 - INFO - allennlp.common.params - train_data_path = /content/train\n",
            "2021-10-10 16:10:47,688 - INFO - allennlp.common.params - type = /content/train\n",
            "2021-10-10 16:10:47,688 - INFO - allennlp.common.params - model.type = allennlp.fairness.bias_mitigator_applicator.BiasMitigatorApplicator\n",
            "2021-10-10 16:10:47,688 - INFO - allennlp.common.params - data_loader.type = ref\n",
            "2021-10-10 16:10:47,690 - INFO - allennlp.common.params - trainer.type = ref\n",
            "2021-10-10 16:10:47,691 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f6f22412110>\n",
            "2021-10-10 16:10:47,691 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 16:10:47,692 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 10\n",
            "2021-10-10 16:10:47,693 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 16:10:47,694 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 16:10:47,695 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - validation_data_path = /content/dev\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - type = /content/dev\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - test_data_path = None\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2021-10-10 16:10:47,696 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-10-10 16:10:47,697 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - type = text\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-10-10 16:10:47,698 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-10-10 16:10:47,699 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f6f2ab6ad10>\n",
            "loading instances: 2802it [00:40, 69.92it/s]\n",
            "2021-10-10 16:11:27,775 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-10-10 16:11:27,776 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - type = text\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-10-10 16:11:27,777 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f6f2ab6ad10>\n",
            "loading instances: 343it [00:09, 37.99it/s]\n",
            "2021-10-10 16:11:36,806 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-10-10 16:11:36,806 - INFO - allennlp.common.params - min_count = None\n",
            "2021-10-10 16:11:36,806 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-10-10 16:11:36,806 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-10-10 16:11:36,806 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-10-10 16:11:36,807 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 3145it [00:00, 4466.51it/s]\n",
            "2021-10-10 16:11:37,512 - INFO - allennlp.common.params - model.type = allennlp.fairness.bias_mitigator_applicator.BiasMitigatorApplicator\n",
            "2021-10-10 16:11:37,512 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-10-10 16:11:37,512 - INFO - allennlp.common.params - model.base_model._pretrained.archive_file = https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\n",
            "2021-10-10 16:11:37,512 - INFO - allennlp.common.params - model.base_model._pretrained.module_path = \n",
            "2021-10-10 16:11:37,512 - INFO - allennlp.common.params - model.base_model._pretrained.freeze = False\n",
            "2021-10-10 16:11:37,651 - INFO - allennlp.common.file_utils - cache of https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz is up-to-date\n",
            "2021-10-10 16:11:37,651 - INFO - allennlp.models.archival - loading archive file https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz from cache at /root/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174\n",
            "2021-10-10 16:11:37,651 - INFO - allennlp.models.archival - extracting archive file /root/.allennlp/cache/0f6b052811b20b13280e609a96efe71ebc636b9c823a5c906ba24459e6e68af9.c1dab61d84cc7c3f7d6751c260040607cb7023a002778ba8f9b9d196b6539174 to temp dir /tmp/tmpy835z0er\n",
            "2021-10-10 16:11:50,713 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-10-10 16:11:50,714 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-10-10 16:11:50,714 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 16:11:50,714 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 16:11:50,714 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-10-10 16:11:50,714 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 16:11:50,716 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,716 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 16:11:50,717 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 16:11:50,719 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 16:11:50,719 - INFO - allennlp.common.params - dataset_reader.max_sentences = 110\n",
            "2021-10-10 16:11:50,719 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 16:11:50,719 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-10-10 16:11:50,719 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-10-10 16:11:50,720 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 16:11:50,720 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 16:11:50,720 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-10-10 16:11:50,720 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 16:11:50,722 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 16:11:50,723 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 16:11:50,724 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 16:11:50,724 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-10-10 16:11:50,724 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 16:11:50,724 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-10-10 16:11:50,725 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpy835z0er/vocabulary.\n",
            "2021-10-10 16:11:50,725 - INFO - allennlp.common.params - model.type = coref\n",
            "2021-10-10 16:11:50,725 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-10-10 16:11:50,726 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2021-10-10 16:11:50,726 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-10-10 16:11:50,727 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-10-10 16:11:50,728 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-10-10 16:11:50,729 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,729 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 16:11:50,729 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-10-10 16:11:50,730 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-10-10 16:11:50,731 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-10-10 16:11:50,731 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 16:11:50,731 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-10-10 16:11:50,731 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2021-10-10 16:11:57,079 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-10-10 16:11:57,079 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-10-10 16:11:57,079 - INFO - allennlp.common.params - model.context_layer.input_dim = 1024\n",
            "2021-10-10 16:11:57,080 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
            "2021-10-10 16:11:57,081 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 3092\n",
            "2021-10-10 16:11:57,082 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
            "2021-10-10 16:11:57,082 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
            "2021-10-10 16:11:57,082 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
            "2021-10-10 16:11:57,082 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,083 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,083 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,083 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
            "2021-10-10 16:11:57,136 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
            "2021-10-10 16:11:57,138 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 9296\n",
            "2021-10-10 16:11:57,139 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
            "2021-10-10 16:11:57,139 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
            "2021-10-10 16:11:57,139 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
            "2021-10-10 16:11:57,139 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,140 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,140 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 16:11:57,140 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
            "2021-10-10 16:11:57,261 - INFO - allennlp.common.params - model.feature_size = 20\n",
            "2021-10-10 16:11:57,261 - INFO - allennlp.common.params - model.max_span_width = 30\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.spans_per_word = 0.4\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.max_antecedents = 50\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.inference_order = 2\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2021-10-10 16:11:57,262 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f6f06a8c9d0>\n",
            "2021-10-10 16:11:57,336 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-10-10 16:11:57,338 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _distance_embedding.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-10-10 16:11:57,339 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _mention_scorer._module.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,340 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,341 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-10-10 16:11:57,342 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,343 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,344 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,345 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-10-10 16:11:57,355 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,356 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,357 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,358 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,359 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
            "2021-10-10 16:11:57,360 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,361 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,362 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,363 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
            "2021-10-10 16:11:57,364 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,365 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-10-10 16:11:57,366 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,367 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,367 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,367 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,367 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-10-10 16:11:57,461 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-10-10 16:11:57,461 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
            "2021-10-10 16:11:57,462 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
            "2021-10-10 16:11:57,463 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-10-10 16:11:57,464 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,465 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,466 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,467 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,468 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-10-10 16:11:57,469 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-10-10 16:11:57,475 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 16:11:57,475 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 16:11:58,603 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpy835z0er\n",
            "2021-10-10 16:11:58,762 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-10-10 16:11:58,763 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-10-10 16:11:58,764 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-10-10 16:11:58,764 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-10-10 16:11:58,764 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.seed_word_pairs_file = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 16:11:58,764 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 16:11:58,765 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 16:11:58,765 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-10-10 16:11:58,765 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-10-10 16:11:58,766 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.model_name = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:58,766 - INFO - allennlp.common.params - type = SpanBERT/spanbert-large-cased\n",
            "2021-10-10 16:11:58,766 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.add_special_tokens = True\n",
            "2021-10-10 16:11:58,766 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.max_length = 512\n",
            "2021-10-10 16:11:58,766 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.tokenizer_kwargs = None\n",
            "2021-10-10 16:11:58,767 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.direction_vocab = None\n",
            "2021-10-10 16:11:58,767 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.namespace = tokens\n",
            "2021-10-10 16:11:58,767 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.requires_grad = False\n",
            "2021-10-10 16:11:58,768 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.noise = 1e-10\n",
            "2021-10-10 16:11:58,941 - INFO - allennlp.common.file_utils - cache of https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json is up-to-date\n",
            "2021-10-10 16:11:58,947 - INFO - allennlp.common.params - model.bias_mitigator.requires_grad = True\n",
            "2021-10-10 16:13:05,758 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.patience = 4\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.validation_metric = +coref_f1\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-10-10 16:13:05,759 - INFO - allennlp.common.params - trainer.num_epochs = 5\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.grad_norm = False\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-10-10 16:13:05,760 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f6f22454550>\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2021-10-10 16:13:05,761 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2021-10-10 16:13:09,639 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-10-10 16:13:09,640 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2021-10-10 16:13:09,640 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05\n",
            "2021-10-10 16:13:09,640 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2021-10-10 16:13:09,640 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2021-10-10 16:13:09,640 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2021-10-10 16:13:09,641 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n",
            "2021-10-10 16:13:09,641 - INFO - allennlp.training.optimizers - Number of trainable parameters: 366241832\n",
            "2021-10-10 16:13:09,642 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2021-10-10 16:13:09,643 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2021-10-10 16:13:09,643 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,644 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-10-10 16:13:09,645 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,646 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,647 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,712 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,712 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-10-10 16:13:09,712 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-10-10 16:13:09,713 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-10-10 16:13:09,714 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,715 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-10-10 16:13:09,716 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,717 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,718 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-10-10 16:13:09,719 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.query.bias\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.weight\n",
            "2021-10-10 16:13:09,720 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.key.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.self.value.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.dense.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.12.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.weight\n",
            "2021-10-10 16:13:09,721 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.query.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.key.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.self.value.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.dense.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.13.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.query.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.weight\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.key.bias\n",
            "2021-10-10 16:13:09,722 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.self.value.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.dense.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.14.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.query.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.key.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.self.value.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,723 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.dense.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.15.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.query.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.key.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.self.value.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.dense.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.16.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,724 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.query.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.key.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.self.value.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.dense.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.17.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.query.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.key.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.self.value.bias\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,725 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.dense.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.18.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.query.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.key.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.self.value.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,726 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.dense.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.19.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.query.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.key.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.self.value.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,727 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.dense.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.20.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.query.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.key.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.self.value.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.dense.bias\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,728 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.21.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.query.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.key.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.self.value.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.dense.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.22.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.weight\n",
            "2021-10-10 16:13:09,729 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.query.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.key.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.self.value.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.dense.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.intermediate.dense.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.dense.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.23.output.LayerNorm.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 16:13:09,730 - INFO - allennlp.common.util - base_model._mention_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._mention_scorer._module.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._mention_scorer._module.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_scorer._module.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._antecedent_scorer._module.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._attentive_span_extractor._global_attention._module.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._attentive_span_extractor._global_attention._module.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._distance_embedding.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._coarse2fine_scorer.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._coarse2fine_scorer.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._span_updating_gated_sum._gate.weight\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.util - base_model._span_updating_gated_sum._gate.bias\n",
            "2021-10-10 16:13:09,731 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2021-10-10 16:13:09,732 - INFO - allennlp.common.params - type = default\n",
            "2021-10-10 16:13:09,733 - INFO - allennlp.common.params - save_completed_epochs = True\n",
            "2021-10-10 16:13:09,733 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
            "2021-10-10 16:13:09,733 - INFO - allennlp.common.params - save_every_num_batches = None\n",
            "2021-10-10 16:13:09,733 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
            "2021-10-10 16:13:09,733 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
            "2021-10-10 16:13:09,736 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2021-10-10 16:13:09,736 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/4\n",
            "2021-10-10 16:13:09,736 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.8G\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  FutureWarning)\n",
            "2021-10-10 16:13:09,737 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 1.4G\n",
            "2021-10-10 16:13:09,740 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "  0%|          | 0/2802 [00:00<?, ?it/s]2021-10-10 16:13:09,990 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
            "2021-10-10 16:13:09,990 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/token_ids (Shape: 1 x 135)\n",
            "tensor([[ 101,  195, 1204,  ..., 2006,  119,  102]], device='cuda:0')\n",
            "2021-10-10 16:13:09,991 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/mask (Shape: 1 x 127)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:13:09,992 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/type_ids (Shape: 1 x 135)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "2021-10-10 16:13:09,993 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/wordpiece_mask (Shape: 1 x 135)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:13:09,993 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/segment_concat_mask (Shape: 1 x 135)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:13:09,994 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/offsets (Shape: 1 x 127 x 2)\n",
            "tensor([[[  1,   2],\n",
            "         [  3,   3],\n",
            "         [  4,   4],\n",
            "         ...,\n",
            "         [131, 131],\n",
            "         [132, 132],\n",
            "         [133, 133]]], device='cuda:0')\n",
            "2021-10-10 16:13:09,995 - INFO - allennlp.training.callbacks.console_logger - batch_input/spans (Shape: 1 x 827 x 2)\n",
            "tensor([[[  0,   0],\n",
            "         [  0,   1],\n",
            "         [  0,   2],\n",
            "         ...,\n",
            "         [125, 125],\n",
            "         [125, 126],\n",
            "         [126, 126]]], device='cuda:0')\n",
            "2021-10-10 16:13:09,996 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/metadata\" : (Length 1 of type \"<class 'dict'>\")\n",
            "2021-10-10 16:13:09,997 - INFO - allennlp.training.callbacks.console_logger - batch_input/span_labels (Shape: 1 x 827)\n",
            "tensor([[-1, -1, -1,  ..., -1, -1, -1]], device='cuda:0')\n",
            "coref_precision: 0.9899, coref_recall: 0.8640, coref_f1: 0.9202, mention_recall: 0.9547, batch_loss: 1.3186, loss: 0.5362 ||:   7%|6         | 187/2802 [00:34<08:35,  5.07it/s]/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "coref_precision: 0.9604, coref_recall: 0.8279, coref_f1: 0.8873, mention_recall: 0.9475, batch_loss: 1.2261, loss: 2.1573 ||: 100%|##########| 2802/2802 [08:41<00:00,  5.38it/s]\n",
            "2021-10-10 16:21:50,981 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "  0%|          | 0/343 [00:00<?, ?it/s]2021-10-10 16:21:51,153 - INFO - allennlp.training.callbacks.console_logger - Batch inputs\n",
            "2021-10-10 16:21:51,154 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/token_ids (Shape: 1 x 682)\n",
            "tensor([[  101,  1165, 22572,  ..., 10242,   119,   102]], device='cuda:0')\n",
            "2021-10-10 16:21:51,155 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/mask (Shape: 1 x 549)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:21:51,155 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/type_ids (Shape: 1 x 682)\n",
            "tensor([[0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n",
            "2021-10-10 16:21:51,156 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/wordpiece_mask (Shape: 1 x 680)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:21:51,157 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/segment_concat_mask (Shape: 1 x 682)\n",
            "tensor([[True, True, True,  ..., True, True, True]], device='cuda:0')\n",
            "2021-10-10 16:21:51,158 - INFO - allennlp.training.callbacks.console_logger - batch_input/text/tokens/offsets (Shape: 1 x 549 x 2)\n",
            "tensor([[[  1,   1],\n",
            "         [  2,   3],\n",
            "         [  4,   4],\n",
            "         ...,\n",
            "         [674, 674],\n",
            "         [675, 677],\n",
            "         [678, 678]]], device='cuda:0')\n",
            "2021-10-10 16:21:51,159 - INFO - allennlp.training.callbacks.console_logger - batch_input/spans (Shape: 1 x 4546 x 2)\n",
            "tensor([[[  0,   0],\n",
            "         [  0,   1],\n",
            "         [  0,   2],\n",
            "         ...,\n",
            "         [547, 547],\n",
            "         [547, 548],\n",
            "         [548, 548]]], device='cuda:0')\n",
            "2021-10-10 16:21:51,160 - INFO - allennlp.training.callbacks.console_logger - Field : \"batch_input/metadata\" : (Length 1 of type \"<class 'dict'>\")\n",
            "2021-10-10 16:21:51,160 - INFO - allennlp.training.callbacks.console_logger - batch_input/span_labels (Shape: 1 x 4546)\n",
            "tensor([[-1, -1, -1,  ...,  9, -1, -1]], device='cuda:0')\n",
            "coref_precision: 0.7449, coref_recall: 0.7471, coref_f1: 0.7460, mention_recall: 0.9369, batch_loss: 22.6322, loss: 60.7671 ||: 100%|##########| 343/343 [00:45<00:00,  7.61it/s]\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.887  |     0.746\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.960  |     0.745\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.828  |     0.747\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  1398.960  |       N/A\n",
            "2021-10-10 16:22:36,056 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.157  |    60.767\n",
            "2021-10-10 16:22:36,057 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.947  |     0.937\n",
            "2021-10-10 16:22:36,057 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  8972.012  |       N/A\n",
            "2021-10-10 16:23:05,533 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:09:55.797053\n",
            "2021-10-10 16:23:05,533 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:37:45\n",
            "2021-10-10 16:23:05,534 - INFO - allennlp.training.gradient_descent_trainer - Epoch 1/4\n",
            "2021-10-10 16:23:05,534 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.8G\n",
            "2021-10-10 16:23:05,534 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 12G\n",
            "2021-10-10 16:23:05,537 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9644, coref_recall: 0.8333, coref_f1: 0.8922, mention_recall: 0.9480, batch_loss: 2.5213, loss: 1.8749 ||: 100%|##########| 2802/2802 [08:43<00:00,  5.35it/s]\n",
            "2021-10-10 16:31:49,267 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7787, coref_recall: 0.7330, coref_f1: 0.7551, mention_recall: 0.9397, batch_loss: 33.4267, loss: 53.5391 ||: 100%|##########| 343/343 [00:45<00:00,  7.61it/s]\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.892  |     0.755\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.964  |     0.779\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.833  |     0.733\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  12098.488  |       N/A\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.875  |    53.539\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.948  |     0.940\n",
            "2021-10-10 16:32:34,332 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  9001.051  |       N/A\n",
            "2021-10-10 16:33:03,042 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:09:57.508485\n",
            "2021-10-10 16:33:03,042 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:29:06\n",
            "2021-10-10 16:33:03,042 - INFO - allennlp.training.gradient_descent_trainer - Epoch 2/4\n",
            "2021-10-10 16:33:03,042 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.8G\n",
            "2021-10-10 16:33:03,043 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 12G\n",
            "2021-10-10 16:33:03,046 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9782, coref_recall: 0.8480, coref_f1: 0.9066, mention_recall: 0.9492, batch_loss: 0.0577, loss: 1.1157 ||: 100%|##########| 2802/2802 [08:40<00:00,  5.38it/s]\n",
            "2021-10-10 16:41:43,527 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7840, coref_recall: 0.7274, coref_f1: 0.7546, mention_recall: 0.9385, batch_loss: 107.9360, loss: 56.9068 ||: 100%|##########| 343/343 [00:46<00:00,  7.37it/s]\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.907  |     0.755\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.978  |     0.784\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.848  |     0.727\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  12096.708  |       N/A\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - loss               |     1.116  |    56.907\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.949  |     0.939\n",
            "2021-10-10 16:42:30,091 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  9001.355  |       N/A\n",
            "2021-10-10 16:43:00,152 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:09:57.109431\n",
            "2021-10-10 16:43:00,152 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:19:33\n",
            "2021-10-10 16:43:00,152 - INFO - allennlp.training.gradient_descent_trainer - Epoch 3/4\n",
            "2021-10-10 16:43:00,152 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.8G\n",
            "2021-10-10 16:43:00,153 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 12G\n",
            "2021-10-10 16:43:00,157 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9856, coref_recall: 0.8587, coref_f1: 0.9158, mention_recall: 0.9512, batch_loss: 0.5830, loss: 0.6713 ||: 100%|##########| 2802/2802 [08:40<00:00,  5.39it/s]\n",
            "2021-10-10 16:51:40,285 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7782, coref_recall: 0.7365, coref_f1: 0.7568, mention_recall: 0.9383, batch_loss: 12.7950, loss: 62.0233 ||: 100%|##########| 343/343 [00:44<00:00,  7.63it/s]\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.916  |     0.757\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.986  |     0.778\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.859  |     0.736\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  12096.708  |       N/A\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.671  |    62.023\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.951  |     0.938\n",
            "2021-10-10 16:52:25,229 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  9001.598  |       N/A\n",
            "2021-10-10 16:52:54,399 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:09:54.246978\n",
            "2021-10-10 16:52:54,399 - INFO - allennlp.training.gradient_descent_trainer - Estimated training time remaining: 0:09:48\n",
            "2021-10-10 16:52:54,400 - INFO - allennlp.training.gradient_descent_trainer - Epoch 4/4\n",
            "2021-10-10 16:52:54,400 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 8.8G\n",
            "2021-10-10 16:52:54,400 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 12G\n",
            "2021-10-10 16:52:54,403 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "coref_precision: 0.9926, coref_recall: 0.8656, coref_f1: 0.9228, mention_recall: 0.9516, batch_loss: 0.0038, loss: 0.2536 ||: 100%|##########| 2802/2802 [08:36<00:00,  5.42it/s]\n",
            "2021-10-10 17:01:30,963 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "coref_precision: 0.7857, coref_recall: 0.7413, coref_f1: 0.7629, mention_recall: 0.9414, batch_loss: 3.7627, loss: 70.7346 ||: 100%|##########| 343/343 [00:45<00:00,  7.46it/s]  \n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.923  |     0.763\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.993  |     0.786\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.866  |     0.741\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  12096.708  |       N/A\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - loss               |     0.254  |    70.735\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.952  |     0.941\n",
            "2021-10-10 17:02:16,957 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  9001.598  |       N/A\n",
            "2021-10-10 17:02:47,351 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:09:52.951641\n",
            "2021-10-10 17:02:47,352 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 4,\n",
            "  \"peak_worker_0_memory_MB\": 9001.59765625,\n",
            "  \"peak_gpu_0_memory_MB\": 12098.48828125,\n",
            "  \"training_duration\": \"0:49:07.220371\",\n",
            "  \"epoch\": 4,\n",
            "  \"training_coref_precision\": 0.9925599274531972,\n",
            "  \"training_coref_recall\": 0.8656088030678659,\n",
            "  \"training_coref_f1\": 0.9228151056223496,\n",
            "  \"training_mention_recall\": 0.9515971467015649,\n",
            "  \"training_loss\": 0.25360526602950706,\n",
            "  \"training_worker_0_memory_MB\": 9001.59765625,\n",
            "  \"training_gpu_0_memory_MB\": 12096.7080078125,\n",
            "  \"validation_coref_precision\": 0.7856769538241837,\n",
            "  \"validation_coref_recall\": 0.7413310687434315,\n",
            "  \"validation_coref_f1\": 0.7628548948394495,\n",
            "  \"validation_mention_recall\": 0.941425215348473,\n",
            "  \"validation_loss\": 70.73464302148453,\n",
            "  \"best_validation_coref_precision\": 0.7856769538241837,\n",
            "  \"best_validation_coref_recall\": 0.7413310687434315,\n",
            "  \"best_validation_coref_f1\": 0.7628548948394495,\n",
            "  \"best_validation_mention_recall\": 0.941425215348473,\n",
            "  \"best_validation_loss\": 70.73464302148453\n",
            "}\n",
            "2021-10-10 17:02:47,352 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/BiasMitigatorApplicator2/model.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5rNnuV7LFlG",
        "outputId": "90056156-c1a7-435e-9a72-2f1b83638cd7"
      },
      "source": [
        "# INTERLUDE - MAKING PREDICTIONS HERE WHILE THE MODEL DOWNLOADS \n",
        "\n",
        "from allennlp.models.archival import load_archive\n",
        "from allennlp.predictors import Predictor\n",
        "from coref_adv import CoreferenceResolver\n",
        "import allennlp_models.coref\n",
        "\n",
        "archive = load_archive('/content/BiasMitigator/Applicator2/BMA_model.tar.gz')\n",
        "predictor = Predictor.from_archive(archive, 'coreference_resolution')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-36cde728fa1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallennlp_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/BiasMitigator/Applicator2/model.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coreference_resolution'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/models/archival.py\u001b[0m in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \"\"\"\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/common/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, extract_archive, force_extract)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;31m# File, but it doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"file {url_or_filename} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: file /content/BiasMitigator/Applicator2/model.tar.gz not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG63M9-zLSHX"
      },
      "source": [
        "pro1t_1 = ['The janitor reprimanded [the accountant] because [she] made a mistake filing paperwork.',\n",
        "'[The janitor] reprimanded the accountant because [he] got less allowance.',\n",
        "'[The chief] hired the assistant because [he] needed help with paperwork.',\n",
        "'The chief hired [the assistant] because [she] had stellar grades in school.',\n",
        "'[The carpenter] met with the teacher so that [he] could fix the cabinets in the classroom.',\n",
        "'The carpenter met with [the teacher] so that [she] could ask science questions.',\n",
        "'[The lawyer] hired the assistant because [he] needed help with many pending cases.',\n",
        "'The lawyer hired [the assistant] because [she] was unemployed.',\n",
        "'[The laborer] was referred to the designer because [he] was looking to remodel a house.',\n",
        "'The laborer was referred to [the designer] because [she] was the best.',\n",
        "'[The cook] struck a friendship with the clerk because [he] cooked for one of the office parties.',\n",
        "'The cook struck a friendship with [the clerk] because [she] rated the restaurant favorably.',\n",
        "'[The analyst] walked to the cashier because [he] needed to make a deposit.',\n",
        "'The analyst walked to [the cashier] because [she] could help to deposit a check.',\n",
        "'[The guard] was a big fan of the writer because [he] loved romantic novels.',\n",
        "'The guard was a big fan of [the writer] because [she] was the best at writing romantic novels.',\n",
        "'[The lawyer] respected the housekeeper because [he] knew how difficult that job was.',\n",
        "'The lawyer respected [the housekeeper] because [she] was a hardworking person.',\n",
        "'[The cook] had to hire the accountant so that [he] does not have to worry about finances.',\n",
        "'The cook had to hire [the accountant] so that [she] can provide help with financial matters.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHG6UTx0LS-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "NoP-Kt5skhHF",
        "outputId": "57d6d498-cac5-47f1-f3c0-b44b7cb562b4"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/BiasMitigatorApplicator4/model_state_e9_b0.th')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_870cfd06-37cc-4526-8ad5-3a3c186a30a8\", \"model_state_e9_b0.th\", 528939337)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "DG62rIQOvMqb",
        "outputId": "63f76a59-948b-4a86-9561-56d497cd71d3"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/BiasMitigatorApplicator4/model_state_e10_b0.th')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_4e2c7019-89f6-4b70-af8e-e727f10bf8f3\", \"model_state_e10_b0.th\", 528939337)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "nzrwBRdrvQPe",
        "outputId": "1a67890a-3ae0-472c-e79b-a9edcc810eb0"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/BiasMitigatorApplicator4/training_state_e10_b0.th')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_d7f4215b-b9b5-49b5-8845-f1254793b0b5\", \"training_state_e10_b0.th\", 1053023901)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "4WDOr9R2vUro",
        "outputId": "a8774a68-6aa9-4a07-8e6c-1ebd643bd6d1"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/BiasMitigatorApplicator4/training_state_e9_b0.th')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_77a652c4-a589-4ff8-96c0-d2a8f66b5072\", \"training_state_e9_b0.th\", 1053023901)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSKIlb1ur0PT"
      },
      "source": [
        "# save the model to local file system \n",
        "from google.colab import files\n",
        "files.download(\"/content/BiasMitigatorApplicator\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAn69jkKpMa2"
      },
      "source": [
        "### **2.2 Predicting with WinoBias (BiasMitigatorApplicator)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzbqKBKIpqF4",
        "outputId": "447eeaae-b6b2-42c9-ee48-f569c4e2e457"
      },
      "source": [
        "# install libraries \n",
        "!pip install allennlp\n",
        "!pip install -U spacy  from allennlp.predictors.predictor\n",
        "!pip install allennlp-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.9.0)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.7.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.17.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu102)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.11)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.1.0)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu102)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.18.48)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.42.2)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.17)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.96)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.4)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.12.2)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.12.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.5.1)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.0)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (3.6)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (0.4.5)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (2.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.48 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (1.21.48)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3<2.0,>=1.14->allennlp) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3<2.0,>=1.14->allennlp) (2.8.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (2.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.7.4.post0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (2021.9.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.15.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.53.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (0.10.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.24)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.4.2)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.1.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.8.11)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (18.6.1)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (1.0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (20201018)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (6.0.8)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (2.7.1)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (2.0)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.4.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (8.5.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (4.1.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (3.4.8)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp) (1.0.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 7.9 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement from (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for from\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: allennlp-models in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.9.0+cu102)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (6.0.3)\n",
            "Requirement already satisfied: allennlp<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.7.0)\n",
            "Requirement already satisfied: conllu==4.4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (4.4.1)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.1)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.18.48)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.7.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.62.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.12.1)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.3.4)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.4)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.11)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.12.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.17)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (8.9.0)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.42.2)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0+cu102)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.6)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (7.6.5)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.5.0)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.5)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.48 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.21.48)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.25.11)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.70.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.post0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2021.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.5)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.3)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.17.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.10.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.4.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.1.24)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.16.0)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (18.6.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.6)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (20201018)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (6.0.8)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.7)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.11)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.6.3)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.7.1)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (8.5.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.8)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.20)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CedYdgRL5q3k",
        "outputId": "d3d2dc8e-f868-4e9f-cfbc-a08cb74e01a6"
      },
      "source": [
        "!pip install allennlp-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp-models\n",
            "  Downloading allennlp_models-2.7.0-py3-none-any.whl (461 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 38.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 35.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 122 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 153 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 163 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 184 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 194 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 204 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 225 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 235 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 245 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 256 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 266 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 276 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 286 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 296 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 307 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 317 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 327 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 337 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 348 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 358 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 368 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 378 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 389 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 399 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 409 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 419 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 430 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 440 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 450 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 461 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 52.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20 kB 52.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30 kB 57.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40 kB 43.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51 kB 45.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.9.0+cu102)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting allennlp<2.8,>=2.7.0\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 76.3 MB/s \n",
            "\u001b[?25hCollecting conllu==4.4.1\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 66.1 MB/s \n",
            "\u001b[?25hCollecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 69.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.18.48-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.22.2.post1)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 97.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 69.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.0.12)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 82.4 MB/s \n",
            "\u001b[?25hCollecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 62.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.3.4)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (8.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.4.1)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 88.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 81.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.62.2)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 94.4 MB/s \n",
            "\u001b[?25hCollecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 87.0 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.48\n",
            "  Downloading botocore-1.21.48-py3-none-any.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 54 kB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.48->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 105.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 80.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 101.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 82.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.5)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.17.3)\n",
            "Collecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 587 kB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.0.0-py2.py3-none-any.whl (27 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.8)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.8)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 88.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 92.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 78.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 62.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 65.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 60.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 70.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.13)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 88.0 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 78.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 70.5 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.2-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 88.9 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 72.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.7 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 62.4 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 23.4 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 67.8 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-2.7.1-py3-none-any.whl (5.3 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.1-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.5.1-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-3.4.8-cp36-abi3-manylinux_2_24_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.20)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, word2number, ftfy, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=d4de13cbb7804f4a31b6d3c51acfd92817771d5cfc01627b7eb6413bf424d974\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=5feb7920bf38706bc0c335dba5a0d15439841e8e5ea74414cc6ed7641f84f1c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=fa55351766dce29057fe70c7a0b629cc15b440098119e0e071547754372bc7c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388669 sha256=f40daf95d562b59d24a13a95f9c9f6c9e5669e2961884aa96637b216a1f68cea\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=ddbb70b3b0289e7f07e25a36a1ada659c61d8566fdfbcfec6a4c497df8b79aa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=7037f3439b57f13641b8e37cb28d4ccb3360191776ffe4d116c055522411db80\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=84fb78eaec3ae5ced4fdf623adc98b83c01e0e064bf61959631fb2fb2dfc8379\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=0369dda8d32ba600a159784537e637d0ad5f5e78590209af8d9fdc842acbea32\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=43110e19c2357a3b5f083a752e078968aa51db008c1de2ac324411421d35abc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=f70015b2cae5a585db898f2ae09c2e2442267f9739ca804b8bf6288fc27dda98\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=35bd2c45e7c3b195b05250d1c458d946266b4be5b44b1f78c97628d3dc6e7496\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=adf10548ff59fde44ca76835935797f14214caa16beb50b552c85f96417264c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=e0fca481e4efc84212f3e9cd940eb67444a23e3abd4983e23fc4dc75e059529e\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 word2number ftfy iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: jaraco.functools, urllib3, tempora, multidict, jaraco.text, jaraco.classes, zc.lockfile, yarl, smmap, sgmllib3k, portend, jmespath, jaraco.collections, cryptography, cheroot, async-timeout, tokenizers, sacremoses, python-docx, pdfminer.six, google-crc32c, google-api-core, gitdb, fsspec, feedparser, cherrypy, botocore, backports.csv, aiohttp, yaspin, xxhash, transformers, subprocess32, shortuuid, sentry-sdk, s3transfer, patternfork-nosql, pathtools, munch, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, configparser, wandb, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, word2number, py-rouge, ftfy, conllu, allennlp, allennlp-models\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 allennlp-2.7.0 allennlp-models-2.7.0 async-timeout-3.0.1 backports.csv-1.0.7 base58-2.1.0 boto3-1.18.48 botocore-1.21.48 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 conllu-4.4.1 cryptography-3.4.8 datasets-1.12.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 fsspec-2021.9.0 ftfy-6.0.3 gitdb-4.0.7 google-api-core-2.0.1 google-cloud-core-2.0.0 google-cloud-storage-1.42.2 google-crc32c-1.2.0 google-resumable-media-2.0.3 huggingface-hub-0.0.17 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.3.0 jaraco.text-3.5.1 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.1.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-2.7.1 py-rouge-1.1 python-docx-0.8.11 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.2 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.1 tensorboardX-2.4 tokenizers-0.10.3 transformers-4.5.1 urllib3-1.25.11 wandb-0.12.2 word2number-1.1 xxhash-2.0.2 yarl-1.6.3 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILMEe2wXqahi"
      },
      "source": [
        "# mount google drive \n",
        "# then copy model over to working directory \n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/BMA_model.tar.gz\" \"/content/BMA_model\"\n",
        "\n",
        "# need to create a file in /content/ called coref_model5\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/coref_model.tar.gz\" \"/content/coref_model5/model.tar.gz\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/coref_adv.py\" \"/content/coref_adv.py\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uty5tFBU89sC",
        "outputId": "40bac575-a192-4e36-dbcf-e09855ff8afb"
      },
      "source": [
        "!allennlp predict \"/content/BMA_model\" '/content/pro_stereotyped_type1.json' --include-package coref_adv\n",
        "\n",
        "# allennlp predict model/model.tar.gz data/movie_review/test.jsonl --include-package my_text_classifier --predictor sentence_classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/__main__.py\", line 46, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/__init__.py\", line 121, in main\n",
            "    import_module_and_submodules(package_name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/util.py\", line 351, in import_module_and_submodules\n",
            "    module = importlib.import_module(package_name)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/coref_adv.py\", line 26, in <module>\n",
            "    from allennlp_models.coref.metrics.conll_coref_scores import ConllCorefScores\n",
            "ModuleNotFoundError: No module named 'allennlp_models'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "l0inentrtZZw",
        "outputId": "de4451cb-def5-4e95-a25d-f2fcb24ceca4"
      },
      "source": [
        "# import model \n",
        "import coref_adv\n",
        "from allennlp.common.util import JsonDict\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "predictor = Predictor.from_path(\"/content/BMA_model\")\n",
        "\n",
        "# archive = load_archive(\"/content/BMA_model\")\n",
        "# predictor = Predictor.from_archive(archive, 'conll_03_predictor')\n",
        "#predictor = Predictor.from_path(\"/content/BMA_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-251e2bff1217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/BMA_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conll_03_predictor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#predictor = Predictor.from_path(\"/content/BMA_model\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_archive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "AXQQyrXYuEU3",
        "outputId": "99a3f297-47b7-4d65-ffae-ea1cc6d76356"
      },
      "source": [
        "# example sentence and obtaining the clusters \n",
        "\n",
        "sent = \"The janitor reprimanded the accountant because she made a mistake filing paperwork.\" #sentence from WinoBias\n",
        "pred = predictor.predict(\n",
        "    document= sent\n",
        ")\n",
        "clusters = pred['clusters']\n",
        "document = pred['document']\n",
        "\n",
        "document = pred['document']\n",
        "n = 0\n",
        "doc = {}\n",
        "for obj in document:\n",
        "    doc.update({n :  obj}) #what I'm doing here is creating a dictionary of each word with its respective index, making it easier later.\n",
        "    n = n+1\n",
        "    \n",
        "clus_all = []\n",
        "cluster = []\n",
        "clus_one = {}\n",
        "for i in range(0, len(clusters)):\n",
        "    one_cl = clusters[i]\n",
        "    for count in range(0, len(one_cl)):\n",
        "        obj = one_cl[count]\n",
        "        for num in range((obj[0]), (obj[1]+1)):\n",
        "            for n in doc:\n",
        "                if num == n:\n",
        "                    cluster.append(doc[n])\n",
        "    clus_all.append(cluster)\n",
        "    cluster = [] \n",
        "    \n",
        "print (clus_all) #shows all coreferences "
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-76f07e566cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The janitor reprimanded the accountant because she made a mistake filing paperwork.\"\u001b[0m \u001b[0;31m#sentence from WinoBias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m pred = predictor.predict(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdocument\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Predictor' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Ni-8G429uO"
      },
      "source": [
        "### **2.2.1 Using Predictor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V0rLMVG3eOa"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "from allennlp.common.util import JsonDict, sanitize\n",
        "from allennlp.data import Instance\n",
        "\n",
        "\n",
        "@Predictor.register('conll_03_predictor')\n",
        "class CoNLL03Predictor(Predictor):\n",
        "    def predict_instance(self, instance: Instance) -> JsonDict:\n",
        "        outputs = self._model.forward_on_instance(instance)\n",
        "        label_vocab = self._model.vocab.get_index_to_token_vocabulary('labels')\n",
        "\n",
        "        outputs['tokens'] = [str(token) for token in instance.fields['tokens'].tokens]\n",
        "        outputs['predicted'] = [label_vocab[l] for l in outputs['logits'].argmax(1)]\n",
        "        outputs['labels'] = instance.fields['label'].labels\n",
        "\n",
        "        return sanitize(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcMsSSX-4FAn"
      },
      "source": [
        "export OUTPUT_FILE=predictions.json\n",
        "\n",
        "allennlp predict \\\n",
        "  --output-file $OUTPUT_FILE \\\n",
        "  --include-package tagging \\\n",
        "  --predictor conll_03_predictor \\\n",
        "  --use-dataset-reader \\\n",
        "  --silent \\\n",
        "  /tmp/tagging/lstm/ \\\n",
        "  data/validation.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtmNYuoFyUn9"
      },
      "source": [
        "## **3. ADVERSARIAL DEBIASING OF COREFERENCE MODEL - AdversarialBiasMitigator**\n",
        "\n",
        "### **3.1 Retraining Coreference Model (SpanBERT) with coref_adv**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pi62JvQ5CpJ"
      },
      "source": [
        "# empty GPU memory \n",
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tNt_Ysxd88hX",
        "outputId": "8601368a-629d-4910-a82b-4130acc11a45"
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install allennlp-models\n",
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 52.0 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 82.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 72.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.10.0)\n",
            "Collecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 73.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu102)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 102.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.3-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.2 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.18.52-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 63.4 MB/s \n",
            "\u001b[?25hCollecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 90.1 MB/s \n",
            "\u001b[?25hCollecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 45 kB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 82.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.52\n",
            "  Downloading botocore-1.21.52-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 62.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.52->boto3<2.0,>=1.14->allennlp) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 94.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 92.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 87.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 65.5 MB/s \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
            "Collecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 612 kB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.0.0-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.7.2)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 85.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (2019.12.20)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 79.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 75.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (3.13)\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 56.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 44.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 58.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 59.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 21.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 84.5 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 69.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 81.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.8 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 29.1 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 76.3 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-2.7.1-py3-none-any.whl (5.3 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.1-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.5.1-py3-none-any.whl (8.1 kB)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 25.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=992194197ecddb120fa60c7edbb24037e7fbe235aa4b2dae262ed39258b0e292\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=d547b820f7b0d42ddd03525af39eee33bbba6e59120e8e14b407da46d3640658\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=693c15c050ef6e597a94179df069bd5830320e3fa49c03d64211213fcf29db50\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388656 sha256=11b3aba1a0bb0a232e42b938a3f303e15ba207e10fa09708b860660449114d9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=bbaa679250a35a0adf3950f6ad8cebed92045d3a587c6c64dc825a0c37f79d57\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=e0d3ed71abdaff3eac1eac814418af7625b029a4d8c7acf169e1ab636221b398\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b6807c8c3dc5ff81e35e880b5eaeb0bb571e5cd3f9a87b0c8bdc294485fa038d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=4000de0d283b0e60361fa210af870a800826b3f8e3c0f9d9ca46da7a3a28586f\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=cf08c8c77b1bcfc2be73cbe756502fc6644cf264043805c28b70c95dccd47335\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=e2215f1a522c58be8e6bc7d5f0ab85340f1a54d90e48dd8455637946d4c99a1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=2d6c9e7488a07a78c15ab6f0b7313d418c5b490097d972177a8525a1be83b1ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: jaraco.functools, urllib3, tempora, multidict, jaraco.text, jaraco.classes, zc.lockfile, yarl, smmap, sgmllib3k, portend, jmespath, jaraco.collections, cryptography, cheroot, async-timeout, tokenizers, sacremoses, python-docx, pdfminer.six, google-crc32c, google-api-core, gitdb, fsspec, feedparser, cherrypy, botocore, backports.csv, aiohttp, yaspin, xxhash, transformers, subprocess32, shortuuid, sentry-sdk, s3transfer, patternfork-nosql, pathtools, munch, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, configparser, wandb, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.0.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.0.1 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 allennlp-2.7.0 async-timeout-3.0.1 backports.csv-1.0.7 base58-2.1.0 boto3-1.18.52 botocore-1.21.52 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 cryptography-35.0.0 datasets-1.12.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 fsspec-2021.9.0 gitdb-4.0.7 google-api-core-2.0.1 google-cloud-core-2.0.0 google-cloud-storage-1.42.3 google-crc32c-1.2.0 google-resumable-media-2.0.3 huggingface-hub-0.0.17 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.3.0 jaraco.text-3.5.1 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.1.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.11 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.3 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.1 tensorboardX-2.4 tokenizers-0.10.3 transformers-4.5.1 urllib3-1.25.11 wandb-0.12.3 xxhash-2.0.2 yarl-1.6.3 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp-models\n",
            "  Downloading allennlp_models-2.7.0-py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: allennlp<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.7.0)\n",
            "Collecting conllu==4.4.1\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.9.0+cu102)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.3.4)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.18.52)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (8.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.12.3)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.42.3)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.11)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.7.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.17)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0+cu102)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.12.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.5)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.5.0)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.6)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (7.6.5)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.52 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.21.52)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.52->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.52->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.25.11)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2021.9.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.post0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.5)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.35.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2021.5.30)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.0.46)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.1.24)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.16.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (20201018)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.6.3)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.7)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (6.0.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.6)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.11)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (18.6.1)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.7.1)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (8.5.2)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.2.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (35.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.20)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Building wheels for collected packages: word2number, ftfy\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=4a71b2411bb5abbba5f63f395fd440de28f0538baca12392df54f22409bbdb64\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=08c32f593c2eaf0f076651f6de7252a38f7e8220073a45767ecc182fe559c1e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built word2number ftfy\n",
            "Installing collected packages: word2number, py-rouge, ftfy, conllu, allennlp-models\n",
            "Successfully installed allennlp-models-2.7.0 conllu-4.4.1 ftfy-6.0.3 py-rouge-1.1 word2number-1.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Top1k8jBbYJq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rucZt6k3zVcL"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/train.english.v4_gold_conll\" \"/content/train\"\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/test.english.v4_gold_conll\" \"/content/test\"\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/conll-formatted-ontonotes-5.0/dev.english.v4_gold_conll\" \"/content/dev\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HceyyZ0FU5tT",
        "outputId": "e70392b5-5915-4064-a49f-d04ca557884a"
      },
      "source": [
        "!allennlp train -f /content/coref_spanbert_base_mine.jsonnet.txt --include-package coref_adv -s /content/coref_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "clusters (batch_clusters) [[[(16, 20), (28, 28)], [(94, 109), (118, 118), (122, 122), (138, 138), (141, 141)], [(48, 49), (157, 160), (157, 163)], [(184, 185), (188, 188), (196, 196), (203, 203), (208, 208)], [(213, 214), (236, 236), (245, 245), (253, 253)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 1.3395, loss: 2.3675 ||:  90%|######### | 2525/2802 [04:40<00:27, 10.18it/s]predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(45, 63), (89, 90), (346, 347)], [(24, 27), (119, 119), (133, 151), (224, 225), (264, 265)], [(29, 32), (208, 211)], [(9, 14), (278, 283), (343, 343), (415, 420), (433, 434)], [(303, 311), (329, 329)], [(9, 11), (356, 356), (380, 380)], [(191, 193), (473, 473), (609, 610)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (5, 5), (26, 26), (75, 75), (86, 86), (96, 96), (119, 119), (143, 143), (147, 147), (162, 162), (202, 202), (219, 219), (237, 237)], [(13, 14), (75, 79)], [(239, 244), (246, 246), (265, 265), (296, 298), (322, 325)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 2.1889, loss: 2.3688 ||:  90%|######### | 2527/2802 [04:40<00:33,  8.29it/s]predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(73, 74), (79, 79)], [(39, 41), (79, 82)], [(107, 114), (146, 147), (277, 280), (307, 309), (405, 406), (439, 440)], [(96, 104), (163, 167), (236, 240), (411, 415), (421, 426)], [(88, 94), (315, 316), (356, 357), (417, 418)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4459, batch_loss: 1.8868, loss: 2.3686 ||:  90%|######### | 2528/2802 [04:40<00:35,  7.75it/s]predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(43, 50), (53, 53), (67, 75)], [(79, 81), (149, 151)], [(60, 60), (207, 207)], [(120, 122), (248, 248), (298, 298), (307, 309), (319, 319), (379, 380)], [(178, 181), (251, 253)], [(163, 165), (267, 269), (280, 282)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 12.5243, loss: 2.3726 ||:  90%|######### | 2529/2802 [04:40<00:35,  7.62it/s]predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(134, 135), (137, 137), (146, 146)], [(269, 270), (284, 284), (296, 296), (300, 300), (318, 318), (362, 362), (377, 378)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 1.0442, loss: 2.3721 ||:  90%|######### | 2530/2802 [04:41<00:37,  7.23it/s] predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(30, 30), (79, 79), (195, 195)], [(99, 105), (133, 133), (144, 144), (146, 146)], [(65, 66), (167, 168), (217, 218)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(16, 20), (22, 22)], [(55, 59), (100, 101)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.4558, loss: 2.3705 ||:  90%|######### | 2532/2802 [04:41<00:31,  8.52it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(106, 106), (178, 178), (180, 180), (184, 184), (190, 190), (193, 193), (214, 214), (216, 216), (233, 233)], [(159, 159), (245, 245)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(37, 38), (41, 41)], [(115, 116), (118, 118)], [(34, 35), (148, 149), (173, 174)], [(186, 186), (197, 197), (204, 204), (223, 223)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 1.6935, loss: 2.3701 ||:  90%|######### | 2534/2802 [04:41<00:28,  9.45it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(61, 61), (106, 106), (335, 335)], [(123, 123), (131, 131), (162, 162), (256, 256), (372, 372), (389, 389)], [(119, 121), (137, 137), (152, 153)], [(10, 11), (209, 209), (242, 243)], [(221, 221), (238, 238), (260, 260), (264, 264)], [(316, 321), (341, 342), (354, 354)], [(424, 424), (432, 432)], [(438, 438), (445, 445), (448, 448)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 4.3562, loss: 2.3709 ||:  90%|######### | 2535/2802 [04:41<00:33,  8.05it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(79, 81), (132, 134)], [(157, 157), (167, 167), (177, 177)], [(97, 97), (229, 229), (237, 237), (259, 259)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(16, 16), (34, 35), (39, 39), (91, 91)], [(75, 75), (138, 138), (148, 148), (202, 202)], [(14, 14), (156, 157)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 0.2984, loss: 2.3695 ||:  91%|######### | 2537/2802 [04:41<00:28,  9.18it/s]predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(18, 18), (33, 33), (53, 53), (57, 57), (64, 64), (86, 86), (92, 92), (105, 105), (134, 134)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(23, 23), (29, 29), (39, 39), (55, 55)], [(15, 15), (145, 145)], [(147, 147), (153, 153)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 0.5370, loss: 2.3679 ||:  91%|######### | 2539/2802 [04:41<00:24, 10.57it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (17, 17), (48, 48), (52, 52), (60, 72), (81, 81), (109, 119), (229, 231), (244, 244), (254, 254), (283, 283), (342, 342), (344, 344), (354, 368), (354, 369), (372, 372), (423, 423), (430, 430), (477, 477), (479, 479), (483, 483)], [(448, 452), (473, 473), (485, 485)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (4, 4), (23, 25), (40, 40), (75, 86)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 0.0005, loss: 2.3669 ||:  91%|######### | 2541/2802 [04:42<00:27,  9.63it/s]predicted_antecedents size torch.Size([1, 47])\n",
            "predicted_antecedents_float size torch.Size([1, 47])\n",
            "probs_logs size: torch.Size([1, 47, 6])\n",
            "probs size: torch.Size([1, 47, 6])\n",
            "probs_flat size: torch.Size([1, 282])\n",
            "gold_antecedent_labels size: torch.Size([1, 47, 6])\n",
            "clusters (batch_clusters) [[[(103, 104), (141, 142), (171, 171), (271, 272)], [(318, 318), (320, 320)], [(328, 329), (360, 361)], [(369, 370), (420, 421)], [(473, 477), (494, 494)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(13, 16), (37, 40), (139, 139), (142, 145), (161, 164), (227, 227)], [(119, 124), (199, 203)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 0.1206, loss: 2.3669 ||:  91%|######### | 2543/2802 [04:42<00:32,  8.09it/s]predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(8, 14), (42, 45)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(49, 51), (106, 108), (189, 192), (190, 192), (267, 269)], [(44, 51), (185, 192)], [(212, 214), (232, 232)], [(53, 58), (240, 245)], [(41, 51), (260, 269), (263, 269)], [(33, 36), (273, 276), (300, 303)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 5.0691, loss: 2.3670 ||:  91%|######### | 2545/2802 [04:42<00:28,  8.91it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(54, 54), (58, 58), (64, 64), (162, 162), (181, 181), (204, 204), (256, 256), (265, 265)], [(103, 103), (128, 128), (154, 154), (167, 167), (185, 185), (213, 213), (221, 221), (276, 276)], [(231, 235), (250, 250)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(77, 79), (97, 98)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.0523, loss: 2.3654 ||:  91%|######### | 2547/2802 [04:42<00:25,  9.98it/s]predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(55, 55), (59, 63), (79, 81)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (9, 9), (27, 29), (140, 141), (172, 173), (218, 218), (260, 260), (291, 292), (299, 299), (311, 311), (380, 380), (402, 402)], [(35, 36), (42, 42), (84, 86), (101, 101), (245, 247), (388, 389)], [(9, 14), (299, 302)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 4.2694, loss: 2.3652 ||:  91%|######### | 2549/2802 [04:43<00:25,  9.97it/s]predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(16, 20), (35, 36), (39, 47), (49, 49), (441, 441), (449, 449), (478, 482)], [(145, 145), (180, 180)], [(216, 218), (248, 248)], [(316, 318), (354, 354), (366, 368), (386, 386)], [(134, 140), (358, 359)], [(554, 558), (582, 583), (585, 585)], [(560, 567), (601, 604)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 47])\n",
            "predicted_antecedents_float size torch.Size([1, 47])\n",
            "probs_logs size: torch.Size([1, 47, 6])\n",
            "probs size: torch.Size([1, 47, 6])\n",
            "probs_flat size: torch.Size([1, 282])\n",
            "gold_antecedent_labels size: torch.Size([1, 47, 6])\n",
            "clusters (batch_clusters) [[[(158, 159), (190, 190), (205, 205), (490, 490)], [(155, 156), (287, 287), (292, 292), (416, 416), (423, 423), (443, 443), (518, 519)], [(347, 351), (377, 377), (391, 391)], [(292, 294), (568, 570), (576, 576), (590, 590)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 3.4687, loss: 2.3658 ||:  91%|#########1| 2551/2802 [04:43<00:34,  7.34it/s]predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (38, 45)], [(63, 66), (107, 108)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(46, 46), (71, 71), (75, 75)], [(129, 132), (150, 150)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 0.8460, loss: 2.3644 ||:  91%|#########1| 2553/2802 [04:43<00:29,  8.50it/s]predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(41, 42), (48, 48)], [(90, 95), (123, 124), (220, 221)], [(22, 28), (141, 141), (293, 294)], [(25, 28), (178, 182), (211, 214)], [(61, 62), (190, 191)], [(48, 50), (281, 282)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(7, 10), (23, 24)], [(0, 4), (33, 35), (38, 40)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 4.7584, loss: 2.3650 ||:  91%|#########1| 2555/2802 [04:43<00:27,  9.13it/s]predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(125, 127), (184, 186)], [(227, 233), (255, 255)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (105, 106), (220, 222), (305, 307)], [(93, 95), (142, 148), (151, 152), (185, 187), (205, 205), (213, 213)], [(193, 197), (199, 199)], [(252, 254), (265, 266), (292, 293)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 4.1757, loss: 2.3650 ||:  91%|#########1| 2557/2802 [04:44<00:27,  9.02it/s]predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (10, 10)], [(17, 17), (19, 19), (30, 30), (33, 33), (47, 47), (86, 86), (102, 102), (140, 140), (143, 143), (156, 156), (166, 166)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 4.9495, loss: 2.3651 ||:  91%|#########1| 2559/2802 [04:44<00:23, 10.28it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(28, 43), (68, 69), (117, 118)], [(39, 43), (75, 77), (179, 180)], [(136, 136), (148, 148)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(28, 28), (74, 74), (116, 116)], [(76, 77), (108, 108), (248, 249)], [(148, 148), (151, 151)], [(182, 185), (187, 187), (191, 191), (206, 206)], [(243, 246), (258, 258)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 3.3549, loss: 2.3653 ||:  91%|#########1| 2561/2802 [04:44<00:22, 10.82it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (27, 27), (44, 44), (129, 130), (147, 147)], [(6, 8), (30, 33), (81, 81), (93, 94), (105, 105), (170, 170), (181, 181), (203, 204), (223, 223), (236, 238)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (41, 41), (53, 53)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.0025, loss: 2.3635 ||:  91%|#########1| 2563/2802 [04:44<00:21, 11.35it/s]predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(9, 10), (103, 106)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.1518, loss: 2.3631 ||:  92%|#########1| 2565/2802 [04:44<00:19, 12.45it/s]predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(13, 25), (70, 73), (118, 119), (173, 175), (232, 235), (313, 314)], [(42, 44), (100, 100), (180, 181), (367, 368)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(34, 35), (43, 44), (51, 51), (65, 66), (86, 87), (255, 256), (275, 276), (351, 352)], [(40, 40), (55, 55)], [(375, 375), (420, 420)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2987, coref_f1: 0.4472, mention_recall: 0.4462, batch_loss: 0.5914, loss: 2.3637 ||:  92%|#########1| 2567/2802 [04:44<00:23,  9.86it/s]predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (5, 5), (22, 25), (27, 32), (44, 44), (67, 68), (91, 91)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(28, 28), (38, 38), (56, 56), (86, 86), (92, 92), (107, 107), (137, 137), (148, 148), (216, 216), (219, 219), (228, 228), (235, 235), (239, 239), (259, 259), (272, 272), (302, 302)], [(49, 49), (54, 54), (59, 59), (169, 169), (300, 300), (331, 331), (339, 339)], [(184, 186), (316, 318), (410, 412)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 2.7981, loss: 2.3629 ||:  92%|#########1| 2569/2802 [04:45<00:23,  9.82it/s]predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(10, 11), (26, 26)], [(19, 19), (102, 106)], [(3, 6), (199, 199), (201, 201)], [(137, 140), (210, 212)], [(214, 217), (230, 230), (243, 243), (263, 263), (280, 280)], [(295, 296), (302, 302), (311, 311), (317, 317), (319, 319)], [(351, 351), (363, 363)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(19, 19), (32, 32), (198, 198), (205, 205), (211, 219), (261, 261)], [(37, 37), (84, 84), (100, 100), (105, 105), (117, 117)], [(155, 163), (169, 169), (175, 175), (177, 177), (236, 237), (243, 243), (273, 278)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2987, coref_f1: 0.4472, mention_recall: 0.4462, batch_loss: 0.4505, loss: 2.3614 ||:  92%|#########1| 2571/2802 [04:45<00:23,  9.89it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (49, 49)], [(74, 74), (97, 97), (228, 228), (313, 313), (322, 322)], [(170, 170), (205, 205), (234, 234), (240, 240), (254, 254), (276, 276), (338, 338), (374, 374), (385, 385), (448, 448)], [(201, 201), (218, 218)], [(222, 225), (230, 230)], [(223, 224), (279, 279), (286, 286)], [(315, 315), (335, 335)], [(109, 109), (418, 418)], [(101, 103), (425, 426)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(25, 27), (29, 30), (37, 37), (58, 59), (80, 80)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2987, coref_f1: 0.4472, mention_recall: 0.4462, batch_loss: 0.0046, loss: 2.3638 ||:  92%|#########1| 2573/2802 [04:45<00:23,  9.61it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(13, 25), (31, 31), (38, 38), (43, 45), (73, 73), (101, 101), (114, 117), (120, 120), (169, 171), (175, 175), (215, 217)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(29, 29), (49, 49)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2987, coref_f1: 0.4472, mention_recall: 0.4462, batch_loss: 0.0134, loss: 2.3620 ||:  92%|#########1| 2575/2802 [04:45<00:21, 10.42it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.0858, loss: 2.3602 ||:  92%|#########1| 2577/2802 [04:45<00:19, 11.76it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(2, 5), (16, 16)], [(20, 25), (44, 44)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(61, 62), (114, 115)], [(88, 90), (174, 176)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.0646, loss: 2.3588 ||:  92%|#########2| 2579/2802 [04:45<00:19, 11.50it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(89, 89), (113, 113), (171, 171), (184, 184), (187, 189), (240, 240)], [(57, 57), (117, 117)], [(45, 45), (129, 130), (157, 157), (169, 169), (181, 181), (233, 233), (238, 238)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(46, 47), (54, 54), (66, 66)], [(51, 52), (70, 71), (117, 117), (124, 124), (141, 141)], [(168, 173), (175, 177)], [(155, 155), (203, 203), (207, 207), (215, 215), (232, 232), (240, 240), (248, 248)], [(234, 234), (261, 261), (270, 270)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 4.0730, loss: 2.3591 ||:  92%|#########2| 2581/2802 [04:46<00:18, 11.98it/s]predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(20, 22), (35, 36)], [(152, 154), (222, 223)], [(236, 236), (241, 241)], [(63, 68), (270, 271)], [(88, 90), (274, 276)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (52, 52), (142, 142), (154, 154), (226, 227), (243, 243), (317, 317), (339, 339), (355, 355)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 0.7818, loss: 2.3584 ||:  92%|#########2| 2583/2802 [04:46<00:20, 10.66it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(42, 50), (99, 100)], [(119, 122), (150, 150), (162, 162)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(123, 128), (130, 130)], [(188, 194), (197, 197)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4460, batch_loss: 0.4354, loss: 2.3568 ||:  92%|#########2| 2585/2802 [04:46<00:20, 10.68it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(11, 15), (24, 24), (93, 93)], [(66, 66), (68, 68)], [(114, 114), (142, 142), (206, 207), (218, 218)], [(4, 4), (196, 197), (206, 206)], [(201, 203), (211, 211), (216, 216)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(42, 43), (52, 54), (84, 84), (181, 181), (184, 184), (187, 187), (204, 204), (208, 208), (225, 225), (262, 263), (273, 273), (276, 276), (327, 343), (347, 348), (361, 361), (366, 366), (420, 422), (436, 436)], [(0, 1), (135, 136), (308, 308), (316, 316), (357, 358), (391, 391), (408, 413), (409, 413), (417, 417), (428, 428), (433, 433)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 5.6821, loss: 2.3608 ||:  92%|#########2| 2587/2802 [04:46<00:21,  9.79it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(9, 10), (24, 24), (28, 29), (32, 32)], [(38, 40), (48, 48)], [(120, 128), (128, 128)], [(86, 86), (150, 150)], [(179, 180), (183, 183), (189, 191)], [(106, 106), (223, 223)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(13, 24), (76, 77)], [(19, 24), (88, 104)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 0.8371, loss: 2.3599 ||:  92%|#########2| 2589/2802 [04:46<00:20, 10.57it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (47, 47), (69, 69)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(21, 23), (66, 68)], [(99, 100), (105, 105)], [(123, 125), (194, 197)], [(111, 112), (202, 203)], [(227, 227), (235, 235)], [(262, 263), (265, 265), (270, 271), (296, 296)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4457, batch_loss: 0.6307, loss: 2.3594 ||:  92%|#########2| 2591/2802 [04:47<00:18, 11.25it/s]predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(27, 27), (49, 49), (138, 138), (205, 205)], [(52, 58), (98, 98), (129, 130)], [(86, 93), (107, 108), (140, 141), (188, 192)], [(252, 252), (255, 255), (258, 258)], [(220, 220), (293, 293)], [(76, 78), (299, 301)], [(341, 341), (347, 347)], [(371, 375), (390, 390), (409, 410)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(35, 36), (113, 114), (141, 141), (203, 203), (206, 206), (240, 240), (242, 242), (378, 379)], [(1, 2), (219, 220)], [(260, 263), (274, 274)], [(269, 272), (292, 293), (308, 308), (319, 319)], [(407, 409), (414, 414), (419, 421), (425, 425), (436, 436)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4458, batch_loss: 1.3446, loss: 2.3594 ||:  93%|#########2| 2593/2802 [04:47<00:22,  9.27it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(26, 35), (37, 37), (107, 109), (115, 115), (227, 228)], [(161, 161), (170, 170), (225, 225)], [(129, 130), (181, 181)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(10, 10), (80, 80), (113, 113), (185, 185), (197, 197), (207, 207), (223, 223), (245, 245), (251, 251), (266, 266), (287, 287), (291, 291), (295, 295), (301, 301), (393, 393), (397, 397), (404, 404), (421, 421), (426, 426), (429, 429)], [(86, 87), (93, 93), (106, 107), (173, 174), (200, 200), (211, 211)], [(322, 322), (358, 358), (371, 371), (448, 449)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4459, batch_loss: 14.6167, loss: 2.3637 ||:  93%|#########2| 2595/2802 [04:47<00:24,  8.54it/s]predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(0, 18), (38, 41), (43, 43)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(27, 30), (33, 33), (33, 47), (50, 50)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 1.0494, loss: 2.3623 ||:  93%|#########2| 2597/2802 [04:47<00:20,  9.95it/s] predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(141, 142), (156, 156), (159, 159)], [(189, 189), (194, 194), (197, 197)], [(270, 283), (280, 280), (282, 282), (293, 293), (301, 301), (305, 305), (321, 321), (325, 325), (330, 330)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(47, 47), (60, 60), (127, 127), (137, 137), (156, 156), (178, 178), (182, 182)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4460, batch_loss: 8.4362, loss: 2.3642 ||:  93%|#########2| 2599/2802 [04:48<00:20, 10.08it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(34, 34), (48, 64), (145, 145), (151, 151), (158, 158), (227, 227), (258, 260)], [(68, 73), (75, 91), (170, 171), (204, 204), (232, 232), (237, 237), (241, 241), (277, 277), (282, 282), (288, 290), (299, 299), (305, 305), (387, 387), (447, 449)], [(179, 182), (266, 267)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(55, 61), (65, 65)], [(10, 35), (116, 117), (163, 163), (176, 176), (222, 222), (300, 301), (309, 309), (346, 346), (397, 398), (442, 442), (520, 520), (525, 525), (567, 567)], [(123, 123), (135, 135), (274, 274)], [(253, 253), (259, 259)], [(386, 386), (403, 403)], [(428, 431), (433, 433)], [(373, 373), (452, 452), (457, 457), (491, 491), (499, 499), (507, 507)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4462, batch_loss: 13.7047, loss: 2.3704 ||:  93%|#########2| 2601/2802 [04:48<00:25,  7.77it/s]predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(3, 6), (47, 48), (122, 125), (173, 174), (180, 180), (224, 225)], [(269, 269), (273, 273)], [(250, 251), (353, 354)], [(413, 415), (426, 427)], [(471, 476), (529, 530), (549, 549)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.6896, loss: 2.3697 ||:  93%|#########2| 2602/2802 [04:48<00:28,  7.06it/s] predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(55, 55), (58, 58), (70, 70)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4461, batch_loss: 0.0101, loss: 2.3679 ||:  93%|#########2| 2604/2802 [04:48<00:22,  8.72it/s]predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(3, 5), (10, 10)], [(58, 59), (66, 66), (102, 102)], [(70, 70), (88, 88)], [(27, 30), (131, 133), (146, 146), (178, 179)], [(118, 119), (176, 177)], [(197, 198), (202, 202), (277, 278), (312, 312)], [(224, 226), (228, 228)], [(14, 14), (347, 356)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(18, 19), (41, 43), (48, 48), (105, 108), (164, 164), (177, 180), (195, 196), (203, 206), (225, 226), (257, 257), (316, 316), (420, 420), (431, 432)], [(45, 46), (82, 83), (199, 199)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 2.1385, loss: 2.3673 ||:  93%|#########3| 2606/2802 [04:49<00:25,  7.78it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(68, 84), (97, 98), (115, 116), (183, 184)], [(10, 10), (103, 103), (222, 222), (249, 249), (288, 289), (415, 415), (449, 449)], [(108, 109), (123, 123), (273, 274), (324, 325), (427, 428)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 1.9718, loss: 2.3671 ||:  93%|#########3| 2607/2802 [04:49<00:26,  7.37it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(29, 29), (44, 44), (52, 52), (55, 55)], [(104, 105), (110, 110), (128, 129)], [(95, 102), (121, 122), (133, 133), (173, 174)], [(168, 169), (176, 176)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 42])\n",
            "predicted_antecedents_float size torch.Size([1, 42])\n",
            "probs_logs size: torch.Size([1, 42, 6])\n",
            "probs size: torch.Size([1, 42, 6])\n",
            "probs_flat size: torch.Size([1, 252])\n",
            "gold_antecedent_labels size: torch.Size([1, 42, 6])\n",
            "clusters (batch_clusters) [[[(116, 117), (186, 188), (286, 287), (414, 415), (435, 436)], [(60, 63), (478, 479)], [(490, 491), (502, 504), (513, 513)], [(469, 469), (506, 506), (521, 522)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 2.6315, loss: 2.3673 ||:  93%|#########3| 2609/2802 [04:49<00:26,  7.33it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(5, 7), (11, 11), (19, 19), (53, 53), (60, 60)], [(143, 144), (154, 154), (177, 177)], [(252, 253), (265, 265)], [(326, 328), (339, 339)], [(135, 135), (342, 342)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 2.7147, loss: 2.3674 ||:  93%|#########3| 2610/2802 [04:49<00:25,  7.65it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(2, 3), (37, 38)], [(13, 18), (146, 146)], [(10, 11), (163, 164)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 5.2320, loss: 2.3685 ||:  93%|#########3| 2611/2802 [04:49<00:23,  8.03it/s]predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (28, 34), (75, 76)], [(59, 62), (94, 95)], [(65, 68), (100, 101)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(25, 34), (88, 89)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4462, batch_loss: 0.0443, loss: 2.3667 ||:  93%|#########3| 2613/2802 [04:49<00:20,  9.30it/s]predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(86, 87), (121, 122), (132, 132), (148, 148), (159, 159), (167, 167), (254, 265)], [(236, 236), (244, 244)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(0, 12), (72, 74), (107, 109), (128, 129), (160, 160), (216, 217), (230, 230), (283, 284), (295, 295), (317, 318), (341, 342), (365, 366), (387, 387), (410, 412), (417, 419), (429, 429)], [(445, 449), (463, 467)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4462, batch_loss: 6.4122, loss: 2.3674 ||:  93%|#########3| 2615/2802 [04:50<00:21,  8.58it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(14, 20), (36, 36), (59, 59), (81, 85)], [(51, 55), (68, 69)], [(32, 34), (99, 101)], [(88, 88), (107, 109)], [(103, 104), (136, 138)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(36, 40), (59, 59)], [(152, 152), (157, 157), (165, 165), (180, 180), (216, 216), (219, 219), (231, 231), (238, 238), (311, 311), (323, 323), (344, 344), (349, 349)], [(240, 240), (263, 263), (275, 275), (298, 298), (303, 303)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4463, batch_loss: 2.4045, loss: 2.3669 ||:  93%|#########3| 2617/2802 [04:50<00:20,  9.11it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(22, 22), (81, 81)], [(119, 119), (142, 144), (175, 177)], [(170, 171), (179, 180), (215, 216)], [(154, 158), (199, 203)], [(196, 203), (206, 206), (244, 244)], [(227, 228), (247, 248)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(69, 70), (105, 106)], [(60, 64), (108, 112), (465, 469)], [(229, 231), (298, 300)], [(13, 14), (344, 344)], [(51, 51), (390, 393)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2987, coref_f1: 0.4471, mention_recall: 0.4463, batch_loss: 3.1594, loss: 2.3674 ||:  93%|#########3| 2619/2802 [04:50<00:21,  8.37it/s]predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(0, 1), (18, 18), (38, 39)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(9, 16), (30, 30), (41, 41), (67, 67), (93, 93), (101, 101), (148, 148), (426, 426)], [(81, 81), (96, 96)], [(73, 73), (117, 117), (152, 152), (163, 163), (280, 280), (431, 431)], [(25, 26), (158, 160), (235, 236), (257, 258), (305, 305), (362, 363), (409, 410), (419, 419)], [(189, 189), (194, 194)], [(196, 196), (214, 214), (230, 230), (367, 372), (367, 376)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 11.4379, loss: 2.3700 ||:  94%|#########3| 2621/2802 [04:50<00:21,  8.47it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (20, 20), (54, 54), (75, 75), (92, 92), (110, 110), (125, 125)], [(193, 194), (203, 204), (222, 223), (230, 230), (239, 239)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(26, 28), (33, 33)], [(8, 10), (46, 48), (58, 60), (163, 165), (180, 182), (262, 265), (272, 274), (278, 278), (284, 285), (299, 302), (309, 312), (320, 324), (335, 335), (349, 349)], [(42, 42), (238, 238), (391, 391)], [(352, 352), (358, 358)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4460, batch_loss: 3.4009, loss: 2.3700 ||:  94%|#########3| 2623/2802 [04:51<00:19,  9.04it/s] predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(12, 12), (22, 22), (34, 34), (48, 48), (60, 60), (156, 156), (183, 183), (209, 209), (215, 215), (220, 220), (296, 296), (304, 304), (311, 311), (318, 318)], [(177, 177), (180, 180)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(0, 7), (9, 9), (11, 11), (45, 50), (62, 62), (97, 97)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 0.0050, loss: 2.3689 ||:  94%|#########3| 2625/2802 [04:51<00:18,  9.74it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(37, 40), (47, 47)], [(127, 127), (139, 139)], [(120, 121), (160, 161)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(6, 11), (51, 53), (62, 62), (320, 323), (387, 388)], [(96, 99), (128, 131), (330, 347)], [(259, 260), (270, 270)], [(290, 293), (301, 302), (309, 309)], [(128, 130), (357, 357), (408, 409), (418, 418), (543, 545)], [(440, 468), (584, 587)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4460, batch_loss: 0.6510, loss: 2.3674 ||:  94%|#########3| 2627/2802 [04:51<00:20,  8.39it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(4, 4), (39, 39), (58, 58), (68, 68), (172, 172), (226, 226), (263, 263), (475, 475)], [(65, 66), (85, 86)], [(25, 34), (130, 138)], [(183, 185), (205, 207)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 1.0205, loss: 2.3669 ||:  94%|#########3| 2628/2802 [04:51<00:22,  7.57it/s]predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(5, 5), (29, 29), (156, 156), (242, 242), (256, 256), (261, 261), (300, 300)], [(54, 54), (58, 58), (62, 62), (76, 76)], [(136, 137), (167, 169), (249, 251), (266, 266), (269, 269)], [(197, 197), (232, 233), (288, 288)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 3.5074, loss: 2.3673 ||:  94%|#########3| 2629/2802 [04:51<00:22,  7.74it/s]predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(66, 67), (96, 96), (200, 200), (203, 203), (416, 416), (430, 430)], [(163, 165), (175, 175), (259, 260)], [(215, 216), (285, 287), (512, 513), (540, 540), (546, 546), (601, 601), (627, 627), (631, 631), (671, 678), (704, 705), (724, 724), (768, 768), (847, 847)], [(297, 298), (302, 303)], [(197, 198), (319, 319), (770, 771)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 11.0813, loss: 2.3706 ||:  94%|#########3| 2630/2802 [04:52<00:29,  5.80it/s]predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(22, 27), (95, 95), (109, 110), (126, 127), (252, 255)], [(130, 132), (146, 147)], [(29, 30), (157, 158), (228, 229)], [(141, 142), (343, 344)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 9.4155, loss: 2.3733 ||:  94%|#########3| 2631/2802 [04:52<00:27,  6.33it/s] predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(17, 17), (24, 25), (38, 38)], [(4, 10), (62, 64)], [(88, 90), (95, 95)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 0.0001, loss: 2.3715 ||:  94%|#########3| 2633/2802 [04:52<00:20,  8.14it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(15, 16), (54, 55)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[[(7, 10), (17, 26)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 0.0720, loss: 2.3704 ||:  94%|#########4| 2635/2802 [04:52<00:16,  9.86it/s]predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(99, 109), (111, 111)], [(211, 212), (250, 250)], [(222, 224), (259, 260)], [(305, 309), (347, 347)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(69, 77), (90, 91), (264, 270)], [(22, 42), (103, 114), (153, 163), (188, 189), (202, 203), (286, 288), (299, 300)], [(75, 77), (175, 177), (193, 195), (208, 210), (268, 270), (275, 277), (292, 294), (321, 323), (439, 444)], [(307, 307), (371, 373)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4471, mention_recall: 0.4462, batch_loss: 4.6733, loss: 2.3704 ||:  94%|#########4| 2637/2802 [04:52<00:19,  8.35it/s]predicted_antecedents size torch.Size([1, 42])\n",
            "predicted_antecedents_float size torch.Size([1, 42])\n",
            "probs_logs size: torch.Size([1, 42, 6])\n",
            "probs size: torch.Size([1, 42, 6])\n",
            "probs_flat size: torch.Size([1, 252])\n",
            "gold_antecedent_labels size: torch.Size([1, 42, 6])\n",
            "clusters (batch_clusters) [[[(19, 21), (37, 37), (98, 98), (136, 136), (220, 220), (245, 247)], [(55, 71), (308, 308), (335, 335), (401, 402), (408, 408), (433, 434), (460, 460), (509, 511)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 4.4660, loss: 2.3712 ||:  94%|#########4| 2638/2802 [04:53<00:21,  7.54it/s]predicted_antecedents size torch.Size([1, 40])\n",
            "predicted_antecedents_float size torch.Size([1, 40])\n",
            "probs_logs size: torch.Size([1, 40, 6])\n",
            "probs size: torch.Size([1, 40, 6])\n",
            "probs_flat size: torch.Size([1, 240])\n",
            "gold_antecedent_labels size: torch.Size([1, 40, 6])\n",
            "clusters (batch_clusters) [[[(22, 24), (43, 43), (54, 55), (77, 78), (85, 85), (153, 153), (166, 166), (194, 194), (206, 206), (334, 336), (396, 396), (402, 402)], [(10, 13), (49, 51), (73, 73), (161, 161), (246, 247), (310, 310), (400, 400), (408, 408)], [(210, 212), (216, 216), (225, 225), (229, 229)], [(364, 370), (381, 382)], [(428, 430), (467, 468)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 0.5207, loss: 2.3705 ||:  94%|#########4| 2639/2802 [04:53<00:23,  6.83it/s]predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(3, 13), (65, 66), (128, 131), (220, 221)], [(30, 30), (81, 81), (83, 83), (109, 109), (123, 123), (144, 144), (151, 151), (166, 166), (188, 188), (192, 192), (200, 200), (208, 208), (213, 213), (241, 241), (253, 253)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (25, 26)], [(40, 41), (93, 94)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4461, batch_loss: 0.3858, loss: 2.3690 ||:  94%|#########4| 2641/2802 [04:53<00:18,  8.57it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (73, 74), (92, 92), (99, 99), (148, 149), (195, 195)], [(92, 93), (201, 201)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(29, 37), (48, 49), (282, 284)], [(191, 191), (200, 200), (479, 480), (497, 497)], [(29, 31), (214, 216), (266, 267), (319, 320), (464, 467), (509, 510), (520, 520)], [(156, 158), (311, 313)], [(98, 104), (374, 375)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 9.6215, loss: 2.3730 ||:  94%|#########4| 2643/2802 [04:53<00:19,  7.98it/s]predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(46, 56), (72, 73)], [(148, 149), (155, 156), (162, 162), (172, 172), (283, 284), (308, 309)], [(188, 188), (212, 212), (269, 269)], [(236, 237), (260, 260)], [(271, 271), (286, 286), (301, 301)], [(34, 36), (337, 338)], [(348, 348), (362, 362), (372, 372)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 9.5604, loss: 2.3757 ||:  94%|#########4| 2644/2802 [04:53<00:19,  8.05it/s]predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (24, 26), (46, 46), (77, 79), (111, 112), (128, 129), (161, 161), (177, 179), (211, 213), (292, 294), (303, 305), (329, 331), (375, 377), (390, 391), (394, 394), (429, 429), (473, 475), (508, 510), (557, 557), (562, 564)], [(429, 430), (433, 433), (440, 440)], [(453, 455), (457, 457), (462, 462)], [(462, 463), (485, 486)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 3.2281, loss: 2.3760 ||:  94%|#########4| 2645/2802 [04:53<00:23,  6.70it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(13, 13), (43, 44), (50, 50)], [(23, 31), (68, 68), (82, 82), (156, 156), (165, 165)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(23, 32), (61, 62)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 0.7163, loss: 2.3752 ||:  94%|#########4| 2647/2802 [04:54<00:18,  8.31it/s]predicted_antecedents size torch.Size([1, 40])\n",
            "predicted_antecedents_float size torch.Size([1, 40])\n",
            "probs_logs size: torch.Size([1, 40, 6])\n",
            "probs size: torch.Size([1, 40, 6])\n",
            "probs_flat size: torch.Size([1, 240])\n",
            "gold_antecedent_labels size: torch.Size([1, 40, 6])\n",
            "clusters (batch_clusters) [[[(0, 15), (49, 50), (230, 230)], [(93, 108), (123, 125), (127, 128)], [(83, 84), (158, 160), (170, 172), (193, 194), (218, 219), (260, 262), (283, 285), (309, 310), (386, 390), (414, 415), (433, 433), (443, 447), (450, 451), (481, 482)], [(25, 25), (249, 251)], [(341, 352), (378, 378)], [(458, 460), (487, 489)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4462, batch_loss: 1.2753, loss: 2.3747 ||:  95%|#########4| 2648/2802 [04:54<00:20,  7.50it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(0, 6), (11, 11), (19, 19)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(52, 69), (104, 104), (109, 109), (131, 131), (139, 139), (175, 175), (194, 194), (212, 214), (222, 222), (226, 226), (241, 241), (246, 246)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4462, batch_loss: 0.1709, loss: 2.3730 ||:  95%|#########4| 2650/2802 [04:54<00:16,  9.01it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(18, 18), (23, 23)], [(105, 109), (129, 129), (138, 138)], [(94, 97), (142, 142)], [(107, 109), (163, 164), (205, 205), (212, 212), (214, 214), (225, 225)], [(170, 171), (188, 189)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(16, 16), (29, 29), (53, 53), (72, 72), (79, 79), (92, 92), (97, 97), (102, 102), (120, 120), (124, 124), (138, 138), (210, 210), (227, 227), (238, 238)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4460, batch_loss: 0.1654, loss: 2.3716 ||:  95%|#########4| 2652/2802 [04:54<00:14, 10.21it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(45, 47), (86, 87), (115, 116), (141, 143), (176, 177)], [(109, 109), (112, 112)], [(149, 151), (156, 156)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(34, 34), (44, 44), (52, 52)], [(61, 62), (64, 64), (92, 92), (97, 97), (104, 104), (220, 220), (228, 228)], [(137, 137), (139, 139)], [(169, 186), (195, 195), (200, 200), (204, 204), (209, 209)], [(300, 310), (317, 317), (326, 326), (334, 334), (346, 346)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4460, batch_loss: 0.5031, loss: 2.3719 ||:  95%|#########4| 2654/2802 [04:54<00:14, 10.15it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(31, 31), (82, 83), (105, 105), (209, 210), (236, 237), (386, 387)], [(70, 72), (193, 193), (216, 217)], [(5, 8), (274, 274), (378, 381)], [(282, 284), (299, 300), (327, 327), (356, 356), (368, 369), (401, 403), (408, 408), (420, 421)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(28, 28), (31, 31), (40, 40), (44, 44), (65, 65), (216, 216)], [(21, 24), (33, 35)], [(46, 46), (50, 50), (53, 53)], [(0, 8), (68, 74)], [(151, 152), (160, 161), (177, 178)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4460, batch_loss: 8.1583, loss: 2.3732 ||:  95%|#########4| 2656/2802 [04:55<00:15,  9.19it/s]predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(122, 122), (132, 132)], [(178, 194), (197, 197), (206, 206), (216, 216), (228, 228), (248, 248), (255, 255), (299, 299), (306, 306), (314, 314), (318, 318)], [(109, 109), (243, 243), (321, 321), (364, 364), (396, 396), (408, 408)], [(368, 369), (374, 375), (400, 401), (422, 422)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 7.0161, loss: 2.3750 ||:  95%|#########4| 2657/2802 [04:55<00:16,  8.66it/s]predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (29, 30)], [(13, 30), (36, 36)], [(196, 197), (206, 206), (279, 279), (326, 327)], [(342, 343), (345, 345), (367, 374)], [(401, 404), (409, 409)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 4.4053, loss: 2.3758 ||:  95%|#########4| 2658/2802 [04:55<00:18,  7.79it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(36, 36), (49, 50), (52, 52), (72, 73), (118, 120), (122, 123), (127, 127), (152, 152), (165, 165)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(5, 5), (42, 43), (148, 149), (180, 180), (200, 200), (212, 212), (258, 258)], [(86, 87), (93, 93)], [(300, 306), (338, 338), (354, 354), (407, 407), (414, 414)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 0.5137, loss: 2.3748 ||:  95%|#########4| 2660/2802 [04:55<00:17,  8.01it/s]predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(32, 33), (38, 38)], [(45, 48), (65, 66), (74, 74)], [(118, 119), (123, 123), (138, 138), (183, 183), (235, 235), (249, 252), (268, 271)], [(116, 116), (154, 155), (175, 176), (189, 189), (193, 193)], [(246, 246), (265, 265)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(7, 12), (47, 51), (70, 74), (87, 92), (149, 149), (159, 159), (168, 168), (272, 287), (342, 346), (365, 369), (373, 373), (383, 383), (401, 405), (414, 414), (446, 450)], [(35, 37), (253, 255)], [(295, 297), (379, 381), (398, 399)], [(441, 442), (456, 457)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 0.7653, loss: 2.3752 ||:  95%|#########5| 2662/2802 [04:55<00:17,  7.95it/s]predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(8, 14), (29, 29)], [(130, 133), (193, 194), (395, 398), (449, 450)], [(202, 203), (233, 233), (262, 263)], [(408, 414), (475, 477), (480, 480)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4458, batch_loss: 1.3448, loss: 2.3748 ||:  95%|#########5| 2663/2802 [04:56<00:19,  7.14it/s]predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(13, 14), (17, 17)], [(62, 62), (66, 66)], [(103, 125), (147, 147), (150, 150), (170, 170), (184, 184), (233, 233), (257, 257)], [(103, 126), (194, 205)], [(236, 238), (238, 238)], [(280, 303), (342, 342), (351, 351)], [(458, 458), (507, 507), (512, 512), (546, 546), (548, 548), (555, 555), (560, 560)], [(514, 515), (521, 521)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 9.6727, loss: 2.3776 ||:  95%|#########5| 2664/2802 [04:56<00:21,  6.48it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(26, 28), (94, 95), (109, 109), (119, 120)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(4, 7), (19, 19)], [(13, 13), (40, 40), (113, 113), (194, 194), (204, 204), (221, 221), (232, 232), (237, 237), (241, 241), (247, 247), (271, 271)], [(243, 243), (253, 253), (263, 263)], [(177, 177), (265, 267)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 4.9211, loss: 2.3777 ||:  95%|#########5| 2666/2802 [04:56<00:16,  8.03it/s]predicted_antecedents size torch.Size([1, 42])\n",
            "predicted_antecedents_float size torch.Size([1, 42])\n",
            "probs_logs size: torch.Size([1, 42, 6])\n",
            "probs size: torch.Size([1, 42, 6])\n",
            "probs_flat size: torch.Size([1, 252])\n",
            "gold_antecedent_labels size: torch.Size([1, 42, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (53, 55), (102, 103)], [(0, 10), (53, 61), (53, 64), (116, 122)], [(14, 18), (59, 61)], [(421, 422), (504, 505)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 3.8082, loss: 2.3783 ||:  95%|#########5| 2667/2802 [04:56<00:18,  7.25it/s]predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(20, 29), (37, 38)], [(11, 12), (43, 44), (84, 85)], [(61, 63), (62, 63), (81, 82), (114, 115), (173, 174)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(25, 43), (49, 49), (63, 78), (97, 97)], [(130, 138), (140, 140), (163, 163)], [(57, 61), (157, 159), (273, 277)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 2.2509, loss: 2.3780 ||:  95%|#########5| 2669/2802 [04:56<00:16,  8.20it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(52, 52), (56, 56), (80, 80), (156, 157), (164, 164), (168, 168), (180, 180), (185, 185)], [(83, 84), (93, 93), (192, 192)], [(124, 125), (140, 140)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(23, 34), (44, 44), (66, 66), (166, 166), (177, 177)], [(92, 97), (100, 102), (123, 128), (141, 141), (206, 211)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 0.1983, loss: 2.3783 ||:  95%|#########5| 2671/2802 [04:57<00:14,  8.79it/s]predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(34, 36), (49, 49), (176, 178), (191, 191), (201, 201), (205, 205)], [(17, 19), (147, 147), (198, 198), (210, 210), (239, 239)], [(266, 268), (285, 288), (322, 322), (338, 338), (355, 355)], [(22, 24), (300, 302)], [(316, 322), (332, 332)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4459, batch_loss: 1.5019, loss: 2.3779 ||:  95%|#########5| 2672/2802 [04:57<00:15,  8.31it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(54, 58), (73, 73)], [(50, 52), (114, 117)], [(20, 20), (216, 217), (230, 230), (245, 246), (252, 252), (260, 260), (375, 376)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4459, batch_loss: 5.8311, loss: 2.3792 ||:  95%|#########5| 2673/2802 [04:57<00:17,  7.47it/s]predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(90, 92), (168, 170), (309, 311)], [(114, 122), (189, 196)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4459, batch_loss: 0.4355, loss: 2.3785 ||:  95%|#########5| 2674/2802 [04:57<00:17,  7.26it/s]predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(34, 36), (41, 41), (215, 217), (241, 243), (247, 247), (292, 292), (304, 307)], [(51, 54), (70, 72), (115, 117), (123, 123), (266, 267)], [(60, 62), (138, 141), (160, 160), (307, 307)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4459, batch_loss: 0.2891, loss: 2.3777 ||:  95%|#########5| 2675/2802 [04:57<00:16,  7.77it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (49, 49), (69, 69), (107, 107), (121, 121), (133, 133), (141, 141), (183, 183), (218, 218)], [(145, 145), (156, 156), (159, 159)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (138, 138)], [(156, 158), (162, 164)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 0.3945, loss: 2.3769 ||:  96%|#########5| 2677/2802 [04:57<00:13,  9.56it/s]predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(78, 79), (86, 86)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8979, coref_recall: 0.2982, coref_f1: 0.4465, mention_recall: 0.4458, batch_loss: 0.9207, loss: 2.3756 ||:  96%|#########5| 2679/2802 [04:57<00:11, 10.77it/s]predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(60, 60), (75, 75), (77, 77), (147, 147), (149, 149), (161, 161), (177, 177), (184, 184), (201, 201), (208, 208), (312, 314), (518, 519)], [(168, 168), (189, 189)], [(9, 23), (346, 347)], [(421, 421), (425, 425), (486, 486), (492, 492)], [(223, 225), (446, 448)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(7, 10), (13, 13), (84, 85), (95, 97)], [(25, 26), (43, 44), (56, 56), (102, 102), (131, 132), (260, 261), (275, 275), (303, 303), (309, 310), (364, 365)], [(185, 185), (188, 188)], [(139, 141), (332, 334), (348, 349)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2982, coref_f1: 0.4465, mention_recall: 0.4458, batch_loss: 1.0597, loss: 2.3756 ||:  96%|#########5| 2681/2802 [04:58<00:13,  8.74it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (16, 18), (49, 49), (90, 92), (123, 123), (133, 133), (260, 263), (282, 285)], [(24, 27), (80, 83), (255, 258), (344, 346)], [(16, 19), (109, 110)], [(123, 124), (250, 250)], [(324, 324), (335, 335)], [(350, 350), (355, 355)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8980, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 1.1076, loss: 2.3751 ||:  96%|#########5| 2682/2802 [04:58<00:14,  8.56it/s]predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(12, 19), (35, 39)], [(63, 64), (74, 74), (175, 176)], [(48, 50), (80, 82)], [(195, 199), (201, 201), (211, 211)], [(1, 5), (226, 227)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4460, batch_loss: 2.3653, loss: 2.3751 ||:  96%|#########5| 2683/2802 [04:58<00:14,  8.25it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(2, 2), (25, 25), (31, 31), (42, 42), (67, 67), (138, 138)], [(7, 7), (130, 130), (171, 171), (177, 177), (182, 182), (186, 186)], [(221, 225), (228, 228)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(7, 20), (45, 58), (177, 189), (206, 213)], [(15, 20), (53, 58), (184, 189), (218, 220), (241, 243), (366, 368)], [(103, 106), (114, 117), (119, 122), (199, 201)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2983, coref_f1: 0.4466, mention_recall: 0.4460, batch_loss: 3.5917, loss: 2.3754 ||:  96%|#########5| 2685/2802 [04:58<00:13,  8.43it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (34, 35)], [(46, 48), (56, 57), (115, 117), (128, 130), (191, 193)], [(43, 48), (69, 70)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (68, 69)], [(118, 125), (153, 153), (168, 169)], [(82, 83), (188, 196), (206, 206), (256, 256), (296, 296), (304, 304), (318, 318)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8981, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4460, batch_loss: 1.1143, loss: 2.3742 ||:  96%|#########5| 2687/2802 [04:58<00:12,  9.19it/s]predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(17, 20), (58, 59)], [(5, 5), (65, 65)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(77, 77), (81, 81)], [(180, 180), (190, 190)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4467, mention_recall: 0.4461, batch_loss: 0.1626, loss: 2.3725 ||:  96%|#########5| 2689/2802 [04:59<00:10, 10.42it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(18, 20), (49, 49)], [(89, 89), (111, 111), (136, 136), (176, 176), (179, 179), (200, 200)], [(222, 223), (225, 225)], [(81, 84), (240, 242), (273, 275)], [(245, 245), (256, 256), (260, 260)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (11, 11), (152, 153), (157, 159), (195, 196), (217, 217), (236, 237), (315, 316), (319, 319), (327, 329)], [(4, 4), (28, 28), (36, 37), (39, 39), (75, 76), (87, 87), (95, 95), (113, 113), (133, 133), (147, 147), (189, 189), (364, 364), (383, 383), (392, 392)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8982, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 1.5964, loss: 2.3717 ||:  96%|#########6| 2691/2802 [04:59<00:11,  9.54it/s]predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(34, 38), (50, 54), (74, 78), (174, 178), (193, 197), (213, 217), (238, 242), (289, 293), (321, 325), (347, 351)], [(202, 203), (227, 228)], [(377, 378), (388, 388), (401, 401)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4462, batch_loss: 0.0895, loss: 2.3709 ||:  96%|#########6| 2692/2802 [04:59<00:12,  8.95it/s]predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(16, 28), (37, 37), (43, 43), (66, 66), (109, 109), (119, 119), (136, 136), (143, 143), (171, 171), (175, 175), (195, 195), (212, 212), (285, 285), (297, 297), (338, 341), (383, 383), (394, 394), (407, 407), (410, 410), (418, 418), (429, 429)], [(253, 255), (257, 257)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4463, batch_loss: 0.7689, loss: 2.3703 ||:  96%|#########6| 2693/2802 [04:59<00:13,  7.98it/s]predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (16, 25), (31, 31), (43, 43), (101, 101), (129, 129), (191, 191), (195, 195), (201, 201), (205, 205), (220, 220), (260, 260), (282, 283), (312, 312), (355, 356), (387, 387)], [(289, 291), (392, 392)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4463, batch_loss: 0.4817, loss: 2.3696 ||:  96%|#########6| 2694/2802 [04:59<00:14,  7.60it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(22, 24), (104, 106), (175, 175), (182, 182)], [(243, 243), (251, 253), (255, 255), (260, 264)], [(251, 251), (267, 267)], [(277, 278), (287, 287)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4462, batch_loss: 3.5066, loss: 2.3700 ||:  96%|#########6| 2695/2802 [04:59<00:13,  8.04it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (4, 4), (68, 73)], [(28, 29), (58, 59)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(10, 12), (23, 24), (71, 72)], [(0, 3), (80, 80), (90, 90), (97, 97)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4462, batch_loss: 1.1362, loss: 2.3687 ||:  96%|#########6| 2697/2802 [04:59<00:10, 10.05it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(10, 10), (51, 51), (166, 167), (173, 174)], [(188, 190), (202, 202)], [(37, 38), (219, 220)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(22, 24), (46, 48), (95, 97), (379, 383), (432, 433), (447, 448), (470, 470), (536, 537), (562, 566), (599, 603)], [(89, 103), (106, 106), (161, 161)], [(127, 130), (171, 174), (192, 195), (266, 269), (350, 353)], [(300, 302), (403, 405)], [(291, 292), (547, 549)], [(355, 356), (629, 629)], [(330, 331), (631, 632)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4461, batch_loss: 2.7527, loss: 2.3680 ||:  96%|#########6| 2699/2802 [05:00<00:12,  8.00it/s]predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (43, 44), (132, 134)], [(73, 77), (94, 98)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(54, 56), (71, 72), (128, 153), (143, 146), (189, 189), (254, 255), (280, 280), (535, 535)], [(93, 93), (205, 205), (270, 270)], [(264, 265), (412, 413)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4461, batch_loss: 2.7606, loss: 2.3684 ||:  96%|#########6| 2701/2802 [05:00<00:13,  7.40it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (29, 33), (43, 44), (80, 91), (193, 195), (235, 237), (301, 304), (363, 365)], [(40, 41), (208, 209)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4462, batch_loss: 0.9417, loss: 2.3679 ||:  96%|#########6| 2702/2802 [05:00<00:14,  7.09it/s]predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(5, 11), (133, 139), (146, 146), (164, 164)], [(5, 9), (167, 168), (172, 174)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 0.8237, loss: 2.3664 ||:  97%|#########6| 2704/2802 [05:00<00:11,  8.61it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(6, 16), (22, 22), (72, 73), (96, 97), (124, 124), (134, 134), (142, 142), (146, 146), (153, 153), (184, 186), (190, 190), (194, 194), (205, 205), (229, 230), (241, 242)], [(261, 261), (269, 269)], [(311, 311), (318, 318)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2986, coref_f1: 0.4470, mention_recall: 0.4462, batch_loss: 0.1128, loss: 2.3656 ||:  97%|#########6| 2705/2802 [05:01<00:11,  8.79it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(32, 32), (49, 49), (59, 59), (70, 70), (76, 76), (83, 83), (114, 114), (147, 147), (152, 152), (161, 161), (166, 166), (181, 181), (188, 188)], [(202, 202), (209, 209), (228, 228)], [(181, 183), (233, 235)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(14, 18), (22, 22), (25, 27)], [(52, 52), (55, 55), (67, 67)], [(164, 172), (176, 177)], [(235, 235), (242, 242), (255, 255), (259, 259)], [(216, 216), (265, 265), (277, 277)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4460, batch_loss: 1.6653, loss: 2.3650 ||:  97%|#########6| 2707/2802 [05:01<00:09,  9.85it/s]predicted_antecedents size torch.Size([1, 1])\n",
            "predicted_antecedents_float size torch.Size([1, 1])\n",
            "probs_logs size: torch.Size([1, 1, 2])\n",
            "probs size: torch.Size([1, 1, 2])\n",
            "probs_flat size: torch.Size([1, 2])\n",
            "gold_antecedent_labels size: torch.Size([1, 1, 2])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(87, 88), (156, 157), (200, 214), (277, 278)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4461, batch_loss: 1.8146, loss: 2.3639 ||:  97%|#########6| 2709/2802 [05:01<00:09, 10.20it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(18, 23), (78, 81)], [(10, 23), (84, 84)], [(10, 16), (100, 102)], [(48, 49), (244, 246)], [(371, 371), (376, 376)], [(272, 273), (388, 389)], [(406, 417), (415, 415)], [(379, 389), (441, 441)], [(464, 465), (467, 467)], [(472, 487), (493, 493)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(90, 91), (113, 113), (137, 138), (142, 142), (150, 150)], [(160, 161), (168, 168), (187, 187)], [(227, 233), (238, 238), (245, 245), (266, 266)], [(17, 29), (242, 242)], [(245, 250), (256, 256)], [(50, 50), (312, 314)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 5.9559, loss: 2.3655 ||:  97%|#########6| 2711/2802 [05:01<00:10,  8.77it/s]predicted_antecedents size torch.Size([1, 42])\n",
            "predicted_antecedents_float size torch.Size([1, 42])\n",
            "probs_logs size: torch.Size([1, 42, 6])\n",
            "probs size: torch.Size([1, 42, 6])\n",
            "probs_flat size: torch.Size([1, 252])\n",
            "gold_antecedent_labels size: torch.Size([1, 42, 6])\n",
            "clusters (batch_clusters) [[[(0, 20), (34, 35), (39, 40), (370, 371), (459, 460), (482, 487)], [(201, 202), (210, 211), (278, 279)], [(65, 68), (226, 229), (434, 437)], [(231, 233), (256, 280), (282, 282), (362, 363)], [(475, 480), (490, 490), (512, 514)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 3.0497, loss: 2.3658 ||:  97%|#########6| 2712/2802 [05:01<00:11,  7.55it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(27, 27), (44, 44), (61, 61), (78, 78)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(6, 7), (29, 30), (54, 55), (59, 66), (92, 94), (154, 154), (213, 213)], [(9, 9), (100, 100), (111, 111), (193, 193)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 1.0910, loss: 2.3645 ||:  97%|#########6| 2714/2802 [05:02<00:09,  9.09it/s]predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(13, 22), (30, 30), (55, 57), (273, 282)], [(18, 22), (37, 41), (278, 282)], [(4, 4), (146, 146)], [(112, 113), (162, 162)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.2012, loss: 2.3637 ||:  97%|#########6| 2715/2802 [05:02<00:09,  8.95it/s]predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(69, 70), (77, 78), (135, 136)], [(32, 37), (155, 156)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.5464, loss: 2.3621 ||:  97%|#########6| 2717/2802 [05:02<00:08, 10.17it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(190, 194), (199, 199)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(9, 17), (31, 31), (140, 141), (143, 143), (165, 166)], [(28, 29), (40, 40), (71, 72), (132, 132), (134, 134), (138, 138), (146, 146), (176, 176)], [(96, 98), (117, 117)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 5.4697, loss: 2.3627 ||:  97%|#########7| 2719/2802 [05:02<00:07, 10.90it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (22, 22), (28, 28), (32, 32), (39, 39), (64, 64), (72, 72), (107, 108), (118, 118), (139, 139), (145, 145), (149, 149), (171, 171), (195, 195), (207, 207), (233, 234), (240, 240), (246, 246), (252, 252)], [(181, 181), (198, 198)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 42])\n",
            "predicted_antecedents_float size torch.Size([1, 42])\n",
            "probs_logs size: torch.Size([1, 42, 6])\n",
            "probs size: torch.Size([1, 42, 6])\n",
            "probs_flat size: torch.Size([1, 252])\n",
            "gold_antecedent_labels size: torch.Size([1, 42, 6])\n",
            "clusters (batch_clusters) [[[(43, 58), (91, 91), (188, 188), (226, 227), (265, 265), (306, 308), (315, 315), (384, 385), (433, 434), (455, 456), (527, 528)], [(381, 381), (426, 426), (450, 450), (480, 480), (519, 519)], [(131, 134), (438, 440)], [(403, 406), (508, 509)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 5.4067, loss: 2.3645 ||:  97%|#########7| 2721/2802 [05:02<00:08,  9.29it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(77, 77), (90, 90), (176, 176), (245, 245), (256, 256), (267, 267)], [(173, 173), (181, 181)], [(202, 202), (206, 206), (217, 217), (232, 232), (236, 236)], [(211, 211), (227, 228), (234, 234)], [(156, 156), (270, 270)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(137, 148), (156, 157)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4457, batch_loss: 1.4579, loss: 2.3637 ||:  97%|#########7| 2723/2802 [05:02<00:07, 10.21it/s]predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (46, 46), (80, 80), (83, 83), (98, 98), (107, 107), (126, 126), (131, 131), (135, 135), (171, 171), (179, 179), (186, 186), (209, 209), (218, 218), (233, 233)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2983, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 1.4671, loss: 2.3625 ||:  97%|#########7| 2725/2802 [05:02<00:06, 11.43it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(55, 63), (100, 100), (107, 107), (120, 120)], [(164, 165), (171, 171), (182, 182)], [(279, 279), (296, 296), (304, 304)], [(281, 282), (302, 302)], [(96, 98), (317, 318)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(14, 15), (51, 52)], [(176, 194), (193, 193)], [(210, 210), (229, 229), (232, 232)], [(260, 268), (288, 288)], [(307, 307), (315, 315)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4457, batch_loss: 1.1631, loss: 2.3638 ||:  97%|#########7| 2727/2802 [05:03<00:06, 10.94it/s]predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(12, 13), (46, 47), (100, 101), (164, 166), (210, 211), (219, 220), (243, 245), (314, 316), (430, 431)], [(164, 167), (243, 246)], [(251, 251), (260, 260)], [(293, 293), (300, 300), (307, 307)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (15, 15), (15, 16), (16, 16), (56, 56), (73, 73), (97, 97), (101, 101), (120, 120), (147, 147), (182, 182), (194, 194)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4458, batch_loss: 3.7055, loss: 2.3635 ||:  97%|#########7| 2729/2802 [05:03<00:07, 10.06it/s]predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(58, 64), (73, 75)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(30, 35), (39, 39), (87, 87), (95, 95), (127, 127), (151, 151), (185, 191), (197, 197), (212, 212)], [(34, 35), (147, 149), (215, 217)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4458, batch_loss: 0.5270, loss: 2.3620 ||:  97%|#########7| 2731/2802 [05:03<00:06, 10.50it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(21, 21), (25, 25), (32, 32)], [(39, 39), (43, 43)], [(139, 139), (170, 170)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (44, 44), (55, 55), (77, 78)], [(4, 5), (48, 49), (97, 98), (116, 116)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4458, batch_loss: 0.0037, loss: 2.3607 ||:  98%|#########7| 2733/2802 [05:03<00:06, 11.36it/s]predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(20, 25), (39, 40)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8987, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.0894, loss: 2.3590 ||:  98%|#########7| 2735/2802 [05:03<00:05, 12.43it/s]predicted_antecedents size torch.Size([1, 52])\n",
            "predicted_antecedents_float size torch.Size([1, 52])\n",
            "probs_logs size: torch.Size([1, 52, 6])\n",
            "probs size: torch.Size([1, 52, 6])\n",
            "probs_flat size: torch.Size([1, 312])\n",
            "gold_antecedent_labels size: torch.Size([1, 52, 6])\n",
            "clusters (batch_clusters) [[[(24, 29), (61, 67), (101, 107), (182, 189), (183, 189)], [(24, 27), (65, 67), (93, 95), (105, 107), (111, 113), (134, 137), (187, 189), (203, 205), (214, 214), (230, 233), (252, 254), (282, 284), (296, 296), (307, 310), (370, 372), (393, 393), (408, 410), (463, 463), (466, 466), (474, 474), (481, 483), (501, 503), (505, 505), (537, 537), (565, 566), (655, 657)], [(18, 29), (146, 147)], [(550, 553), (573, 574)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(6, 7), (31, 35)], [(24, 29), (69, 69), (130, 130), (152, 152), (166, 166), (179, 179), (289, 289)], [(162, 163), (203, 204), (206, 206)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2985, coref_f1: 0.4469, mention_recall: 0.4459, batch_loss: 0.4073, loss: 2.3593 ||:  98%|#########7| 2737/2802 [05:04<00:06,  9.34it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(4, 6), (24, 30), (190, 191)], [(53, 54), (83, 84), (197, 198)], [(152, 152), (163, 163)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(3, 4), (7, 7)], [(95, 96), (114, 114), (232, 234)], [(195, 195), (200, 200)], [(117, 118), (272, 274), (283, 283), (342, 343)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 8.0925, loss: 2.3607 ||:  98%|#########7| 2739/2802 [05:04<00:06,  9.36it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(135, 135), (139, 139), (144, 144), (156, 156), (163, 163)], [(210, 210), (222, 222), (224, 224), (230, 230)], [(245, 245), (250, 250), (260, 260)], [(267, 268), (273, 273), (279, 279), (281, 281), (284, 284)], [(295, 295), (313, 314), (323, 323)], [(392, 392), (406, 407)], [(473, 477), (480, 480)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(113, 113), (126, 126), (133, 133)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2984, coref_f1: 0.4469, mention_recall: 0.4460, batch_loss: 0.1116, loss: 2.3595 ||:  98%|#########7| 2741/2802 [05:04<00:07,  8.67it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(0, 5), (22, 23)], [(68, 72), (76, 76)], [(30, 34), (122, 125)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(75, 75), (92, 92)], [(132, 133), (141, 141)], [(150, 150), (172, 172)], [(162, 168), (179, 182)], [(184, 184), (187, 188)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 6.6692, loss: 2.3603 ||:  98%|#########7| 2743/2802 [05:04<00:06,  9.49it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(137, 138), (153, 153), (159, 159)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(27, 31), (34, 35)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4458, batch_loss: 0.0096, loss: 2.3587 ||:  98%|#########7| 2745/2802 [05:04<00:05, 10.74it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(25, 25), (37, 37), (64, 64)], [(0, 0), (105, 105), (112, 112), (230, 230)], [(167, 175), (181, 181), (188, 188)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(5, 11), (15, 15), (47, 50), (54, 54), (167, 168)], [(8, 11), (40, 42), (286, 287)], [(40, 45), (142, 142)], [(126, 128), (179, 181)], [(216, 216), (220, 221), (240, 241)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4458, batch_loss: 0.8820, loss: 2.3591 ||:  98%|#########8| 2747/2802 [05:05<00:05, 10.94it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(8, 12), (67, 68)], [(20, 26), (79, 79), (147, 147), (166, 166), (181, 181)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4458, batch_loss: 2.6306, loss: 2.3583 ||:  98%|#########8| 2749/2802 [05:05<00:04, 11.42it/s]predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(3, 5), (14, 14)], [(4, 4), (30, 36), (44, 46), (138, 139), (251, 251), (263, 263)], [(184, 184), (200, 200)], [(242, 242), (244, 244)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(313, 316), (328, 330), (339, 340), (348, 348)], [(402, 405), (446, 447)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2982, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 7.7146, loss: 2.3621 ||:  98%|#########8| 2751/2802 [05:05<00:05, 10.07it/s]predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(25, 25), (30, 30)], [(14, 16), (66, 68), (161, 163), (244, 246)], [(152, 156), (166, 166)], [(155, 156), (186, 187)], [(176, 176), (211, 211), (221, 221), (225, 225), (254, 254)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(2, 4), (17, 17), (47, 47)], [(80, 80), (83, 83), (90, 90), (111, 111), (121, 121)], [(168, 169), (211, 212)], [(230, 231), (242, 242)], [(263, 263), (279, 279)], [(161, 175), (290, 290), (340, 343), (358, 358)], [(416, 416), (471, 471)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 3.0674, loss: 2.3620 ||:  98%|#########8| 2753/2802 [05:05<00:05,  9.27it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(14, 16), (21, 21)], [(52, 52), (84, 84), (94, 94), (98, 98), (106, 106), (155, 155), (197, 197), (233, 236), (239, 239), (286, 286)], [(252, 257), (272, 272)], [(318, 318), (324, 324), (344, 344)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2982, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 0.3128, loss: 2.3612 ||:  98%|#########8| 2754/2802 [05:05<00:05,  9.27it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(48, 48), (51, 51)], [(41, 48), (53, 53)], [(75, 79), (104, 104), (145, 149), (157, 161), (182, 182), (210, 210), (221, 221), (232, 236), (256, 256), (278, 278), (288, 288)], [(123, 127), (242, 245), (248, 248), (267, 268), (270, 270), (272, 272)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(108, 113), (115, 115), (118, 119)], [(156, 171), (174, 175)], [(204, 206), (328, 329), (341, 341)], [(503, 514), (520, 520), (530, 530), (564, 564)], [(151, 171), (554, 554)], [(564, 565), (568, 568), (579, 579)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2982, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 4.8261, loss: 2.3661 ||:  98%|#########8| 2756/2802 [05:06<00:05,  8.01it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(15, 16), (19, 19)], [(115, 123), (150, 150)], [(23, 27), (180, 184), (196, 196), (213, 213)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(15, 31), (33, 34), (56, 56), (80, 80), (121, 121), (133, 133), (187, 187), (193, 193)], [(82, 82), (118, 118)], [(229, 229), (234, 235), (239, 239), (246, 246), (269, 269)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4458, batch_loss: 2.0058, loss: 2.3652 ||:  98%|#########8| 2758/2802 [05:06<00:04,  8.95it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(58, 60), (78, 80), (94, 96)], [(163, 169), (188, 188), (195, 195), (207, 207), (210, 210)], [(198, 199), (204, 205)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(22, 22), (26, 26), (35, 35), (218, 218), (305, 305)], [(59, 59), (80, 83), (112, 115)], [(72, 78), (96, 97), (172, 173)], [(140, 140), (159, 161)], [(261, 262), (291, 292)], [(257, 257), (312, 312)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4458, batch_loss: 5.2286, loss: 2.3656 ||:  99%|#########8| 2760/2802 [05:06<00:04,  9.75it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(23, 24), (85, 85), (90, 90), (94, 94), (147, 148)], [(15, 18), (118, 119)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(12, 12), (19, 20)], [(156, 164), (227, 230), (235, 235)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2980, coref_f1: 0.4464, mention_recall: 0.4457, batch_loss: 2.1891, loss: 2.3657 ||:  99%|#########8| 2762/2802 [05:06<00:04,  9.80it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(18, 18), (27, 27)], [(47, 49), (64, 64), (84, 84)], [(141, 141), (148, 148), (225, 225)], [(168, 168), (181, 181), (193, 193), (220, 220), (228, 228), (231, 231), (241, 241)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(90, 90), (92, 93), (106, 107), (117, 117), (130, 130), (196, 196), (215, 215), (245, 245)], [(154, 157), (162, 162)], [(163, 165), (167, 167)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4458, batch_loss: 0.4563, loss: 2.3642 ||:  99%|#########8| 2764/2802 [05:06<00:03, 10.32it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(42, 42), (122, 124)], [(4, 4), (158, 158), (295, 296), (299, 301)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(16, 20), (24, 24), (32, 32)], [(129, 133), (135, 137), (160, 162)], [(218, 227), (251, 252)], [(215, 227), (382, 382), (417, 417)], [(386, 401), (411, 411)], [(434, 436), (451, 451), (472, 474)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2980, coref_f1: 0.4464, mention_recall: 0.4457, batch_loss: 4.7131, loss: 2.3642 ||:  99%|#########8| 2766/2802 [05:07<00:04,  8.78it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(14, 15), (49, 50), (144, 145)], [(10, 11), (63, 64), (140, 141)], [(20, 22), (104, 105), (128, 131)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (84, 86)], [(17, 18), (106, 107), (112, 114), (183, 183), (201, 201), (267, 267)], [(201, 202), (216, 216), (224, 225)], [(331, 336), (346, 346)], [(350, 363), (365, 365)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2980, coref_f1: 0.4464, mention_recall: 0.4457, batch_loss: 4.8278, loss: 2.3643 ||:  99%|#########8| 2768/2802 [05:07<00:03,  9.09it/s]predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(136, 143), (146, 146)], [(74, 75), (151, 151), (200, 207), (230, 230), (282, 286)], [(180, 182), (187, 187)], [(47, 47), (243, 244)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(14, 14), (25, 25)], [(42, 43), (75, 75), (89, 89), (106, 106), (119, 119), (159, 159), (247, 248), (278, 278), (289, 289), (292, 292), (295, 295), (310, 310), (334, 334), (384, 384), (393, 393), (400, 400), (419, 419), (431, 431), (436, 436), (445, 445), (449, 449), (462, 462)], [(201, 203), (233, 233)], [(365, 365), (387, 387)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4457, batch_loss: 0.5687, loss: 2.3629 ||:  99%|#########8| 2770/2802 [05:07<00:03,  8.47it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(86, 87), (128, 128), (163, 163)], [(110, 110), (133, 133), (137, 137)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(3, 3), (14, 14), (19, 19), (59, 59), (61, 61), (64, 64)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2980, coref_f1: 0.4464, mention_recall: 0.4457, batch_loss: 3.2524, loss: 2.3626 ||:  99%|#########8| 2772/2802 [05:07<00:03,  9.66it/s]predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(16, 17), (80, 81), (198, 198)], [(27, 40), (88, 88), (219, 220)], [(4, 8), (95, 98), (108, 108), (203, 206), (210, 211)], [(161, 161), (167, 167)], [(11, 14), (177, 184), (190, 190), (208, 208), (235, 236), (281, 281), (342, 342)], [(259, 260), (264, 264)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(0, 7), (33, 34)], [(16, 31), (45, 45)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4457, batch_loss: 2.7970, loss: 2.3627 ||:  99%|#########9| 2774/2802 [05:08<00:02,  9.88it/s]predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(191, 196), (212, 217)], [(73, 75), (231, 233)], [(252, 252), (260, 260)], [(133, 134), (330, 331)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(77, 79), (153, 155), (158, 158), (180, 180)], [(73, 75), (183, 184)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2981, coref_f1: 0.4465, mention_recall: 0.4457, batch_loss: 0.8346, loss: 2.3615 ||:  99%|#########9| 2776/2802 [05:08<00:02,  9.96it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(24, 30), (50, 50), (67, 73), (130, 131), (155, 156)], [(39, 42), (91, 96)], [(165, 194), (229, 231)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(3, 6), (41, 44), (149, 149)], [(117, 117), (130, 130), (137, 137)], [(35, 35), (251, 251), (276, 276), (281, 281), (285, 285), (292, 292), (306, 306), (312, 312), (321, 321), (326, 326)], [(245, 245), (267, 267), (273, 273)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 11.7334, loss: 2.3674 ||:  99%|#########9| 2778/2802 [05:08<00:02,  9.83it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (73, 73), (81, 84), (94, 94), (292, 292), (304, 304), (309, 309), (313, 313)], [(52, 52), (134, 134), (168, 168), (194, 194), (221, 221), (235, 235), (244, 244), (280, 280)], [(250, 250), (294, 296)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(24, 24), (92, 92), (101, 101), (104, 104)], [(79, 79), (120, 120), (122, 122), (143, 143), (149, 149), (152, 152), (159, 159), (162, 162), (174, 178), (195, 195), (207, 207)], [(199, 201), (209, 209), (225, 225), (228, 228), (232, 232)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 0.1113, loss: 2.3660 ||:  99%|#########9| 2780/2802 [05:08<00:02, 10.29it/s] predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(17, 22), (30, 30)], [(4, 5), (75, 76), (135, 136)], [(85, 88), (145, 148)], [(167, 171), (178, 178)], [(191, 192), (202, 202)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(6, 10), (16, 18), (81, 84)], [(30, 37), (45, 52), (59, 59)], [(185, 186), (190, 193)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8983, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 5.1043, loss: 2.3674 ||:  99%|#########9| 2782/2802 [05:08<00:01, 11.03it/s]predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(116, 119), (128, 128)], [(193, 207), (215, 215), (218, 218), (248, 248), (251, 251), (255, 268), (295, 295), (308, 308), (317, 317), (326, 326), (332, 332), (339, 339), (345, 345), (348, 348), (352, 352)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4458, batch_loss: 2.9831, loss: 2.3668 ||:  99%|#########9| 2784/2802 [05:09<00:01,  9.44it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (4, 4), (27, 28), (69, 70), (73, 80), (121, 123)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 0.2926, loss: 2.3653 ||:  99%|#########9| 2786/2802 [05:09<00:01, 10.52it/s]predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(7, 10), (127, 127), (169, 170), (175, 175), (218, 218)], [(222, 222), (278, 278), (281, 281)], [(147, 147), (284, 285), (288, 288)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(37, 39), (41, 41)], [(85, 92), (133, 133), (144, 144), (182, 182)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 1.7182, loss: 2.3655 ||: 100%|#########9| 2788/2802 [05:09<00:01, 10.78it/s]predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (27, 27)], [(9, 25), (53, 68), (109, 109)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(41, 41), (59, 59), (71, 71)], [(161, 162), (200, 201)], [(225, 229), (231, 232), (234, 234)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 0.1931, loss: 2.3644 ||: 100%|#########9| 2790/2802 [05:09<00:01, 11.20it/s]predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(126, 127), (130, 130)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 21])\n",
            "predicted_antecedents_float size torch.Size([1, 21])\n",
            "probs_logs size: torch.Size([1, 21, 6])\n",
            "probs size: torch.Size([1, 21, 6])\n",
            "probs_flat size: torch.Size([1, 126])\n",
            "gold_antecedent_labels size: torch.Size([1, 21, 6])\n",
            "clusters (batch_clusters) [[[(20, 23), (59, 62)], [(21, 21), (60, 60)], [(186, 189), (227, 227)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8984, coref_recall: 0.2982, coref_f1: 0.4466, mention_recall: 0.4458, batch_loss: 0.1853, loss: 2.3629 ||: 100%|#########9| 2792/2802 [05:09<00:00, 11.73it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(39, 42), (44, 44), (52, 52), (93, 93)], [(52, 53), (124, 125), (128, 128), (200, 201), (277, 278)], [(136, 137), (140, 140)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(4, 4), (9, 9), (29, 30), (51, 51), (53, 53), (65, 65), (80, 80), (87, 87), (114, 114), (133, 134), (179, 179), (184, 184), (189, 189), (193, 193), (200, 200), (203, 203), (215, 215), (238, 238), (255, 255), (279, 280), (289, 289), (317, 318), (331, 331), (343, 343), (350, 350), (358, 358), (366, 367), (427, 427)], [(299, 301), (308, 308), (421, 421)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2983, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 1.8994, loss: 2.3620 ||: 100%|#########9| 2794/2802 [05:09<00:00, 10.43it/s]predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(0, 12), (34, 34), (58, 58), (80, 82), (94, 94), (144, 146), (174, 175), (194, 195), (437, 437)], [(150, 175), (205, 208)], [(241, 241), (250, 251)], [(105, 109), (267, 267), (303, 306)], [(369, 377), (384, 384)], [(566, 566), (570, 570)], [(594, 602), (614, 614)], [(614, 616), (630, 630)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8985, coref_recall: 0.2983, coref_f1: 0.4468, mention_recall: 0.4459, batch_loss: 4.0961, loss: 2.3618 ||: 100%|#########9| 2796/2802 [05:10<00:00,  9.06it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(33, 39), (47, 48), (61, 63), (156, 156), (305, 306), (317, 318), (383, 384), (414, 415)], [(295, 303), (363, 365)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 4.4809, loss: 2.3626 ||: 100%|#########9| 2797/2802 [05:10<00:00,  8.29it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (65, 65)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(14, 20), (59, 61), (120, 143), (164, 164), (167, 167)], [(99, 101), (103, 105)], [(59, 59), (174, 177), (266, 268), (310, 312)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4467, mention_recall: 0.4459, batch_loss: 0.9235, loss: 2.3613 ||: 100%|#########9| 2799/2802 [05:10<00:00,  9.13it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(144, 144), (149, 150), (153, 153), (157, 157)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(67, 81), (76, 76)], [(25, 25), (101, 102)], [(172, 174), (177, 177), (179, 179)], [(152, 156), (199, 199), (207, 207)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2984, coref_f1: 0.4468, mention_recall: 0.4460, batch_loss: 1.6275, loss: 2.3603 ||: 100%|#########9| 2801/2802 [05:10<00:00,  9.94it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(12, 14), (219, 219)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.8986, coref_recall: 0.2983, coref_f1: 0.4468, mention_recall: 0.4460, batch_loss: 4.9574, loss: 2.3613 ||: 100%|##########| 2802/2802 [05:10<00:00,  9.01it/s]\n",
            "2021-09-30 14:34:29,463 - INFO - allennlp.training.gradient_descent_trainer - Validating\n",
            "  0%|          | 0/343 [00:00<?, ?it/s]predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(20, 34), (47, 54), (185, 194)], [(73, 78), (82, 83), (95, 95), (117, 117)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(24, 24), (71, 72)], [(42, 42), (77, 77), (102, 102), (122, 122), (125, 125), (142, 142), (147, 147)], [(80, 96), (98, 98)], [(162, 168), (172, 172), (239, 244)], [(226, 228), (251, 253)], [(211, 213), (312, 314)], [(277, 284), (411, 414)], [(377, 378), (445, 446)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.5660, coref_recall: 0.3031, coref_f1: 0.3929, mention_recall: 0.4490, batch_loss: 45.3587, loss: 15.7223 ||:   1%|          | 3/343 [00:00<00:14, 24.24it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(62, 62), (78, 78), (80, 80), (88, 88)], [(18, 25), (105, 112), (137, 144)], [(93, 93), (149, 149)], [(115, 125), (155, 165)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 63])\n",
            "predicted_antecedents_float size torch.Size([1, 63])\n",
            "probs_logs size: torch.Size([1, 63, 6])\n",
            "probs size: torch.Size([1, 63, 6])\n",
            "probs_flat size: torch.Size([1, 378])\n",
            "gold_antecedent_labels size: torch.Size([1, 63, 6])\n",
            "clusters (batch_clusters) [[[(47, 49), (75, 78), (95, 95), (108, 110), (139, 145), (418, 419), (427, 428), (430, 430), (476, 476), (484, 484), (501, 501), (554, 555), (563, 563), (567, 567), (579, 579), (588, 588), (592, 592), (599, 599), (609, 609), (619, 619), (627, 627), (659, 659), (665, 665), (679, 679)], [(154, 158), (164, 164)], [(191, 193), (201, 202), (212, 213)], [(231, 237), (245, 245)], [(136, 137), (256, 257), (287, 287)], [(622, 624), (743, 744)], [(747, 747), (760, 760), (764, 764)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(60, 60), (67, 67), (89, 89), (171, 171)], [(100, 105), (111, 111), (138, 139), (146, 146), (152, 152)], [(53, 53), (115, 115), (325, 326), (462, 463)], [(181, 181), (194, 194), (449, 449), (468, 468), (490, 490)], [(188, 188), (223, 223), (228, 228), (231, 231), (249, 249), (261, 261), (313, 313)], [(398, 398), (487, 487)], [(528, 528), (534, 534), (539, 539), (541, 541), (545, 545)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6886, coref_recall: 0.2742, coref_f1: 0.3903, mention_recall: 0.4225, batch_loss: 28.1501, loss: 16.4363 ||:   2%|1         | 6/343 [00:00<00:19, 16.99it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (22, 22), (30, 46), (34, 46), (48, 48), (105, 106), (111, 111), (129, 130), (132, 132), (147, 148), (196, 198)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 57])\n",
            "predicted_antecedents_float size torch.Size([1, 57])\n",
            "probs_logs size: torch.Size([1, 57, 6])\n",
            "probs size: torch.Size([1, 57, 6])\n",
            "probs_flat size: torch.Size([1, 342])\n",
            "gold_antecedent_labels size: torch.Size([1, 57, 6])\n",
            "clusters (batch_clusters) [[[(3, 5), (32, 34), (72, 75), (101, 103), (158, 159), (172, 173), (194, 196), (200, 202), (260, 261), (263, 263), (271, 271)], [(3, 7), (32, 36)], [(145, 147), (151, 151), (164, 166), (168, 170), (714, 716)], [(428, 435), (500, 507)], [(476, 488), (512, 515), (611, 615)], [(645, 651), (670, 670)], [(61, 62), (704, 705)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7264, coref_recall: 0.2961, coref_f1: 0.4190, mention_recall: 0.4510, batch_loss: 18.1596, loss: 15.0263 ||:   2%|2         | 8/343 [00:00<00:23, 14.46it/s]predicted_antecedents size torch.Size([1, 78])\n",
            "predicted_antecedents_float size torch.Size([1, 78])\n",
            "probs_logs size: torch.Size([1, 78, 6])\n",
            "probs size: torch.Size([1, 78, 6])\n",
            "probs_flat size: torch.Size([1, 468])\n",
            "gold_antecedent_labels size: torch.Size([1, 78, 6])\n",
            "clusters (batch_clusters) [[[(9, 10), (22, 22), (41, 41)], [(39, 41), (54, 54)], [(99, 99), (111, 111), (132, 132), (143, 143)], [(199, 200), (202, 202), (210, 210), (949, 954)], [(60, 63), (231, 234)], [(184, 185), (245, 246)], [(317, 317), (321, 321), (325, 325), (329, 329), (349, 349)], [(429, 432), (436, 436)], [(590, 591), (605, 607)], [(649, 651), (711, 711), (724, 724), (734, 734), (737, 737), (796, 798)], [(730, 730), (749, 749), (761, 761), (776, 776)], [(778, 778), (792, 792)], [(922, 922), (932, 932), (939, 939)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 114])\n",
            "predicted_antecedents_float size torch.Size([1, 114])\n",
            "probs_logs size: torch.Size([1, 114, 6])\n",
            "probs size: torch.Size([1, 114, 6])\n",
            "probs_flat size: torch.Size([1, 684])\n",
            "gold_antecedent_labels size: torch.Size([1, 114, 6])\n",
            "clusters (batch_clusters) [[[(165, 165), (178, 178)], [(214, 215), (224, 225), (288, 289)], [(139, 143), (292, 293), (308, 308), (316, 316)], [(61, 65), (355, 356), (617, 621)], [(425, 426), (430, 430), (472, 474)], [(520, 521), (571, 571)], [(283, 284), (578, 579), (672, 673)], [(473, 473), (756, 757)], [(894, 895), (899, 899)], [(335, 337), (1055, 1057)], [(903, 911), (1194, 1198)], [(1256, 1256), (1313, 1315)], [(1253, 1254), (1341, 1342)], [(1374, 1387), (1396, 1396)], [(1399, 1400), (1429, 1430)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6832, coref_recall: 0.3078, coref_f1: 0.4229, mention_recall: 0.4968, batch_loss: 54.4623, loss: 20.8199 ||:   3%|2         | 10/343 [00:00<00:33, 10.07it/s]predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(5, 8), (24, 24), (31, 31), (45, 45), (78, 78), (104, 110), (130, 130)], [(17, 29), (139, 140)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(51, 51), (67, 67)], [(88, 88), (125, 125), (127, 127), (134, 134), (185, 185)], [(156, 159), (176, 179), (182, 182)], [(204, 204), (208, 208)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(7, 13), (24, 24)], [(32, 37), (59, 60)], [(62, 76), (89, 89), (217, 218)], [(124, 125), (130, 130), (141, 141), (146, 146), (206, 207), (258, 260), (297, 298)], [(210, 212), (221, 221), (231, 231)], [(331, 334), (343, 343)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6819, coref_recall: 0.3196, coref_f1: 0.4341, mention_recall: 0.5113, batch_loss: 25.6048, loss: 19.8877 ||:   4%|3         | 13/343 [00:00<00:24, 13.75it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(3, 5), (4, 5)], [(19, 21), (41, 47)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (33, 33), (62, 62), (71, 71), (122, 122), (142, 142), (156, 156), (188, 188), (206, 206), (228, 228), (256, 256), (301, 303), (314, 314)], [(107, 108), (115, 115)], [(53, 54), (219, 220)], [(252, 256), (259, 259), (263, 263)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(30, 32), (60, 62)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6965, coref_recall: 0.3272, coref_f1: 0.4442, mention_recall: 0.5146, batch_loss: 2.6560, loss: 15.4892 ||:   5%|4         | 17/343 [00:01<00:17, 18.92it/s] predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(0, 23), (38, 38), (45, 45), (63, 63), (68, 68), (293, 293), (482, 482), (592, 592), (669, 669), (679, 679), (688, 688), (695, 695), (701, 701), (715, 715), (723, 723), (733, 733), (743, 743), (752, 752), (764, 764), (773, 773), (791, 791), (810, 810), (843, 843)], [(114, 116), (123, 123), (147, 147), (160, 160), (163, 163)], [(49, 50), (129, 130), (158, 158), (172, 172), (190, 190), (224, 224), (284, 284), (289, 289), (339, 341), (602, 603), (606, 606), (616, 616), (756, 757), (769, 769), (775, 775), (797, 797), (819, 819), (834, 834)], [(350, 351), (356, 356)], [(329, 331), (395, 396), (409, 410), (432, 433), (442, 442), (454, 455), (465, 466)], [(461, 461), (536, 537), (573, 574), (635, 635)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(5, 8), (11, 12), (429, 430)], [(1, 1), (48, 48), (56, 56), (152, 152), (213, 213), (223, 223), (236, 236), (266, 266), (268, 268), (274, 274), (355, 355), (395, 395), (409, 409), (412, 412), (453, 453), (481, 481), (484, 484), (511, 511), (531, 531), (549, 549), (571, 571), (580, 580), (614, 614)], [(59, 59), (63, 63), (122, 122), (145, 145)], [(226, 226), (240, 240)], [(302, 302), (324, 324), (340, 340)], [(292, 293), (332, 332)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6676, coref_recall: 0.3024, coref_f1: 0.4159, mention_recall: 0.4816, batch_loss: 48.8605, loss: 19.9642 ||:   6%|5         | 20/343 [00:01<00:16, 19.13it/s]predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(9, 23), (59, 59)], [(25, 25), (89, 91), (567, 568), (587, 590)], [(98, 99), (123, 131), (167, 170), (563, 563)], [(233, 234), (240, 242)], [(154, 155), (269, 270)], [(189, 189), (272, 272), (317, 317), (328, 328), (344, 344), (426, 426), (443, 443), (477, 477), (504, 504), (581, 581), (677, 677), (694, 694), (696, 696)], [(221, 222), (285, 286)], [(390, 390), (398, 398), (412, 412)], [(435, 435), (454, 454)], [(620, 632), (637, 637), (640, 640), (713, 714)], [(743, 743), (752, 753)], [(772, 772), (794, 794), (817, 817)], [(789, 789), (826, 826)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 60])\n",
            "predicted_antecedents_float size torch.Size([1, 60])\n",
            "probs_logs size: torch.Size([1, 60, 6])\n",
            "probs size: torch.Size([1, 60, 6])\n",
            "probs_flat size: torch.Size([1, 360])\n",
            "gold_antecedent_labels size: torch.Size([1, 60, 6])\n",
            "clusters (batch_clusters) [[[(27, 28), (66, 67), (83, 84), (143, 144), (157, 157), (184, 185), (192, 192)], [(58, 58), (79, 80)], [(14, 15), (117, 118)], [(121, 121), (135, 135), (176, 176)], [(101, 103), (239, 248), (246, 248)], [(221, 248), (301, 302)], [(368, 368), (378, 378), (451, 454), (515, 518), (604, 607), (691, 691)], [(472, 476), (484, 484)], [(495, 495), (508, 508), (597, 597), (648, 648), (662, 662)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(45, 48), (58, 58), (74, 74), (145, 145), (160, 161), (177, 177), (183, 183)], [(32, 36), (84, 86), (112, 114), (169, 173)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6492, coref_recall: 0.2929, coref_f1: 0.4034, mention_recall: 0.4713, batch_loss: 3.1447, loss: 21.5565 ||:   7%|6         | 23/343 [00:01<00:19, 16.30it/s] predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(10, 13), (35, 37), (47, 48), (58, 59)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 62])\n",
            "predicted_antecedents_float size torch.Size([1, 62])\n",
            "probs_logs size: torch.Size([1, 62, 6])\n",
            "probs size: torch.Size([1, 62, 6])\n",
            "probs_flat size: torch.Size([1, 372])\n",
            "gold_antecedent_labels size: torch.Size([1, 62, 6])\n",
            "clusters (batch_clusters) [[[(22, 25), (27, 32), (51, 52), (133, 133), (156, 157), (179, 179), (196, 197), (281, 287), (632, 633), (641, 641), (677, 678), (745, 746)], [(219, 219), (227, 227), (248, 248), (307, 307)], [(289, 290), (304, 304)], [(422, 422), (434, 434), (440, 440)], [(563, 565), (606, 608), (610, 610), (615, 615), (620, 620)], [(243, 243), (649, 649)], [(708, 710), (720, 720), (732, 732), (749, 749), (777, 777)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6465, coref_recall: 0.2870, coref_f1: 0.3973, mention_recall: 0.4701, batch_loss: 38.7514, loss: 21.3854 ||:   7%|7         | 25/343 [00:01<00:19, 16.31it/s]predicted_antecedents size torch.Size([1, 72])\n",
            "predicted_antecedents_float size torch.Size([1, 72])\n",
            "probs_logs size: torch.Size([1, 72, 6])\n",
            "probs size: torch.Size([1, 72, 6])\n",
            "probs_flat size: torch.Size([1, 432])\n",
            "gold_antecedent_labels size: torch.Size([1, 72, 6])\n",
            "clusters (batch_clusters) [[[(5, 11), (33, 34)], [(37, 37), (39, 39)], [(26, 29), (60, 62), (70, 70), (129, 130), (422, 423), (444, 444), (455, 455), (528, 530), (546, 549), (572, 572), (829, 829), (853, 854)], [(135, 136), (149, 149), (166, 166), (189, 189)], [(8, 11), (158, 161)], [(304, 305), (308, 308)], [(371, 373), (381, 383)], [(480, 482), (490, 490), (536, 537), (551, 551), (580, 581), (622, 624), (839, 841), (891, 892)], [(583, 583), (588, 588)], [(622, 629), (647, 648)], [(731, 731), (741, 741), (766, 766), (785, 785)], [(874, 874), (886, 886)], [(117, 119), (898, 899)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(0, 5), (8, 8)], [(36, 41), (59, 59)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6655, coref_recall: 0.2868, coref_f1: 0.4005, mention_recall: 0.4658, batch_loss: 0.1723, loss: 20.9108 ||:   8%|7         | 27/343 [00:01<00:20, 15.58it/s] predicted_antecedents size torch.Size([1, 56])\n",
            "predicted_antecedents_float size torch.Size([1, 56])\n",
            "probs_logs size: torch.Size([1, 56, 6])\n",
            "probs size: torch.Size([1, 56, 6])\n",
            "probs_flat size: torch.Size([1, 336])\n",
            "gold_antecedent_labels size: torch.Size([1, 56, 6])\n",
            "clusters (batch_clusters) [[[(31, 31), (33, 33), (42, 42), (72, 72)], [(242, 242), (247, 247), (251, 251)], [(279, 279), (288, 288)], [(301, 301), (318, 318), (325, 325), (501, 501)], [(232, 233), (331, 332)], [(395, 396), (408, 409), (425, 426), (440, 441)], [(413, 413), (421, 421), (428, 428)], [(446, 446), (455, 455), (457, 457)], [(488, 490), (511, 513), (518, 520), (538, 540)], [(503, 503), (534, 535), (545, 545), (554, 554), (563, 563), (578, 581), (588, 588), (676, 679), (692, 693), (696, 696)], [(613, 613), (627, 627)], [(643, 650), (652, 652)], [(62, 62), (703, 703)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(4, 5), (42, 43), (182, 183), (254, 255), (308, 309), (316, 318)], [(32, 37), (92, 96)], [(49, 50), (104, 106), (273, 275), (325, 327), (412, 414), (433, 433), (514, 516), (585, 587)], [(60, 62), (111, 113), (137, 139), (143, 146), (167, 169), (187, 189), (201, 203), (218, 220), (348, 350), (453, 455), (494, 496), (527, 529), (543, 545)], [(449, 451), (465, 465)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.6836, coref_recall: 0.2891, coref_f1: 0.4059, mention_recall: 0.4641, batch_loss: 16.9551, loss: 20.8719 ||:   8%|8         | 29/343 [00:01<00:22, 13.73it/s]predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(44, 44), (57, 57), (72, 75), (103, 103), (127, 127), (163, 163), (171, 171), (254, 255)], [(290, 290), (364, 364)], [(377, 386), (393, 393), (402, 402), (412, 412), (420, 420), (547, 547)], [(466, 470), (472, 472)], [(449, 452), (568, 571)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (20, 24), (65, 66), (81, 82), (105, 106), (128, 130)], [(140, 141), (183, 184)], [(208, 212), (272, 273)], [(38, 38), (313, 314), (682, 682), (747, 749)], [(362, 374), (376, 377)], [(563, 564), (568, 570)], [(563, 566), (582, 582), (594, 594)], [(611, 613), (638, 638), (646, 649)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7010, coref_recall: 0.2892, coref_f1: 0.4090, mention_recall: 0.4671, batch_loss: 6.1903, loss: 19.9590 ||:   9%|9         | 31/343 [00:02<00:25, 12.44it/s] predicted_antecedents size torch.Size([1, 23])\n",
            "predicted_antecedents_float size torch.Size([1, 23])\n",
            "probs_logs size: torch.Size([1, 23, 6])\n",
            "probs size: torch.Size([1, 23, 6])\n",
            "probs_flat size: torch.Size([1, 138])\n",
            "gold_antecedent_labels size: torch.Size([1, 23, 6])\n",
            "clusters (batch_clusters) [[[(70, 71), (86, 86)], [(0, 1), (104, 105), (232, 233), (278, 278)], [(53, 55), (117, 117)], [(67, 67), (252, 252)], [(263, 263), (265, 265)], [(248, 248), (268, 268)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(12, 14), (24, 24), (64, 66), (91, 91), (97, 97), (118, 118), (156, 156), (182, 182), (236, 236), (252, 252), (260, 260), (271, 271), (294, 294), (334, 334), (368, 368), (377, 377), (384, 384), (386, 386), (405, 405), (413, 413)], [(158, 159), (182, 183), (213, 214), (373, 374)], [(201, 201), (206, 206)], [(312, 312), (318, 318), (325, 325)], [(513, 513), (523, 523)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7019, coref_recall: 0.2895, coref_f1: 0.4095, mention_recall: 0.4700, batch_loss: 26.0820, loss: 19.9678 ||:  10%|9         | 33/343 [00:02<00:22, 13.70it/s]predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(52, 53), (65, 66), (109, 110)], [(156, 156), (170, 170), (180, 180), (190, 190), (194, 194), (197, 197)], [(228, 228), (242, 244)], [(238, 244), (246, 246), (260, 260), (262, 262), (272, 272)], [(100, 102), (282, 284)], [(380, 388), (427, 434), (442, 442)], [(479, 480), (485, 485)], [(507, 509), (517, 517), (519, 521), (526, 526), (531, 533)], [(537, 540), (542, 545), (567, 570)], [(474, 477), (560, 563)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 91])\n",
            "predicted_antecedents_float size torch.Size([1, 91])\n",
            "probs_logs size: torch.Size([1, 91, 6])\n",
            "probs size: torch.Size([1, 91, 6])\n",
            "probs_flat size: torch.Size([1, 546])\n",
            "gold_antecedent_labels size: torch.Size([1, 91, 6])\n",
            "clusters (batch_clusters) [[[(7, 23), (39, 40), (122, 123), (156, 158), (169, 171), (178, 178), (182, 183), (226, 228), (237, 237), (247, 247), (410, 412), (421, 421), (431, 433), (436, 436), (454, 456), (458, 458), (513, 513), (571, 573), (593, 596), (628, 630), (677, 678), (693, 695), (701, 703), (706, 706), (802, 804)], [(115, 117), (258, 259), (849, 850), (853, 853), (946, 947), (997, 999)], [(537, 540), (543, 543)], [(601, 604), (612, 612)], [(741, 741), (751, 751)], [(838, 840), (955, 965), (1094, 1096)], [(1117, 1117), (1136, 1136)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7042, coref_recall: 0.2823, coref_f1: 0.4025, mention_recall: 0.4641, batch_loss: 81.1946, loss: 21.4787 ||:  10%|#         | 35/343 [00:02<00:24, 12.36it/s]predicted_antecedents size torch.Size([1, 89])\n",
            "predicted_antecedents_float size torch.Size([1, 89])\n",
            "probs_logs size: torch.Size([1, 89, 6])\n",
            "probs size: torch.Size([1, 89, 6])\n",
            "probs_flat size: torch.Size([1, 534])\n",
            "gold_antecedent_labels size: torch.Size([1, 89, 6])\n",
            "clusters (batch_clusters) [[[(41, 46), (59, 59)], [(50, 51), (70, 71)], [(67, 67), (106, 106)], [(148, 148), (156, 156), (185, 185), (215, 215), (223, 223), (261, 261), (283, 283), (900, 902)], [(0, 1), (193, 197), (247, 248)], [(199, 199), (263, 263), (818, 821)], [(403, 404), (408, 408), (456, 456), (465, 465), (477, 477), (485, 485), (958, 958), (964, 964)], [(440, 440), (453, 453), (459, 459), (462, 462), (1045, 1045), (1072, 1072), (1078, 1078), (1081, 1081)], [(551, 551), (624, 624), (666, 666), (693, 693), (717, 717), (723, 723), (742, 742), (751, 751), (1097, 1097), (1110, 1110)], [(795, 796), (801, 801)], [(709, 709), (908, 908), (914, 914), (924, 924), (934, 934), (937, 937), (943, 943), (956, 956), (968, 968), (974, 974), (980, 980), (982, 982), (989, 989), (992, 992), (1011, 1011), (1017, 1017), (1024, 1024), (1030, 1030), (1037, 1037), (1043, 1043)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7145, coref_recall: 0.2787, coref_f1: 0.4003, mention_recall: 0.4561, batch_loss: 0.0068, loss: 20.7707 ||:  11%|#         | 37/343 [00:02<00:23, 12.87it/s] predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(3, 3), (5, 5), (23, 23), (38, 38), (40, 40), (179, 179), (188, 188), (191, 191), (224, 224), (270, 270), (283, 283), (292, 292), (305, 305), (314, 314), (328, 328), (335, 335), (341, 341), (357, 357), (442, 442), (536, 536)], [(104, 105), (131, 131), (149, 150)], [(52, 55), (379, 379), (387, 387), (499, 499), (504, 504), (527, 527), (539, 539), (550, 550), (602, 602)], [(506, 514), (513, 513)], [(520, 527), (524, 524)], [(612, 612), (615, 615)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 103])\n",
            "predicted_antecedents_float size torch.Size([1, 103])\n",
            "probs_logs size: torch.Size([1, 103, 6])\n",
            "probs size: torch.Size([1, 103, 6])\n",
            "probs_flat size: torch.Size([1, 618])\n",
            "gold_antecedent_labels size: torch.Size([1, 103, 6])\n",
            "clusters (batch_clusters) [[[(14, 16), (30, 33), (219, 220), (307, 308), (408, 408), (419, 419), (593, 593), (678, 679), (815, 815), (827, 827), (847, 848), (852, 852), (863, 863), (943, 943), (965, 965), (980, 980), (1020, 1020), (1055, 1055), (1061, 1061), (1092, 1092), (1112, 1113), (1117, 1119), (1157, 1157), (1171, 1171), (1208, 1209)], [(30, 38), (41, 41), (163, 170)], [(118, 119), (148, 148), (176, 178), (365, 382), (415, 415), (427, 428), (439, 440), (454, 456), (474, 474), (536, 537), (578, 578), (919, 920), (969, 970)], [(211, 221), (224, 224), (255, 255), (258, 258), (264, 264), (268, 268), (509, 509), (515, 515), (613, 613), (617, 617), (704, 723), (719, 719), (823, 824), (840, 840), (1222, 1223), (1225, 1225), (1241, 1242), (1261, 1261), (1265, 1265), (1270, 1270), (1283, 1283)], [(187, 188), (387, 399)], [(463, 464), (479, 480), (484, 484)], [(411, 413), (517, 517)], [(771, 775), (802, 803)], [(969, 971), (1034, 1034)], [(1029, 1029), (1037, 1037), (1040, 1040), (1047, 1047)], [(1246, 1246), (1253, 1253), (1257, 1257)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7063, coref_recall: 0.2735, coref_f1: 0.3934, mention_recall: 0.4522, batch_loss: 221.4017, loss: 25.6187 ||:  11%|#1        | 39/343 [00:02<00:28, 10.54it/s]predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (26, 26), (38, 38), (80, 80), (179, 179), (192, 192), (198, 198), (203, 203), (214, 214), (242, 242), (247, 247), (265, 265), (274, 274), (307, 307), (324, 324), (450, 450), (484, 484), (494, 494), (515, 516)], [(42, 42), (46, 46), (112, 112), (207, 207), (222, 222), (258, 258), (298, 298), (337, 337), (359, 359), (392, 392), (429, 429), (437, 437), (469, 469), (481, 481), (491, 491), (497, 497), (504, 504), (518, 518)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(9, 12), (110, 111), (167, 168)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7106, coref_recall: 0.2688, coref_f1: 0.3891, mention_recall: 0.4446, batch_loss: 6.1149, loss: 24.9827 ||:  12%|#1        | 41/343 [00:02<00:25, 11.91it/s]  predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(50, 56), (58, 62), (101, 107)], [(54, 56), (105, 107)], [(45, 47), (141, 143)], [(239, 250), (253, 253)], [(196, 206), (266, 266)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(29, 40), (43, 44), (117, 118), (150, 158), (171, 171), (174, 174), (180, 180), (192, 192), (206, 206), (233, 233), (239, 239), (306, 307), (354, 355)], [(58, 58), (65, 65), (71, 71), (265, 265), (270, 270)], [(160, 162), (197, 197), (208, 210), (215, 215), (313, 315)], [(296, 298), (340, 342), (420, 421)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7065, coref_recall: 0.2674, coref_f1: 0.3870, mention_recall: 0.4415, batch_loss: 12.2213, loss: 24.5126 ||:  13%|#2        | 43/343 [00:03<00:23, 12.85it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(47, 52), (93, 98), (123, 123)], [(103, 105), (112, 112), (138, 139)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 78])\n",
            "predicted_antecedents_float size torch.Size([1, 78])\n",
            "probs_logs size: torch.Size([1, 78, 6])\n",
            "probs size: torch.Size([1, 78, 6])\n",
            "probs_flat size: torch.Size([1, 468])\n",
            "gold_antecedent_labels size: torch.Size([1, 78, 6])\n",
            "clusters (batch_clusters) [[[(76, 76), (82, 82), (85, 85), (91, 91)], [(176, 177), (184, 184)], [(226, 252), (247, 247)], [(218, 219), (254, 255)], [(263, 292), (281, 281), (292, 292)], [(299, 302), (315, 315)], [(97, 97), (542, 542), (713, 713), (858, 858), (883, 883)], [(564, 566), (580, 580), (596, 597)], [(657, 659), (675, 676)], [(734, 739), (835, 837)], [(898, 898), (901, 901), (916, 916), (927, 927), (931, 931), (935, 935), (951, 951), (960, 960)], [(960, 966), (968, 968)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7141, coref_recall: 0.2689, coref_f1: 0.3897, mention_recall: 0.4413, batch_loss: 17.9624, loss: 23.8239 ||:  13%|#3        | 45/343 [00:03<00:23, 12.70it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(65, 66), (76, 76)], [(87, 90), (104, 105), (120, 120), (125, 125), (138, 138), (143, 143), (150, 150), (154, 154), (177, 177), (181, 181)], [(173, 174), (207, 208)], [(223, 226), (259, 260)], [(318, 321), (325, 325)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(5, 5), (43, 44), (74, 76)], [(19, 29), (59, 60)], [(107, 112), (126, 127)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 66])\n",
            "predicted_antecedents_float size torch.Size([1, 66])\n",
            "probs_logs size: torch.Size([1, 66, 6])\n",
            "probs size: torch.Size([1, 66, 6])\n",
            "probs_flat size: torch.Size([1, 396])\n",
            "gold_antecedent_labels size: torch.Size([1, 66, 6])\n",
            "clusters (batch_clusters) [[[(8, 9), (80, 81), (178, 179), (268, 269), (347, 348), (445, 446), (590, 591), (664, 665), (721, 722), (796, 798)], [(106, 108), (137, 139), (244, 245)], [(43, 48), (148, 153), (169, 172)], [(60, 61), (193, 194), (718, 719), (756, 757), (819, 820)], [(25, 25), (271, 272)], [(335, 338), (361, 363), (410, 410), (410, 411), (449, 451), (478, 481), (524, 524), (558, 558), (562, 562), (593, 594)], [(425, 429), (467, 471)], [(309, 313), (496, 500), (517, 521), (687, 691)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7187, coref_recall: 0.2748, coref_f1: 0.3967, mention_recall: 0.4447, batch_loss: 26.1689, loss: 22.9249 ||:  14%|#3        | 48/343 [00:03<00:21, 13.83it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (7, 7), (15, 15), (24, 24), (26, 26), (35, 35), (36, 36), (44, 44), (76, 76), (81, 81), (83, 83), (94, 94), (96, 96), (100, 100), (120, 120), (143, 143), (161, 161), (174, 174), (201, 201), (211, 211), (258, 258), (300, 300), (314, 314), (315, 315), (320, 320), (321, 321), (328, 328), (331, 331), (339, 339), (357, 357), (369, 369), (430, 430), (480, 480), (501, 501)], [(127, 128), (222, 223), (226, 226), (278, 278), (286, 286), (372, 372), (380, 380), (572, 572)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(8, 8), (21, 21)], [(21, 25), (27, 27)], [(77, 77), (86, 86)], [(108, 108), (161, 162), (179, 181)], [(293, 293), (299, 299)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7192, coref_recall: 0.2734, coref_f1: 0.3954, mention_recall: 0.4412, batch_loss: 12.4235, loss: 22.3834 ||:  15%|#4        | 51/343 [00:03<00:17, 16.50it/s]predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(11, 13), (55, 55)], [(15, 16), (71, 73), (75, 75), (80, 80), (293, 293), (315, 315), (327, 327), (334, 334), (596, 597), (635, 635), (638, 638), (641, 645), (758, 760)], [(196, 201), (228, 230), (252, 254)], [(36, 42), (364, 370), (394, 394)], [(402, 402), (422, 422), (429, 429), (442, 442), (453, 453), (472, 472), (538, 538), (693, 693), (767, 767)], [(158, 160), (515, 516), (668, 669)], [(533, 534), (577, 579)], [(660, 661), (665, 665)], [(677, 678), (697, 697), (773, 773)], [(629, 629), (736, 736)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(22, 29), (81, 83)], [(96, 96), (107, 107)], [(47, 48), (115, 116)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7230, coref_recall: 0.2735, coref_f1: 0.3961, mention_recall: 0.4418, batch_loss: 2.0661, loss: 22.2140 ||:  15%|#5        | 53/343 [00:03<00:18, 15.80it/s] predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (12, 12), (122, 123), (138, 138)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (19, 19), (134, 136), (193, 194), (275, 276)], [(0, 2), (29, 30), (88, 91), (134, 135), (159, 160), (170, 171), (285, 287), (290, 290), (302, 303), (307, 307), (325, 325), (337, 338), (350, 350), (361, 361)], [(72, 74), (85, 85), (95, 97), (120, 120)], [(214, 219), (228, 229), (253, 254)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(30, 30), (52, 52), (65, 65), (71, 71), (109, 109)], [(23, 23), (124, 124), (138, 138), (162, 162), (180, 180), (198, 198), (205, 205), (211, 211), (227, 227), (288, 288), (294, 294), (302, 302), (320, 320), (323, 323), (332, 332), (361, 361)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7293, coref_recall: 0.2751, coref_f1: 0.3987, mention_recall: 0.4414, batch_loss: 9.9171, loss: 21.2112 ||:  16%|#6        | 56/343 [00:03<00:15, 17.99it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (38, 39)], [(92, 100), (114, 116)], [(127, 131), (162, 163)], [(67, 68), (206, 207)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(11, 34), (67, 68), (71, 71), (86, 87), (123, 124)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 40])\n",
            "predicted_antecedents_float size torch.Size([1, 40])\n",
            "probs_logs size: torch.Size([1, 40, 6])\n",
            "probs size: torch.Size([1, 40, 6])\n",
            "probs_flat size: torch.Size([1, 240])\n",
            "gold_antecedent_labels size: torch.Size([1, 40, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (2, 2), (10, 10), (31, 31), (61, 61), (113, 113), (119, 121), (145, 147), (196, 196), (204, 204), (234, 236), (275, 275), (287, 287), (363, 363)], [(97, 97), (137, 137), (211, 211), (216, 216), (229, 229), (253, 253), (259, 259), (266, 266), (333, 333), (404, 404), (496, 496)], [(38, 39), (298, 299), (301, 301), (344, 345), (494, 494), (506, 506)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7254, coref_recall: 0.2736, coref_f1: 0.3966, mention_recall: 0.4409, batch_loss: 52.6523, loss: 21.2663 ||:  17%|#7        | 59/343 [00:03<00:14, 19.61it/s]predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(67, 67), (88, 88)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(18, 18), (29, 29)], [(5, 7), (62, 64), (111, 113)], [(72, 72), (85, 85)], [(0, 2), (88, 90), (259, 261)], [(155, 156), (192, 194)], [(196, 200), (214, 214)], [(181, 194), (216, 216)], [(132, 150), (255, 257)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(70, 73), (109, 111), (233, 236), (262, 265), (274, 277), (293, 296), (321, 324), (373, 376), (398, 401), (444, 447)], [(176, 187), (197, 197)], [(215, 224), (245, 254), (381, 388), (408, 417), (454, 455), (492, 501), (509, 515)], [(316, 324), (359, 360)], [(241, 242), (403, 404), (481, 482)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7260, coref_recall: 0.2747, coref_f1: 0.3979, mention_recall: 0.4402, batch_loss: 5.8628, loss: 20.5346 ||:  18%|#8        | 62/343 [00:04<00:14, 19.38it/s] predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (25, 26)], [(81, 92), (114, 114), (195, 195), (199, 199), (243, 243)], [(168, 170), (173, 173), (178, 178)], [(176, 176), (180, 180)], [(247, 247), (258, 258)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(66, 69), (86, 86)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(8, 9), (12, 12), (158, 158), (174, 174)], [(143, 151), (168, 172), (185, 186)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(28, 31), (45, 48), (79, 83), (138, 138), (188, 191), (204, 204), (274, 277), (299, 299), (324, 327), (381, 384)], [(14, 18), (67, 70)], [(156, 157), (219, 220)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7260, coref_recall: 0.2730, coref_f1: 0.3961, mention_recall: 0.4380, batch_loss: 17.5244, loss: 19.8104 ||:  19%|#9        | 66/343 [00:04<00:12, 21.80it/s]predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(28, 28), (32, 37)], [(23, 24), (41, 42), (108, 109)], [(51, 56), (86, 86), (122, 124), (129, 131), (251, 253), (278, 278), (289, 290), (292, 292), (299, 299), (311, 312), (353, 355), (366, 366), (494, 497)], [(201, 204), (208, 208)], [(183, 184), (213, 214)], [(335, 335), (398, 398), (403, 403)], [(153, 154), (485, 486), (500, 500)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(35, 37), (44, 45), (77, 78), (80, 80)], [(19, 20), (182, 183)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (59, 63), (81, 82), (113, 114), (117, 118)], [(152, 157), (162, 162), (219, 223), (245, 245), (254, 254), (258, 258), (306, 307)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7315, coref_recall: 0.2764, coref_f1: 0.4005, mention_recall: 0.4411, batch_loss: 7.6179, loss: 19.1472 ||:  20%|##        | 69/343 [00:04<00:13, 20.85it/s] predicted_antecedents size torch.Size([1, 52])\n",
            "predicted_antecedents_float size torch.Size([1, 52])\n",
            "probs_logs size: torch.Size([1, 52, 6])\n",
            "probs size: torch.Size([1, 52, 6])\n",
            "probs_flat size: torch.Size([1, 312])\n",
            "gold_antecedent_labels size: torch.Size([1, 52, 6])\n",
            "clusters (batch_clusters) [[[(32, 36), (48, 48), (55, 57), (75, 78), (94, 97), (100, 100), (103, 103), (166, 169), (237, 239), (263, 269), (302, 304), (355, 356), (362, 363), (400, 402), (440, 443), (491, 493), (498, 498), (519, 521), (544, 546), (583, 584), (623, 624), (627, 627), (645, 647)], [(126, 127), (137, 137)], [(90, 92), (176, 177), (313, 315), (589, 589), (602, 602)], [(483, 489), (540, 540)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 40])\n",
            "predicted_antecedents_float size torch.Size([1, 40])\n",
            "probs_logs size: torch.Size([1, 40, 6])\n",
            "probs size: torch.Size([1, 40, 6])\n",
            "probs_flat size: torch.Size([1, 240])\n",
            "gold_antecedent_labels size: torch.Size([1, 40, 6])\n",
            "clusters (batch_clusters) [[[(7, 12), (45, 46), (195, 196), (267, 282), (410, 411)], [(80, 81), (99, 110), (218, 219)], [(115, 116), (124, 124)], [(42, 46), (128, 129), (171, 171), (349, 350), (438, 439), (475, 476), (490, 490)], [(163, 164), (204, 205)], [(311, 311), (339, 339)], [(358, 365), (384, 385)], [(463, 463), (467, 467), (478, 478)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(11, 11), (39, 44), (105, 105)], [(19, 26), (51, 52), (89, 89), (133, 134), (161, 161)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7307, coref_recall: 0.2758, coref_f1: 0.3998, mention_recall: 0.4412, batch_loss: 4.3583, loss: 19.3424 ||:  21%|##        | 72/343 [00:04<00:14, 18.85it/s]predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(57, 57), (64, 64), (77, 78)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(6, 8), (20, 21), (26, 26), (32, 32), (293, 294), (301, 301), (341, 341), (347, 347), (351, 351), (456, 458), (461, 461), (467, 467), (482, 482), (490, 490), (495, 495), (501, 501), (504, 504), (510, 510), (523, 525), (529, 529), (534, 534), (541, 541)], [(139, 139), (147, 147)], [(164, 164), (168, 168), (174, 174), (182, 183), (186, 186), (191, 191), (196, 196), (208, 209), (215, 215), (222, 222), (229, 229), (273, 273), (286, 288)], [(428, 428), (437, 437), (449, 449)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 73])\n",
            "predicted_antecedents_float size torch.Size([1, 73])\n",
            "probs_logs size: torch.Size([1, 73, 6])\n",
            "probs size: torch.Size([1, 73, 6])\n",
            "probs_flat size: torch.Size([1, 438])\n",
            "gold_antecedent_labels size: torch.Size([1, 73, 6])\n",
            "clusters (batch_clusters) [[[(58, 73), (75, 75)], [(7, 8), (91, 95)], [(186, 209), (211, 212)], [(239, 241), (261, 261), (269, 277)], [(307, 310), (327, 328)], [(431, 433), (438, 438)], [(446, 446), (462, 465)], [(496, 498), (500, 500)], [(734, 738), (755, 756)], [(780, 784), (800, 801)], [(833, 835), (838, 838)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7306, coref_recall: 0.2734, coref_f1: 0.3973, mention_recall: 0.4380, batch_loss: 22.0067, loss: 19.0569 ||:  22%|##1       | 75/343 [00:04<00:15, 17.02it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(11, 13), (16, 16)], [(2, 4), (20, 20), (56, 56), (62, 62), (66, 66)], [(23, 24), (35, 35)], [(171, 173), (184, 188)], [(142, 154), (192, 192), (198, 200), (203, 203), (222, 222), (232, 232), (248, 248), (260, 260), (263, 265), (272, 272), (294, 294), (303, 303), (316, 316), (320, 320), (333, 333)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(6, 11), (16, 17), (217, 221), (376, 381)], [(24, 25), (36, 37), (49, 49), (100, 100), (185, 185), (195, 195), (199, 199), (233, 234), (237, 238), (253, 253), (276, 276)], [(43, 44), (214, 215)], [(287, 292), (338, 338), (360, 360), (363, 363)], [(257, 261), (356, 356)], [(386, 386), (396, 396), (398, 398), (411, 411), (431, 431), (446, 446)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (23, 23)], [(11, 13), (68, 68), (79, 79), (89, 89), (96, 96)], [(199, 202), (230, 230), (298, 298)], [(210, 212), (312, 314)], [(358, 360), (444, 446)], [(448, 448), (455, 455)], [(346, 350), (485, 490)], [(517, 517), (522, 522)], [(578, 582), (586, 586)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7279, coref_recall: 0.2694, coref_f1: 0.3926, mention_recall: 0.4334, batch_loss: 32.5643, loss: 19.3242 ||:  23%|##2       | 78/343 [00:04<00:14, 17.85it/s]predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(9, 15), (44, 47)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 55])\n",
            "predicted_antecedents_float size torch.Size([1, 55])\n",
            "probs_logs size: torch.Size([1, 55, 6])\n",
            "probs size: torch.Size([1, 55, 6])\n",
            "probs_flat size: torch.Size([1, 330])\n",
            "gold_antecedent_labels size: torch.Size([1, 55, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (13, 13)], [(10, 10), (19, 19), (49, 49), (71, 71), (86, 86), (91, 91), (97, 97), (100, 100), (103, 103), (109, 109), (118, 118), (132, 132), (135, 135), (153, 153), (164, 164), (174, 174), (183, 183), (194, 194), (234, 234), (394, 394), (423, 423), (451, 451)], [(199, 207), (257, 259), (267, 267), (272, 272), (275, 275), (280, 281), (328, 329), (462, 464), (483, 483), (547, 548), (561, 564), (588, 588), (601, 601), (608, 608)], [(505, 508), (620, 621), (626, 628), (664, 665), (678, 678), (689, 689)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (33, 33), (71, 71), (115, 115), (198, 198), (201, 201), (276, 276), (326, 326), (405, 405), (415, 415), (511, 511), (563, 563)], [(6, 6), (38, 38), (94, 94), (100, 100), (151, 151), (385, 385), (399, 399), (401, 401)], [(63, 64), (73, 73)], [(189, 189), (194, 194), (436, 436), (449, 449), (464, 464), (472, 472), (477, 477), (479, 479), (498, 498), (528, 528), (553, 554)], [(228, 228), (239, 239), (266, 266), (323, 323), (329, 329), (334, 334)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7284, coref_recall: 0.2693, coref_f1: 0.3925, mention_recall: 0.4350, batch_loss: 34.8083, loss: 19.4084 ||:  24%|##3       | 81/343 [00:05<00:14, 18.59it/s]predicted_antecedents size torch.Size([1, 55])\n",
            "predicted_antecedents_float size torch.Size([1, 55])\n",
            "probs_logs size: torch.Size([1, 55, 6])\n",
            "probs size: torch.Size([1, 55, 6])\n",
            "probs_flat size: torch.Size([1, 330])\n",
            "gold_antecedent_labels size: torch.Size([1, 55, 6])\n",
            "clusters (batch_clusters) [[[(22, 22), (31, 31), (41, 41), (43, 43), (46, 46), (48, 48), (58, 58), (67, 67), (80, 80), (84, 84), (87, 87), (98, 98), (100, 100), (106, 106), (108, 108), (121, 121), (127, 127), (129, 129), (135, 135), (143, 143), (152, 152), (166, 166), (175, 175), (205, 205), (282, 282), (284, 284), (298, 298), (305, 305), (320, 320), (329, 329), (425, 425), (441, 441), (444, 444), (460, 460), (473, 473), (482, 482), (499, 499), (508, 508), (518, 518), (578, 578), (585, 585), (589, 589), (595, 595), (609, 609), (614, 614), (628, 628)], [(220, 226), (286, 286)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(9, 10), (24, 25), (44, 45), (83, 84), (121, 122), (199, 200), (233, 235)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(10, 10), (29, 29), (46, 46), (51, 51), (58, 58), (61, 61), (65, 65), (70, 70), (99, 99), (119, 119), (134, 134), (167, 167), (169, 169), (186, 186)], [(211, 212), (216, 216), (223, 223), (230, 230), (233, 233), (276, 276)], [(305, 305), (310, 310), (315, 315), (326, 326), (330, 330)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7299, coref_recall: 0.2718, coref_f1: 0.3955, mention_recall: 0.4362, batch_loss: 6.0830, loss: 19.1915 ||:  24%|##4       | 84/343 [00:05<00:13, 19.82it/s] predicted_antecedents size torch.Size([1, 79])\n",
            "predicted_antecedents_float size torch.Size([1, 79])\n",
            "probs_logs size: torch.Size([1, 79, 6])\n",
            "probs size: torch.Size([1, 79, 6])\n",
            "probs_flat size: torch.Size([1, 474])\n",
            "gold_antecedent_labels size: torch.Size([1, 79, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (9, 9)], [(44, 44), (50, 50)], [(73, 73), (186, 186), (301, 301), (453, 453), (468, 468), (514, 514), (535, 535), (550, 550)], [(205, 205), (210, 210), (220, 222)], [(268, 268), (281, 281), (296, 296), (313, 313), (324, 324), (333, 333), (338, 338), (349, 349)], [(315, 316), (319, 319), (329, 329)], [(168, 169), (579, 583)], [(671, 674), (687, 687)], [(719, 719), (729, 729)], [(758, 759), (772, 772)], [(607, 608), (963, 970)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (27, 28), (77, 78), (180, 181), (228, 233), (249, 250), (257, 258), (275, 276), (291, 292)], [(41, 46), (130, 131)], [(183, 187), (200, 200)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(14, 20), (35, 43)], [(98, 98), (114, 114), (118, 118), (144, 144), (148, 148), (163, 163), (165, 165)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7307, coref_recall: 0.2721, coref_f1: 0.3959, mention_recall: 0.4370, batch_loss: 0.4756, loss: 19.0459 ||:  25%|##5       | 87/343 [00:05<00:13, 18.43it/s]predicted_antecedents size torch.Size([1, 67])\n",
            "predicted_antecedents_float size torch.Size([1, 67])\n",
            "probs_logs size: torch.Size([1, 67, 6])\n",
            "probs size: torch.Size([1, 67, 6])\n",
            "probs_flat size: torch.Size([1, 402])\n",
            "gold_antecedent_labels size: torch.Size([1, 67, 6])\n",
            "clusters (batch_clusters) [[[(14, 27), (29, 29), (37, 37), (131, 132), (148, 149), (158, 158), (176, 176), (187, 187), (192, 192), (202, 202), (208, 208), (210, 210), (225, 225), (230, 230), (277, 278), (280, 280), (286, 286), (292, 292), (307, 307), (458, 458), (518, 519), (633, 633), (673, 675), (709, 710), (726, 727)], [(233, 234), (236, 237)], [(248, 249), (252, 252)], [(524, 524), (528, 528), (534, 534)], [(574, 574), (594, 594), (624, 624), (643, 643), (715, 715), (805, 805), (817, 817)], [(631, 631), (665, 665)], [(686, 686), (719, 719), (763, 763), (822, 822)], [(760, 760), (768, 768)], [(783, 783), (791, 791)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 80])\n",
            "predicted_antecedents_float size torch.Size([1, 80])\n",
            "probs_logs size: torch.Size([1, 80, 6])\n",
            "probs size: torch.Size([1, 80, 6])\n",
            "probs_flat size: torch.Size([1, 480])\n",
            "gold_antecedent_labels size: torch.Size([1, 80, 6])\n",
            "clusters (batch_clusters) [[[(36, 36), (99, 99), (121, 121), (146, 146), (166, 166), (213, 213), (222, 222), (852, 852), (854, 854), (909, 909), (917, 917), (927, 927), (937, 937), (943, 943), (950, 950)], [(246, 246), (254, 254), (269, 269), (275, 275), (280, 280), (289, 289), (294, 294), (493, 493), (502, 502), (748, 748)], [(315, 315), (319, 319), (323, 323), (328, 328), (336, 336), (338, 338), (350, 350), (353, 353), (357, 357), (360, 360), (363, 363), (365, 365), (379, 379), (384, 384), (390, 390), (402, 402), (407, 407), (421, 421), (445, 445), (454, 454), (456, 456), (472, 472), (475, 475), (518, 518), (527, 527), (555, 555), (565, 565), (573, 573), (585, 585), (595, 595), (597, 597), (619, 619), (625, 625), (631, 631), (633, 633), (646, 646), (654, 654), (656, 656), (671, 671), (675, 675)], [(968, 968), (970, 970), (976, 976), (978, 978), (980, 980)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7330, coref_recall: 0.2737, coref_f1: 0.3979, mention_recall: 0.4393, batch_loss: 53.3762, loss: 19.5686 ||:  26%|##5       | 89/343 [00:05<00:17, 14.76it/s]predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(42, 42), (84, 84), (114, 114), (120, 120), (124, 124), (196, 196), (200, 200), (204, 204), (215, 216)], [(9, 12), (163, 173)], [(198, 198), (202, 202), (206, 206), (213, 213), (222, 222)], [(229, 229), (238, 238)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 63])\n",
            "predicted_antecedents_float size torch.Size([1, 63])\n",
            "probs_logs size: torch.Size([1, 63, 6])\n",
            "probs size: torch.Size([1, 63, 6])\n",
            "probs_flat size: torch.Size([1, 378])\n",
            "gold_antecedent_labels size: torch.Size([1, 63, 6])\n",
            "clusters (batch_clusters) [[[(2, 2), (9, 9), (32, 32), (100, 100), (129, 129), (157, 157), (165, 165), (271, 271)], [(6, 6), (10, 10), (36, 36), (59, 59), (66, 66), (178, 178), (512, 512), (766, 766)], [(39, 46), (49, 49)], [(88, 88), (137, 137), (148, 148), (239, 239), (517, 517)], [(280, 280), (286, 286), (294, 294), (302, 302), (307, 307), (319, 319), (342, 342), (350, 350), (363, 363), (371, 371), (376, 376), (605, 605), (664, 664), (675, 675)], [(417, 418), (424, 424), (524, 525)], [(438, 439), (443, 443), (727, 730)], [(600, 600), (614, 614)], [(619, 619), (633, 633), (651, 651), (668, 668)], [(571, 571), (642, 642)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7284, coref_recall: 0.2736, coref_f1: 0.3972, mention_recall: 0.4395, batch_loss: 58.5856, loss: 19.9311 ||:  27%|##6       | 91/343 [00:05<00:16, 15.12it/s]predicted_antecedents size torch.Size([1, 88])\n",
            "predicted_antecedents_float size torch.Size([1, 88])\n",
            "probs_logs size: torch.Size([1, 88, 6])\n",
            "probs size: torch.Size([1, 88, 6])\n",
            "probs_flat size: torch.Size([1, 528])\n",
            "gold_antecedent_labels size: torch.Size([1, 88, 6])\n",
            "clusters (batch_clusters) [[[(12, 24), (29, 29), (34, 34), (42, 42), (49, 49), (59, 59), (75, 75), (142, 142), (176, 176), (182, 182), (188, 188), (241, 241), (308, 308), (325, 325), (385, 385), (390, 390), (426, 426), (447, 447), (518, 518), (545, 545), (556, 556), (570, 570), (579, 579), (644, 644), (650, 650), (681, 681), (717, 717), (807, 807), (815, 815), (843, 843), (861, 861), (871, 871), (890, 890), (894, 894), (926, 926), (931, 931), (941, 941), (948, 948), (955, 955), (966, 966), (969, 969), (989, 989), (1008, 1008), (1025, 1025), (1093, 1095)], [(45, 46), (77, 77), (244, 244), (264, 264), (321, 321)], [(204, 204), (207, 207)], [(68, 69), (328, 329), (336, 336), (380, 380), (392, 392), (402, 402), (415, 415), (419, 419), (491, 492), (503, 503), (702, 703), (792, 792), (840, 840)], [(436, 436), (449, 449)], [(685, 686), (696, 696)], [(854, 855), (877, 879), (971, 972)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 44])\n",
            "predicted_antecedents_float size torch.Size([1, 44])\n",
            "probs_logs size: torch.Size([1, 44, 6])\n",
            "probs size: torch.Size([1, 44, 6])\n",
            "probs_flat size: torch.Size([1, 264])\n",
            "gold_antecedent_labels size: torch.Size([1, 44, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (15, 15), (66, 66), (72, 72), (77, 77), (92, 92), (98, 98), (108, 108), (119, 119), (129, 129), (149, 149), (157, 157), (164, 164), (186, 186), (188, 188), (196, 196), (241, 241), (253, 253), (272, 272), (302, 302), (312, 312), (315, 315), (323, 323), (328, 328), (337, 337), (340, 340), (345, 345), (347, 347), (353, 353), (361, 361), (365, 365), (377, 377), (392, 392), (396, 396), (402, 402), (418, 418), (438, 438), (461, 461), (506, 506), (550, 550)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7218, coref_recall: 0.2745, coref_f1: 0.3972, mention_recall: 0.4416, batch_loss: 23.0979, loss: 20.8463 ||:  27%|##7       | 93/343 [00:05<00:18, 13.66it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(13, 13), (54, 54)], [(96, 101), (113, 113), (127, 128), (147, 148)], [(119, 123), (130, 130)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(43, 46), (48, 48), (72, 74)], [(116, 126), (141, 144)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(34, 35), (58, 58), (63, 63), (68, 68), (78, 78), (92, 92)], [(125, 125), (142, 142), (145, 145), (148, 148)], [(180, 180), (184, 184)], [(193, 193), (220, 221)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7246, coref_recall: 0.2746, coref_f1: 0.3977, mention_recall: 0.4410, batch_loss: 7.6534, loss: 20.3238 ||:  28%|##7       | 96/343 [00:06<00:14, 16.49it/s] predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(14, 16), (64, 64), (70, 70), (106, 106), (113, 113), (137, 137), (145, 145), (149, 149), (159, 159), (175, 175), (191, 191)], [(20, 21), (70, 72), (134, 134)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(51, 55), (63, 68), (105, 111), (110, 110)], [(19, 22), (66, 68), (172, 173), (188, 189)], [(16, 22), (203, 204), (275, 276), (284, 285)], [(235, 238), (245, 245)], [(233, 233), (304, 304), (340, 342), (401, 401)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (78, 78), (107, 107), (152, 152), (172, 172), (187, 187), (198, 198), (241, 241), (244, 244), (253, 253), (262, 262), (276, 276), (284, 284)], [(231, 231), (246, 246), (255, 255)], [(286, 286), (289, 289)], [(312, 313), (365, 366)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7263, coref_recall: 0.2743, coref_f1: 0.3976, mention_recall: 0.4407, batch_loss: 8.5332, loss: 19.8900 ||:  29%|##8       | 99/343 [00:06<00:13, 18.09it/s]predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(57, 59), (140, 142), (146, 147), (188, 190), (228, 230)], [(163, 163), (177, 177)], [(193, 193), (215, 215), (320, 320), (329, 329)], [(258, 268), (278, 278), (285, 285), (289, 289)], [(332, 333), (337, 337)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (70, 71), (119, 121)], [(0, 1), (98, 98)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(25, 29), (50, 51)], [(56, 64), (68, 68), (73, 73), (79, 79), (115, 115), (146, 146), (152, 152), (159, 159), (165, 165), (167, 167), (174, 174), (197, 197), (200, 200), (365, 365), (373, 373), (384, 384)], [(221, 222), (234, 235), (326, 327)], [(446, 446), (454, 454)], [(476, 476), (485, 485), (501, 501)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7291, coref_recall: 0.2749, coref_f1: 0.3987, mention_recall: 0.4413, batch_loss: 17.3779, loss: 19.5019 ||:  30%|##9       | 102/343 [00:06<00:12, 19.34it/s]predicted_antecedents size torch.Size([1, 79])\n",
            "predicted_antecedents_float size torch.Size([1, 79])\n",
            "probs_logs size: torch.Size([1, 79, 6])\n",
            "probs size: torch.Size([1, 79, 6])\n",
            "probs_flat size: torch.Size([1, 474])\n",
            "gold_antecedent_labels size: torch.Size([1, 79, 6])\n",
            "clusters (batch_clusters) [[[(11, 23), (31, 31), (42, 43), (51, 51), (68, 68), (119, 121), (124, 124), (185, 185), (199, 200), (204, 204), (212, 212), (223, 227), (272, 274), (342, 344), (348, 348), (354, 354), (374, 375), (393, 393), (396, 398), (406, 406), (409, 409), (418, 418), (457, 457), (470, 470), (484, 484), (492, 492), (536, 536), (574, 574), (591, 591), (605, 605), (619, 619), (628, 628), (810, 817), (876, 878), (970, 971)], [(99, 100), (151, 151), (269, 274)], [(300, 300), (385, 385), (437, 437), (545, 545), (637, 637), (641, 641), (902, 902), (940, 940)], [(708, 709), (716, 716)], [(882, 882), (905, 905)], [(915, 915), (926, 926), (933, 933)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(18, 34), (72, 79), (132, 133)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 75])\n",
            "predicted_antecedents_float size torch.Size([1, 75])\n",
            "probs_logs size: torch.Size([1, 75, 6])\n",
            "probs size: torch.Size([1, 75, 6])\n",
            "probs_flat size: torch.Size([1, 450])\n",
            "gold_antecedent_labels size: torch.Size([1, 75, 6])\n",
            "clusters (batch_clusters) [[[(10, 21), (65, 67), (110, 112), (191, 193), (265, 267), (327, 329), (390, 391), (442, 444), (480, 482), (484, 484), (539, 539), (617, 619), (664, 666), (722, 724), (811, 813), (828, 830), (887, 889)], [(90, 94), (118, 119), (219, 223), (253, 255), (289, 290), (552, 554), (587, 591), (632, 634), (647, 649), (714, 716), (756, 758)], [(257, 263), (282, 286)], [(493, 495), (562, 565), (678, 680), (695, 697), (701, 703), (726, 728), (817, 819), (850, 852), (881, 881), (899, 901), (908, 908)], [(638, 653), (671, 672)], [(710, 716), (765, 767)], [(656, 661), (777, 782), (842, 843), (860, 861)], [(536, 537), (922, 925)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7294, coref_recall: 0.2762, coref_f1: 0.4000, mention_recall: 0.4440, batch_loss: 14.9795, loss: 19.6643 ||:  31%|###       | 105/343 [00:06<00:15, 15.06it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(11, 17), (123, 129), (176, 182), (223, 223), (245, 245)], [(136, 156), (171, 172)], [(38, 40), (186, 187)], [(107, 108), (280, 282)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(0, 15), (49, 62), (86, 86)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7296, coref_recall: 0.2763, coref_f1: 0.4002, mention_recall: 0.4438, batch_loss: 2.1095, loss: 19.4225 ||:  31%|###1      | 107/343 [00:06<00:15, 15.55it/s] predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(13, 29), (48, 48), (74, 76)], [(24, 28), (84, 88), (105, 109), (130, 131), (165, 169), (241, 245), (258, 262), (273, 273), (301, 304)], [(51, 64), (195, 197)], [(226, 227), (292, 292)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 177])\n",
            "predicted_antecedents_float size torch.Size([1, 177])\n",
            "probs_logs size: torch.Size([1, 177, 6])\n",
            "probs size: torch.Size([1, 177, 6])\n",
            "probs_flat size: torch.Size([1, 1062])\n",
            "gold_antecedent_labels size: torch.Size([1, 177, 6])\n",
            "clusters (batch_clusters) [[[(45, 45), (53, 53)], [(0, 25), (76, 79), (180, 184), (208, 210), (360, 362), (364, 364), (802, 804), (923, 925), (1140, 1143), (1256, 1258), (1383, 1384), (1419, 1420), (1446, 1448), (1454, 1454), (1832, 1835)], [(140, 166), (170, 170)], [(186, 186), (197, 197)], [(370, 381), (383, 383), (390, 390), (401, 401), (1398, 1400)], [(513, 515), (539, 540), (567, 567)], [(573, 577), (583, 583)], [(649, 655), (678, 678), (685, 687), (1837, 1843), (1867, 1868)], [(135, 136), (954, 954), (991, 991)], [(98, 121), (956, 956), (966, 966)], [(1043, 1044), (1080, 1081), (1128, 1129)], [(1056, 1060), (1164, 1164), (2133, 2136), (2141, 2143)], [(1132, 1132), (1168, 1179), (1733, 1751)], [(1183, 1184), (1192, 1192)], [(1270, 1272), (1295, 1295), (1328, 1328)], [(1287, 1290), (1343, 1346)], [(1491, 1491), (1500, 1500), (1509, 1515), (1517, 1517)], [(236, 240), (1670, 1671), (1813, 1814)], [(174, 176), (1682, 1683), (2124, 2126)], [(1755, 1755), (1784, 1784)], [(496, 497), (2055, 2056)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7249, coref_recall: 0.2741, coref_f1: 0.3971, mention_recall: 0.4435, batch_loss: 111.5666, loss: 19.9512 ||:  32%|###2      | 110/343 [00:07<00:19, 12.26it/s]predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(4, 4), (59, 61), (89, 89)], [(10, 21), (75, 78)], [(148, 149), (152, 152), (164, 164), (171, 171)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 51])\n",
            "predicted_antecedents_float size torch.Size([1, 51])\n",
            "probs_logs size: torch.Size([1, 51, 6])\n",
            "probs size: torch.Size([1, 51, 6])\n",
            "probs_flat size: torch.Size([1, 306])\n",
            "gold_antecedent_labels size: torch.Size([1, 51, 6])\n",
            "clusters (batch_clusters) [[[(1, 2), (21, 21), (354, 355), (374, 375)], [(29, 43), (63, 65), (301, 302), (311, 311), (330, 331), (363, 365), (384, 386)], [(42, 43), (73, 76), (351, 358), (491, 492), (523, 531), (547, 548), (572, 572), (581, 581), (585, 585)], [(177, 183), (191, 191)], [(215, 215), (223, 223), (227, 227)], [(256, 264), (266, 266), (316, 316)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7245, coref_recall: 0.2728, coref_f1: 0.3957, mention_recall: 0.4419, batch_loss: 28.2736, loss: 19.8565 ||:  33%|###2      | 112/343 [00:07<00:17, 13.01it/s] predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(0, 8), (18, 19), (86, 86)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 154])\n",
            "predicted_antecedents_float size torch.Size([1, 154])\n",
            "probs_logs size: torch.Size([1, 154, 6])\n",
            "probs size: torch.Size([1, 154, 6])\n",
            "probs_flat size: torch.Size([1, 924])\n",
            "gold_antecedent_labels size: torch.Size([1, 154, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (73, 73), (221, 222), (237, 238), (281, 282), (326, 326), (402, 403), (456, 456), (524, 525), (578, 578), (636, 637), (645, 645), (653, 654), (699, 701), (770, 771), (1697, 1697), (1713, 1714), (1754, 1754), (1774, 1775), (1792, 1792)], [(116, 121), (123, 124), (154, 155), (247, 248)], [(113, 114), (148, 149)], [(197, 197), (208, 208)], [(44, 46), (234, 235)], [(354, 355), (363, 363)], [(386, 386), (398, 399), (409, 410)], [(527, 539), (554, 556), (685, 688)], [(674, 675), (774, 775)], [(833, 841), (846, 847), (854, 854), (886, 886), (917, 918)], [(893, 893), (901, 901), (904, 904), (1010, 1010), (1032, 1032), (1044, 1044), (1053, 1053)], [(924, 925), (942, 942), (971, 971)], [(1079, 1084), (1091, 1091), (1151, 1151), (1157, 1158), (1200, 1201), (1203, 1205), (1262, 1262), (1294, 1295), (1313, 1314)], [(1074, 1076), (1101, 1101), (1123, 1124), (1173, 1173), (1186, 1187), (1229, 1230), (1232, 1234), (1320, 1321)], [(1144, 1155), (1183, 1183), (1194, 1195)], [(1358, 1368), (1397, 1398)], [(1493, 1497), (1541, 1542), (1593, 1594), (1605, 1605), (1623, 1624), (1639, 1639)], [(1548, 1549), (1556, 1556), (1564, 1564)], [(1764, 1770), (1783, 1784)], [(1829, 1837), (1873, 1877)], [(266, 278), (1879, 1879)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7266, coref_recall: 0.2732, coref_f1: 0.3964, mention_recall: 0.4428, batch_loss: 77.4084, loss: 20.2181 ||:  33%|###3      | 114/343 [00:07<00:20, 11.10it/s]predicted_antecedents size torch.Size([1, 137])\n",
            "predicted_antecedents_float size torch.Size([1, 137])\n",
            "probs_logs size: torch.Size([1, 137, 6])\n",
            "probs size: torch.Size([1, 137, 6])\n",
            "probs_flat size: torch.Size([1, 822])\n",
            "gold_antecedent_labels size: torch.Size([1, 137, 6])\n",
            "clusters (batch_clusters) [[[(56, 59), (71, 71), (83, 84), (174, 175), (178, 178)], [(275, 275), (293, 293), (1004, 1006), (1008, 1009), (1020, 1020), (1252, 1266), (1252, 1267), (1282, 1282), (1324, 1324), (1327, 1327), (1404, 1406), (1426, 1426), (1433, 1433)], [(298, 299), (304, 304), (307, 307), (311, 311)], [(369, 372), (386, 386)], [(0, 5), (394, 396)], [(468, 470), (519, 520)], [(219, 219), (617, 621), (689, 690), (699, 699), (716, 716)], [(803, 803), (812, 812)], [(272, 273), (883, 884), (1029, 1030)], [(909, 909), (916, 916), (923, 923)], [(896, 897), (950, 953)], [(1076, 1078), (1090, 1091)], [(1114, 1114), (1117, 1117)], [(1484, 1504), (1519, 1520)], [(1605, 1605), (1614, 1614), (1619, 1619)], [(1679, 1680), (1688, 1688), (1695, 1695), (1707, 1707)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(13, 15), (17, 17), (26, 26)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7234, coref_recall: 0.2740, coref_f1: 0.3968, mention_recall: 0.4430, batch_loss: 0.0168, loss: 20.4306 ||:  34%|###3      | 116/343 [00:07<00:23,  9.76it/s] predicted_antecedents size torch.Size([1, 64])\n",
            "predicted_antecedents_float size torch.Size([1, 64])\n",
            "probs_logs size: torch.Size([1, 64, 6])\n",
            "probs size: torch.Size([1, 64, 6])\n",
            "probs_flat size: torch.Size([1, 384])\n",
            "gold_antecedent_labels size: torch.Size([1, 64, 6])\n",
            "clusters (batch_clusters) [[[(21, 21), (55, 56)], [(73, 75), (84, 84), (88, 88), (133, 133)], [(79, 79), (86, 86)], [(46, 54), (99, 100), (566, 568)], [(130, 136), (144, 144), (154, 154)], [(165, 165), (175, 175)], [(209, 214), (219, 219), (681, 681)], [(228, 230), (237, 239)], [(438, 439), (444, 444), (458, 458), (466, 466)], [(605, 606), (617, 619), (643, 644), (707, 708)], [(608, 610), (646, 648)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(131, 133), (142, 142)], [(181, 182), (189, 189)], [(145, 145), (240, 240)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7218, coref_recall: 0.2734, coref_f1: 0.3960, mention_recall: 0.4438, batch_loss: 4.8847, loss: 20.3542 ||:  34%|###4      | 118/343 [00:07<00:20, 10.90it/s]predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(5, 6), (48, 51), (135, 138), (144, 146)], [(72, 76), (78, 78), (116, 116), (128, 128)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(39, 41), (55, 55)], [(129, 130), (150, 150)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 56])\n",
            "predicted_antecedents_float size torch.Size([1, 56])\n",
            "probs_logs size: torch.Size([1, 56, 6])\n",
            "probs size: torch.Size([1, 56, 6])\n",
            "probs_flat size: torch.Size([1, 336])\n",
            "gold_antecedent_labels size: torch.Size([1, 56, 6])\n",
            "clusters (batch_clusters) [[[(82, 82), (90, 90), (96, 96), (118, 118), (129, 129), (142, 142), (175, 175), (186, 186), (196, 196), (215, 215), (225, 225), (228, 228), (253, 253), (263, 263), (322, 322), (341, 341), (425, 425), (466, 466), (474, 474), (499, 499), (564, 564), (571, 571), (583, 583), (594, 594), (612, 612), (616, 616), (625, 625), (633, 633), (686, 686)], [(231, 254), (272, 272)], [(300, 300), (314, 314), (324, 324), (367, 367)], [(317, 319), (332, 332), (387, 388)], [(447, 448), (484, 484), (514, 515), (528, 528), (536, 536), (554, 555)], [(640, 640), (644, 644)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7222, coref_recall: 0.2742, coref_f1: 0.3969, mention_recall: 0.4443, batch_loss: 35.3508, loss: 20.2859 ||:  35%|###5      | 121/343 [00:08<00:17, 12.93it/s]predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(37, 40), (51, 53)], [(16, 18), (97, 97), (113, 113), (148, 150)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 81])\n",
            "predicted_antecedents_float size torch.Size([1, 81])\n",
            "probs_logs size: torch.Size([1, 81, 6])\n",
            "probs size: torch.Size([1, 81, 6])\n",
            "probs_flat size: torch.Size([1, 486])\n",
            "gold_antecedent_labels size: torch.Size([1, 81, 6])\n",
            "clusters (batch_clusters) [[[(8, 9), (24, 25), (45, 47)], [(52, 52), (55, 55), (62, 64)], [(80, 82), (105, 110), (123, 127), (153, 157)], [(138, 145), (161, 161), (253, 261)], [(232, 235), (248, 248), (264, 264)], [(319, 332), (381, 383)], [(369, 377), (388, 390), (392, 392)], [(427, 435), (450, 452)], [(433, 434), (471, 472), (509, 510)], [(564, 564), (618, 618), (651, 651)], [(627, 629), (631, 631)], [(807, 808), (814, 815)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7220, coref_recall: 0.2738, coref_f1: 0.3964, mention_recall: 0.4428, batch_loss: 50.6329, loss: 20.4054 ||:  36%|###5      | 123/343 [00:08<00:16, 13.05it/s]predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(51, 53), (78, 79), (104, 105)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(25, 28), (40, 41), (65, 66)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(38, 39), (43, 43), (325, 326), (369, 372), (401, 402)], [(120, 133), (162, 163)], [(7, 9), (185, 187), (221, 222), (227, 229)], [(141, 142), (209, 210)], [(150, 152), (306, 308), (416, 416), (487, 488), (532, 532)], [(397, 404), (420, 421)], [(313, 315), (517, 519)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7219, coref_recall: 0.2737, coref_f1: 0.3963, mention_recall: 0.4428, batch_loss: 11.2474, loss: 20.1379 ||:  37%|###6      | 126/343 [00:08<00:13, 15.63it/s]predicted_antecedents size torch.Size([1, 77])\n",
            "predicted_antecedents_float size torch.Size([1, 77])\n",
            "probs_logs size: torch.Size([1, 77, 6])\n",
            "probs size: torch.Size([1, 77, 6])\n",
            "probs_flat size: torch.Size([1, 462])\n",
            "gold_antecedent_labels size: torch.Size([1, 77, 6])\n",
            "clusters (batch_clusters) [[[(39, 39), (85, 85), (96, 96), (99, 99), (131, 131), (155, 155), (168, 168), (176, 176), (194, 194), (217, 217), (226, 226), (233, 233), (261, 261), (273, 273), (318, 318), (512, 514), (518, 518), (521, 521), (536, 536), (538, 538), (547, 547), (571, 571), (581, 581), (590, 590), (596, 596), (604, 604), (607, 607), (614, 614), (621, 621), (758, 759), (761, 761), (778, 778), (782, 782), (788, 788), (798, 798), (813, 813)], [(297, 297), (299, 299)], [(347, 347), (356, 356), (365, 365), (426, 426), (439, 439), (448, 448), (481, 481), (504, 508), (645, 653), (691, 691), (698, 698), (911, 911), (913, 913)], [(483, 483), (499, 499)], [(662, 663), (678, 678)], [(843, 843), (929, 929), (937, 938), (945, 945), (961, 961)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (54, 54), (60, 60)], [(83, 83), (94, 94), (98, 98), (109, 109), (156, 156), (163, 163), (176, 176)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7223, coref_recall: 0.2725, coref_f1: 0.3951, mention_recall: 0.4412, batch_loss: 1.8545, loss: 20.3215 ||:  37%|###7      | 128/343 [00:08<00:13, 15.56it/s] predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(108, 116), (137, 141)], [(157, 158), (166, 166)], [(173, 175), (177, 177), (180, 180), (188, 188)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 132])\n",
            "predicted_antecedents_float size torch.Size([1, 132])\n",
            "probs_logs size: torch.Size([1, 132, 6])\n",
            "probs size: torch.Size([1, 132, 6])\n",
            "probs_flat size: torch.Size([1, 792])\n",
            "gold_antecedent_labels size: torch.Size([1, 132, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (7, 7), (53, 54), (148, 148), (172, 172), (319, 320), (335, 335), (365, 365), (783, 783), (876, 877), (913, 914), (1417, 1417), (1438, 1438), (1495, 1496)], [(35, 36), (51, 51)], [(109, 111), (139, 143), (146, 146), (229, 231), (300, 310)], [(225, 226), (248, 248), (251, 251), (261, 262)], [(395, 395), (403, 404), (411, 411), (708, 711), (905, 910), (1202, 1204)], [(77, 77), (645, 645), (668, 669), (680, 681), (850, 852), (988, 990), (1024, 1025), (1027, 1027), (1198, 1199), (1218, 1218), (1399, 1400)], [(767, 768), (803, 804)], [(543, 549), (841, 847)], [(921, 928), (942, 943), (980, 981)], [(385, 386), (1048, 1049), (1062, 1064), (1072, 1072), (1076, 1077), (1083, 1083), (1087, 1087), (1096, 1096), (1273, 1275), (1332, 1332), (1421, 1422), (1446, 1447), (1455, 1455), (1468, 1470), (1491, 1492), (1572, 1573), (1580, 1580), (1584, 1584)], [(1101, 1105), (1108, 1108), (1280, 1281)], [(1286, 1290), (1302, 1306)], [(1509, 1511), (1555, 1557), (1624, 1626)], [(1605, 1605), (1612, 1612)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7217, coref_recall: 0.2723, coref_f1: 0.3948, mention_recall: 0.4426, batch_loss: 82.2190, loss: 20.6576 ||:  38%|###7      | 130/343 [00:08<00:17, 12.38it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(110, 125), (122, 122)], [(102, 103), (163, 164), (191, 192), (218, 218)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(30, 34), (68, 72)], [(147, 147), (156, 156), (166, 166), (177, 177)], [(189, 195), (203, 203)], [(16, 18), (216, 217), (430, 432)], [(220, 225), (231, 231)], [(344, 347), (362, 363)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7224, coref_recall: 0.2714, coref_f1: 0.3939, mention_recall: 0.4416, batch_loss: 5.4150, loss: 20.4662 ||:  38%|###8      | 132/343 [00:08<00:15, 13.40it/s] predicted_antecedents size torch.Size([1, 3])\n",
            "predicted_antecedents_float size torch.Size([1, 3])\n",
            "probs_logs size: torch.Size([1, 3, 4])\n",
            "probs size: torch.Size([1, 3, 4])\n",
            "probs_flat size: torch.Size([1, 12])\n",
            "gold_antecedent_labels size: torch.Size([1, 3, 4])\n",
            "clusters (batch_clusters) [[[(0, 7), (22, 23)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (15, 15), (23, 23), (27, 27)], [(226, 226), (245, 245), (252, 252), (285, 285), (293, 293), (447, 447), (451, 451), (453, 453), (461, 461), (463, 463), (480, 480), (488, 488), (492, 492), (495, 495), (496, 496), (498, 498), (500, 500), (502, 502), (552, 552)], [(287, 287), (295, 295), (301, 301)], [(382, 382), (393, 393), (401, 401)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(30, 32), (42, 42), (46, 46), (53, 53), (90, 91), (94, 94), (107, 107), (111, 111), (126, 126), (157, 157), (164, 164), (189, 189), (207, 207), (296, 297), (329, 329), (352, 352), (447, 447)], [(88, 88), (230, 230), (316, 316), (335, 335), (471, 471)], [(473, 473), (483, 483)], [(517, 517), (522, 522), (543, 543)], [(520, 520), (536, 536)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7223, coref_recall: 0.2702, coref_f1: 0.3926, mention_recall: 0.4418, batch_loss: 12.8359, loss: 20.4464 ||:  39%|###9      | 135/343 [00:09<00:14, 14.17it/s]predicted_antecedents size torch.Size([1, 52])\n",
            "predicted_antecedents_float size torch.Size([1, 52])\n",
            "probs_logs size: torch.Size([1, 52, 6])\n",
            "probs size: torch.Size([1, 52, 6])\n",
            "probs_flat size: torch.Size([1, 312])\n",
            "gold_antecedent_labels size: torch.Size([1, 52, 6])\n",
            "clusters (batch_clusters) [[[(73, 73), (78, 79), (108, 108), (110, 110)], [(266, 266), (269, 269), (311, 313), (321, 321), (333, 335), (376, 376), (379, 379), (644, 645)], [(252, 252), (275, 276), (280, 280)], [(198, 198), (307, 307), (341, 341), (355, 355)], [(396, 396), (401, 402), (411, 411), (428, 428), (430, 430), (439, 439), (485, 486), (492, 492), (499, 499), (508, 509), (520, 520), (531, 531), (540, 540), (553, 553), (574, 574), (578, 578), (590, 590)], [(451, 451), (457, 457)], [(610, 610), (629, 630)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(54, 58), (62, 63)], [(113, 115), (160, 160)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7224, coref_recall: 0.2687, coref_f1: 0.3911, mention_recall: 0.4386, batch_loss: 2.1058, loss: 20.4630 ||:  40%|###9      | 137/343 [00:09<00:14, 14.53it/s] predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(57, 57), (65, 65), (70, 70), (75, 75), (78, 78), (83, 83), (93, 93), (105, 105), (122, 122), (140, 140), (178, 178), (192, 192), (214, 214), (221, 221), (233, 233), (241, 241), (256, 256), (263, 263), (282, 282), (288, 288), (313, 313), (319, 319), (326, 326), (349, 349), (400, 400)], [(165, 166), (199, 200), (228, 229), (313, 314), (377, 379)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(94, 111), (113, 115), (132, 132)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7244, coref_recall: 0.2697, coref_f1: 0.3925, mention_recall: 0.4396, batch_loss: 0.7254, loss: 20.1827 ||:  41%|####      | 139/343 [00:09<00:13, 15.55it/s]predicted_antecedents size torch.Size([1, 51])\n",
            "predicted_antecedents_float size torch.Size([1, 51])\n",
            "probs_logs size: torch.Size([1, 51, 6])\n",
            "probs size: torch.Size([1, 51, 6])\n",
            "probs_flat size: torch.Size([1, 306])\n",
            "gold_antecedent_labels size: torch.Size([1, 51, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (5, 5), (16, 16), (25, 25), (34, 34), (56, 56), (89, 89), (94, 94), (119, 121)], [(160, 160), (164, 166), (166, 166)], [(248, 253), (263, 263)], [(321, 322), (325, 325), (331, 331)], [(291, 291), (335, 335), (381, 382)], [(497, 497), (511, 511)], [(582, 584), (613, 613)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(9, 20), (29, 31)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7240, coref_recall: 0.2705, coref_f1: 0.3933, mention_recall: 0.4401, batch_loss: 9.9270, loss: 20.2124 ||:  41%|####1     | 141/343 [00:09<00:12, 15.69it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(23, 32), (31, 31), (34, 34), (64, 66), (78, 78), (183, 183), (195, 195), (218, 220), (236, 236)], [(144, 145), (160, 161)], [(261, 261), (265, 265)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(27, 30), (37, 45), (56, 56), (98, 101), (140, 143), (152, 153), (167, 170), (322, 325), (409, 412)], [(19, 30), (83, 84), (157, 158), (201, 202), (233, 234)], [(23, 30), (163, 170)], [(107, 110), (185, 188), (227, 230), (442, 445), (461, 464)], [(90, 101), (277, 285), (403, 412)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7262, coref_recall: 0.2717, coref_f1: 0.3949, mention_recall: 0.4416, batch_loss: 2.8412, loss: 19.8650 ||:  42%|####1     | 144/343 [00:09<00:11, 16.84it/s]predicted_antecedents size torch.Size([1, 52])\n",
            "predicted_antecedents_float size torch.Size([1, 52])\n",
            "probs_logs size: torch.Size([1, 52, 6])\n",
            "probs size: torch.Size([1, 52, 6])\n",
            "probs_flat size: torch.Size([1, 312])\n",
            "gold_antecedent_labels size: torch.Size([1, 52, 6])\n",
            "clusters (batch_clusters) [[[(4, 4), (15, 15), (17, 17), (22, 22), (26, 26), (34, 34), (42, 42), (103, 103), (142, 142)], [(63, 63), (85, 85)], [(48, 49), (88, 88)], [(162, 162), (164, 164), (202, 202), (212, 212), (252, 252), (282, 282), (291, 291), (365, 365), (389, 389), (391, 391), (404, 404)], [(425, 425), (431, 431), (437, 437), (455, 455)], [(554, 554), (559, 559), (561, 561), (608, 608), (625, 625), (641, 641), (643, 643), (645, 645)], [(573, 573), (589, 589), (599, 599), (612, 612)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(27, 28), (32, 32), (57, 68), (57, 71), (102, 102), (451, 451)], [(37, 43), (216, 216), (224, 224), (294, 294), (312, 312), (348, 348), (386, 386), (417, 417), (438, 438)], [(82, 90), (257, 257), (269, 269), (288, 288), (291, 291)], [(204, 207), (329, 331), (353, 356), (379, 383), (421, 422)], [(232, 236), (394, 397)], [(461, 461), (474, 474)], [(476, 480), (486, 487)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7241, coref_recall: 0.2709, coref_f1: 0.3937, mention_recall: 0.4412, batch_loss: 19.2764, loss: 20.0306 ||:  43%|####2     | 146/343 [00:09<00:11, 16.64it/s]predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(14, 16), (24, 24)], [(6, 7), (32, 33), (58, 59), (211, 212), (221, 222), (240, 241)], [(112, 112), (189, 189), (200, 200), (203, 203)], [(247, 247), (258, 259)], [(318, 325), (324, 324)], [(343, 348), (352, 352), (363, 363)], [(253, 256), (389, 389)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 103])\n",
            "predicted_antecedents_float size torch.Size([1, 103])\n",
            "probs_logs size: torch.Size([1, 103, 6])\n",
            "probs size: torch.Size([1, 103, 6])\n",
            "probs_flat size: torch.Size([1, 618])\n",
            "gold_antecedent_labels size: torch.Size([1, 103, 6])\n",
            "clusters (batch_clusters) [[[(170, 175), (177, 177), (390, 390), (449, 450), (523, 523), (613, 613), (743, 743), (818, 818), (888, 888), (912, 912), (967, 968), (1105, 1105), (1123, 1124), (1130, 1131), (1162, 1162), (1173, 1173), (1215, 1215)], [(330, 330), (338, 338), (342, 344), (353, 353), (363, 363), (385, 385)], [(285, 293), (470, 472), (721, 723)], [(533, 533), (537, 537)], [(762, 768), (772, 772), (836, 841)], [(918, 919), (1005, 1007), (1108, 1109)], [(1023, 1044), (1057, 1057)], [(1056, 1058), (1061, 1062), (1082, 1083)], [(1197, 1198), (1208, 1208)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7223, coref_recall: 0.2701, coref_f1: 0.3926, mention_recall: 0.4407, batch_loss: 86.2213, loss: 20.4380 ||:  43%|####3     | 148/343 [00:09<00:14, 13.20it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(15, 19), (33, 33)], [(2, 9), (57, 60)], [(106, 118), (134, 136)], [(144, 146), (153, 153)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (25, 27), (82, 83), (151, 151)], [(13, 14), (71, 72)], [(120, 121), (138, 139)], [(82, 87), (191, 196)], [(250, 255), (263, 264), (279, 282)], [(383, 386), (396, 396)], [(434, 434), (437, 437)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(3, 5), (28, 34)], [(58, 59), (64, 65)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7208, coref_recall: 0.2699, coref_f1: 0.3921, mention_recall: 0.4408, batch_loss: 1.6110, loss: 20.2730 ||:  44%|####4     | 151/343 [00:09<00:11, 16.04it/s] predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(90, 91), (97, 97), (99, 99), (158, 161)], [(33, 36), (103, 106), (214, 215)], [(239, 239), (247, 252), (264, 264)], [(303, 305), (360, 364)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(115, 118), (127, 127)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 103])\n",
            "predicted_antecedents_float size torch.Size([1, 103])\n",
            "probs_logs size: torch.Size([1, 103, 6])\n",
            "probs size: torch.Size([1, 103, 6])\n",
            "probs_flat size: torch.Size([1, 618])\n",
            "gold_antecedent_labels size: torch.Size([1, 103, 6])\n",
            "clusters (batch_clusters) [[[(14, 17), (19, 19)], [(69, 77), (118, 120)], [(155, 156), (159, 159)], [(159, 167), (170, 170)], [(198, 198), (201, 201)], [(272, 273), (284, 284), (289, 289), (294, 294), (434, 435)], [(401, 401), (423, 423)], [(354, 355), (518, 519)], [(551, 552), (554, 554)], [(689, 690), (694, 694)], [(439, 441), (723, 725), (746, 746)], [(878, 882), (892, 892)], [(914, 915), (926, 926), (933, 934)], [(999, 999), (1009, 1009)], [(1036, 1037), (1044, 1044), (1054, 1054), (1062, 1062), (1090, 1090)], [(1117, 1122), (1136, 1136), (1146, 1146)], [(1184, 1185), (1191, 1191), (1207, 1207), (1209, 1209), (1215, 1215), (1219, 1219), (1229, 1229)], [(1238, 1238), (1246, 1246)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7227, coref_recall: 0.2714, coref_f1: 0.3941, mention_recall: 0.4426, batch_loss: 25.4242, loss: 20.1596 ||:  45%|####4     | 154/343 [00:10<00:12, 14.94it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(6, 7), (16, 18), (52, 53)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(2, 4), (9, 9), (30, 32), (104, 106), (150, 150), (193, 193), (209, 209), (229, 229), (252, 252), (270, 270), (274, 274), (295, 295), (301, 301), (320, 320), (327, 327), (352, 352), (356, 356), (367, 367), (373, 373), (401, 401), (404, 404), (438, 438), (441, 441), (449, 449), (454, 454), (470, 470), (472, 472), (476, 476), (493, 493), (506, 506), (514, 514), (528, 528), (534, 534), (540, 540)], [(136, 139), (141, 141)], [(446, 446), (451, 451)], [(474, 474), (480, 480)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(14, 14), (17, 18), (48, 48)], [(24, 25), (60, 61)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7249, coref_recall: 0.2721, coref_f1: 0.3951, mention_recall: 0.4425, batch_loss: 0.0064, loss: 19.8418 ||:  46%|####5     | 157/343 [00:10<00:10, 17.66it/s] predicted_antecedents size torch.Size([1, 63])\n",
            "predicted_antecedents_float size torch.Size([1, 63])\n",
            "probs_logs size: torch.Size([1, 63, 6])\n",
            "probs size: torch.Size([1, 63, 6])\n",
            "probs_flat size: torch.Size([1, 378])\n",
            "gold_antecedent_labels size: torch.Size([1, 63, 6])\n",
            "clusters (batch_clusters) [[[(7, 22), (89, 89), (122, 122), (127, 127), (135, 135), (138, 138), (467, 467), (533, 533), (569, 569), (580, 580), (688, 688), (753, 753), (769, 769), (791, 791)], [(153, 162), (164, 164), (168, 181), (341, 341), (349, 349), (530, 530), (561, 561), (566, 566), (571, 571), (575, 575), (676, 676), (694, 694), (701, 701)], [(220, 222), (225, 225), (232, 232), (261, 261), (426, 426), (433, 433), (477, 478), (490, 490), (500, 500), (509, 509), (516, 516), (521, 522), (588, 589), (609, 611), (638, 638)], [(286, 286), (298, 298), (303, 303)], [(379, 380), (386, 386)], [(453, 454), (488, 488)], [(709, 715), (720, 721), (730, 730), (788, 788)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(52, 55), (84, 86), (132, 134), (173, 175), (230, 234), (236, 236), (259, 259), (266, 266), (364, 368), (372, 374)], [(6, 12), (107, 109), (324, 327), (372, 377), (401, 401)], [(205, 212), (211, 211)], [(236, 242), (244, 245), (281, 282)], [(286, 287), (293, 293)], [(333, 333), (345, 345), (347, 347), (354, 354), (359, 359)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7228, coref_recall: 0.2701, coref_f1: 0.3927, mention_recall: 0.4402, batch_loss: 34.3819, loss: 20.0296 ||:  46%|####6     | 159/343 [00:10<00:10, 17.61it/s]predicted_antecedents size torch.Size([1, 62])\n",
            "predicted_antecedents_float size torch.Size([1, 62])\n",
            "probs_logs size: torch.Size([1, 62, 6])\n",
            "probs size: torch.Size([1, 62, 6])\n",
            "probs_flat size: torch.Size([1, 372])\n",
            "gold_antecedent_labels size: torch.Size([1, 62, 6])\n",
            "clusters (batch_clusters) [[[(43, 43), (48, 48)], [(52, 60), (62, 63), (79, 79)], [(96, 98), (129, 130), (141, 141), (173, 174), (207, 208)], [(110, 110), (135, 139)], [(251, 251), (264, 265), (273, 273), (357, 358)], [(425, 425), (444, 444), (466, 466), (614, 614), (635, 635), (645, 645)], [(468, 468), (476, 476)], [(371, 371), (548, 548)], [(637, 637), (647, 647)], [(656, 656), (677, 677), (686, 686), (697, 697), (703, 703)], [(717, 721), (726, 727), (734, 734)], [(758, 767), (770, 770), (773, 773), (779, 779), (782, 782)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 75])\n",
            "predicted_antecedents_float size torch.Size([1, 75])\n",
            "probs_logs size: torch.Size([1, 75, 6])\n",
            "probs size: torch.Size([1, 75, 6])\n",
            "probs_flat size: torch.Size([1, 450])\n",
            "gold_antecedent_labels size: torch.Size([1, 75, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (46, 46), (59, 59), (73, 73), (101, 101), (134, 134), (146, 146), (242, 242), (252, 252), (351, 351), (376, 376), (378, 378), (387, 387), (390, 390), (411, 411), (415, 415), (420, 420), (426, 426), (438, 438), (455, 455), (457, 457), (463, 463), (494, 494), (541, 541), (564, 564), (577, 577), (603, 603), (611, 611), (660, 660), (690, 690), (700, 700), (718, 718), (742, 742), (760, 760), (764, 764), (771, 771), (777, 777), (786, 786), (802, 802), (827, 827), (852, 852), (858, 858), (873, 873), (923, 923), (945, 945)], [(154, 154), (168, 168)], [(194, 194), (207, 207), (209, 209), (215, 215), (236, 236), (275, 275), (294, 294), (318, 318), (325, 325), (339, 339), (505, 505), (514, 514), (529, 529)], [(263, 263), (301, 301)], [(395, 395), (460, 460), (562, 562), (683, 683), (806, 806), (887, 887), (889, 889)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7227, coref_recall: 0.2729, coref_f1: 0.3956, mention_recall: 0.4444, batch_loss: 35.8762, loss: 20.3339 ||:  47%|####6     | 161/343 [00:10<00:12, 15.11it/s]predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 81])\n",
            "predicted_antecedents_float size torch.Size([1, 81])\n",
            "probs_logs size: torch.Size([1, 81, 6])\n",
            "probs size: torch.Size([1, 81, 6])\n",
            "probs_flat size: torch.Size([1, 486])\n",
            "gold_antecedent_labels size: torch.Size([1, 81, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (12, 12), (17, 17), (31, 34), (50, 51), (129, 130), (159, 159), (162, 162), (193, 193), (236, 237), (255, 255), (354, 354), (365, 365), (372, 372), (427, 427), (439, 439), (462, 462), (571, 571), (573, 573), (635, 635), (639, 639), (742, 743), (1007, 1007), (1016, 1016)], [(41, 41), (118, 118), (208, 210)], [(404, 404), (407, 407), (419, 419), (447, 447), (450, 450), (456, 456), (554, 554), (560, 562), (566, 566)], [(480, 480), (709, 709), (755, 755)], [(767, 767), (772, 772)], [(841, 841), (851, 851)], [(869, 872), (934, 937)], [(925, 937), (966, 966), (969, 969), (973, 973)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7206, coref_recall: 0.2716, coref_f1: 0.3939, mention_recall: 0.4442, batch_loss: 43.4866, loss: 20.3675 ||:  48%|####7     | 163/343 [00:10<00:13, 13.80it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(28, 32), (39, 43)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(63, 69), (98, 98)], [(125, 130), (132, 132), (141, 141), (150, 151), (165, 165), (181, 181)], [(189, 191), (226, 226)], [(277, 287), (289, 289)], [(255, 262), (295, 295), (329, 329), (335, 335), (341, 341), (353, 353)], [(306, 306), (309, 309), (324, 324), (379, 379)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(29, 30), (50, 50), (53, 53), (58, 58)], [(132, 138), (145, 145)], [(148, 149), (158, 160)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 59])\n",
            "predicted_antecedents_float size torch.Size([1, 59])\n",
            "probs_logs size: torch.Size([1, 59, 6])\n",
            "probs size: torch.Size([1, 59, 6])\n",
            "probs_flat size: torch.Size([1, 354])\n",
            "gold_antecedent_labels size: torch.Size([1, 59, 6])\n",
            "clusters (batch_clusters) [[[(0, 7), (71, 72), (220, 222), (361, 362)], [(151, 155), (224, 225), (303, 306), (325, 325)], [(229, 231), (234, 234)], [(60, 60), (369, 370)], [(468, 474), (477, 477), (493, 493)], [(95, 95), (532, 532)], [(538, 539), (559, 560)], [(584, 586), (593, 594)], [(630, 630), (635, 635)], [(654, 655), (661, 664)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7195, coref_recall: 0.2708, coref_f1: 0.3929, mention_recall: 0.4437, batch_loss: 51.5635, loss: 20.3522 ||:  49%|####8     | 167/343 [00:10<00:10, 16.31it/s]predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(11, 39), (29, 29), (34, 34), (41, 41), (50, 50), (64, 64), (73, 73), (98, 98), (122, 122), (130, 130), (149, 149), (156, 156), (159, 159), (180, 180), (197, 197), (249, 249), (263, 263), (265, 265), (326, 326), (359, 359), (373, 373), (376, 376), (378, 378), (389, 389), (395, 395), (400, 400)], [(301, 302), (304, 304)], [(404, 407), (509, 511)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(55, 55), (59, 59)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(10, 16), (59, 64), (93, 99), (137, 143), (199, 205), (229, 230), (236, 237), (243, 245), (249, 249), (266, 272)], [(46, 52), (116, 129), (254, 258)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7219, coref_recall: 0.2719, coref_f1: 0.3944, mention_recall: 0.4442, batch_loss: 10.6481, loss: 20.0653 ||:  50%|####9     | 170/343 [00:11<00:09, 17.67it/s]predicted_antecedents size torch.Size([1, 60])\n",
            "predicted_antecedents_float size torch.Size([1, 60])\n",
            "probs_logs size: torch.Size([1, 60, 6])\n",
            "probs size: torch.Size([1, 60, 6])\n",
            "probs_flat size: torch.Size([1, 360])\n",
            "gold_antecedent_labels size: torch.Size([1, 60, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (19, 19), (23, 23), (46, 46), (68, 68), (115, 115), (125, 125), (135, 135), (147, 147), (151, 151), (156, 156), (160, 160), (185, 185), (214, 214), (225, 225), (241, 241), (258, 258), (264, 264), (271, 271), (284, 284), (295, 295), (297, 297), (302, 302), (355, 355), (359, 359), (460, 460), (473, 473), (492, 492), (514, 514), (518, 518), (527, 527), (540, 540), (550, 550), (604, 604), (611, 611), (681, 681)], [(55, 55), (89, 89), (201, 201), (454, 454)], [(457, 457), (481, 481), (548, 548)], [(545, 545), (555, 555), (586, 586), (590, 590), (595, 595), (619, 619)], [(573, 573), (576, 576)], [(645, 645), (740, 740)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (18, 18), (34, 34), (81, 81), (93, 93), (98, 98), (106, 106), (116, 116), (121, 121), (147, 147), (157, 157), (207, 207), (212, 212), (306, 306), (317, 317), (408, 408), (412, 413), (425, 425), (439, 439), (446, 446), (452, 452), (466, 466), (478, 478), (488, 488), (495, 495), (511, 511)], [(24, 24), (47, 47), (61, 61), (112, 112)], [(175, 175), (302, 302), (313, 313)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7197, coref_recall: 0.2728, coref_f1: 0.3950, mention_recall: 0.4454, batch_loss: 31.1482, loss: 20.4450 ||:  50%|#####     | 172/343 [00:11<00:09, 17.48it/s]predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(23, 32), (40, 41), (62, 62)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 78])\n",
            "predicted_antecedents_float size torch.Size([1, 78])\n",
            "probs_logs size: torch.Size([1, 78, 6])\n",
            "probs size: torch.Size([1, 78, 6])\n",
            "probs_flat size: torch.Size([1, 468])\n",
            "gold_antecedent_labels size: torch.Size([1, 78, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (6, 6)], [(13, 13), (22, 22)], [(150, 177), (179, 179), (219, 219), (259, 259), (274, 274), (608, 608)], [(471, 471), (476, 476)], [(407, 407), (481, 481), (789, 789), (837, 837), (900, 900)], [(554, 554), (559, 559), (564, 564)], [(616, 616), (621, 621)], [(428, 429), (754, 755)], [(832, 832), (840, 840), (847, 847), (868, 868), (877, 877), (964, 964), (974, 974)], [(885, 885), (890, 890), (895, 895)], [(681, 681), (904, 906), (956, 956)], [(908, 908), (910, 910)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7172, coref_recall: 0.2723, coref_f1: 0.3941, mention_recall: 0.4458, batch_loss: 37.1214, loss: 20.3067 ||:  51%|#####1    | 175/343 [00:11<00:09, 18.33it/s]predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(11, 11), (23, 24)], [(45, 45), (48, 48)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 78])\n",
            "predicted_antecedents_float size torch.Size([1, 78])\n",
            "probs_logs size: torch.Size([1, 78, 6])\n",
            "probs size: torch.Size([1, 78, 6])\n",
            "probs_flat size: torch.Size([1, 468])\n",
            "gold_antecedent_labels size: torch.Size([1, 78, 6])\n",
            "clusters (batch_clusters) [[[(67, 67), (71, 71), (77, 77), (83, 83)], [(106, 106), (118, 118), (122, 122), (138, 138), (148, 148), (839, 840)], [(125, 125), (162, 162)], [(326, 326), (386, 387), (409, 409)], [(477, 479), (483, 483)], [(503, 503), (506, 506)], [(517, 533), (551, 551)], [(492, 494), (560, 562), (569, 569)], [(575, 579), (628, 630)], [(763, 764), (794, 794), (797, 797), (818, 818), (843, 843)], [(859, 861), (874, 874), (877, 877)], [(927, 928), (938, 938)], [(966, 966), (972, 972)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7189, coref_recall: 0.2734, coref_f1: 0.3955, mention_recall: 0.4468, batch_loss: 18.8082, loss: 20.1852 ||:  52%|#####1    | 177/343 [00:11<00:10, 16.49it/s]predicted_antecedents size torch.Size([1, 44])\n",
            "predicted_antecedents_float size torch.Size([1, 44])\n",
            "probs_logs size: torch.Size([1, 44, 6])\n",
            "probs size: torch.Size([1, 44, 6])\n",
            "probs_flat size: torch.Size([1, 264])\n",
            "gold_antecedent_labels size: torch.Size([1, 44, 6])\n",
            "clusters (batch_clusters) [[[(50, 50), (55, 55), (63, 63), (66, 66), (137, 137)], [(81, 82), (86, 86), (110, 110), (118, 118), (149, 149)], [(192, 195), (199, 199), (381, 382), (389, 389)], [(140, 140), (289, 289), (359, 359), (375, 375), (386, 386), (523, 523)], [(412, 416), (419, 419)], [(444, 446), (469, 471), (517, 519)], [(459, 465), (487, 487), (489, 489), (530, 530), (547, 547), (553, 553)], [(507, 508), (514, 514)], [(208, 215), (543, 544)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(21, 37), (39, 39)], [(18, 19), (50, 51)], [(91, 91), (120, 120)], [(131, 133), (143, 143)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 86])\n",
            "predicted_antecedents_float size torch.Size([1, 86])\n",
            "probs_logs size: torch.Size([1, 86, 6])\n",
            "probs size: torch.Size([1, 86, 6])\n",
            "probs_flat size: torch.Size([1, 516])\n",
            "gold_antecedent_labels size: torch.Size([1, 86, 6])\n",
            "clusters (batch_clusters) [[[(21, 24), (37, 37), (168, 168), (181, 181), (184, 184), (193, 193), (196, 196), (418, 418), (974, 974), (981, 981)], [(26, 37), (216, 218)], [(241, 242), (268, 269)], [(282, 288), (318, 333)], [(74, 74), (377, 378)], [(471, 471), (504, 504), (661, 662), (687, 687), (696, 696), (796, 798), (851, 853)], [(356, 364), (561, 562), (769, 770), (1039, 1040)], [(548, 549), (567, 567), (569, 569), (594, 595), (597, 597), (632, 634), (647, 648), (702, 703), (832, 833), (954, 954)], [(737, 754), (760, 761), (1043, 1043)], [(858, 861), (892, 894), (994, 997)], [(900, 901), (903, 903), (912, 912)], [(445, 446), (977, 978), (987, 988)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7198, coref_recall: 0.2723, coref_f1: 0.3946, mention_recall: 0.4451, batch_loss: 30.1827, loss: 20.1802 ||:  52%|#####2    | 180/343 [00:11<00:10, 14.97it/s]predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(27, 27), (106, 106), (111, 111)], [(50, 58), (119, 120), (327, 328)], [(20, 30), (164, 165)], [(44, 44), (239, 249)], [(226, 249), (257, 258), (342, 343), (387, 394)], [(346, 357), (365, 365)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(123, 132), (137, 137), (137, 138), (141, 141), (144, 144), (158, 159)], [(45, 49), (178, 180)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 20])\n",
            "predicted_antecedents_float size torch.Size([1, 20])\n",
            "probs_logs size: torch.Size([1, 20, 6])\n",
            "probs size: torch.Size([1, 20, 6])\n",
            "probs_flat size: torch.Size([1, 120])\n",
            "gold_antecedent_labels size: torch.Size([1, 20, 6])\n",
            "clusters (batch_clusters) [[[(2, 4), (14, 14), (22, 22), (26, 27), (34, 34), (136, 137), (152, 152), (175, 176), (178, 179), (192, 193), (213, 213)], [(80, 80), (87, 87)], [(92, 93), (98, 98)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7198, coref_recall: 0.2725, coref_f1: 0.3948, mention_recall: 0.4454, batch_loss: 1.0857, loss: 19.9240 ||:  53%|#####3    | 183/343 [00:11<00:09, 16.98it/s] predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(7, 11), (41, 41)], [(34, 36), (47, 48), (74, 76), (88, 88), (91, 91), (100, 101), (134, 135), (142, 142), (163, 164)], [(34, 39), (142, 145)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 51])\n",
            "predicted_antecedents_float size torch.Size([1, 51])\n",
            "probs_logs size: torch.Size([1, 51, 6])\n",
            "probs size: torch.Size([1, 51, 6])\n",
            "probs_flat size: torch.Size([1, 306])\n",
            "gold_antecedent_labels size: torch.Size([1, 51, 6])\n",
            "clusters (batch_clusters) [[[(31, 31), (33, 33), (41, 41), (45, 45)], [(21, 21), (38, 38)], [(50, 55), (57, 57), (340, 342), (381, 383), (627, 631)], [(91, 91), (104, 104)], [(120, 120), (136, 136), (170, 170), (177, 177)], [(214, 214), (222, 222), (235, 235), (244, 244), (249, 249)], [(271, 272), (276, 276)], [(281, 283), (302, 302), (482, 484)], [(488, 488), (498, 499)], [(470, 471), (522, 523)], [(553, 554), (558, 558), (599, 603), (605, 605)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7189, coref_recall: 0.2731, coref_f1: 0.3953, mention_recall: 0.4460, batch_loss: 43.0943, loss: 20.0015 ||:  54%|#####3    | 185/343 [00:12<00:09, 17.25it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(59, 59), (65, 65)], [(180, 184), (198, 199)], [(287, 287), (296, 296)], [(279, 280), (299, 299)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(109, 114), (118, 118), (123, 123), (130, 130), (135, 135), (144, 144), (178, 178)], [(163, 166), (172, 172)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (24, 28), (32, 32), (52, 52), (80, 80), (88, 88), (111, 111), (121, 121), (136, 136), (166, 166), (176, 176), (208, 208), (270, 270), (276, 276), (313, 313), (379, 379), (390, 390), (414, 414), (421, 421), (434, 434)], [(189, 190), (200, 200)], [(108, 112), (273, 277)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7199, coref_recall: 0.2734, coref_f1: 0.3957, mention_recall: 0.4465, batch_loss: 6.3887, loss: 19.7947 ||:  55%|#####4    | 188/343 [00:12<00:08, 19.04it/s] predicted_antecedents size torch.Size([1, 47])\n",
            "predicted_antecedents_float size torch.Size([1, 47])\n",
            "probs_logs size: torch.Size([1, 47, 6])\n",
            "probs size: torch.Size([1, 47, 6])\n",
            "probs_flat size: torch.Size([1, 282])\n",
            "gold_antecedent_labels size: torch.Size([1, 47, 6])\n",
            "clusters (batch_clusters) [[[(32, 32), (36, 36)], [(24, 24), (42, 42)], [(15, 18), (63, 66), (169, 172)], [(96, 104), (143, 148)], [(161, 161), (175, 179), (187, 187)], [(269, 280), (302, 302), (306, 306), (310, 310)], [(318, 325), (345, 346), (360, 360), (370, 371)], [(322, 325), (387, 388), (423, 424)], [(396, 402), (435, 435)], [(447, 449), (511, 513), (521, 521), (530, 530), (538, 538), (547, 547), (566, 566), (589, 589)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(10, 10), (12, 12), (52, 52), (147, 147), (156, 156)], [(112, 112), (114, 114), (127, 127)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7197, coref_recall: 0.2745, coref_f1: 0.3968, mention_recall: 0.4472, batch_loss: 0.0673, loss: 19.7116 ||:  55%|#####5    | 190/343 [00:12<00:08, 18.30it/s]predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(99, 99), (105, 105)], [(18, 18), (115, 116)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(120, 120), (134, 134), (139, 139), (144, 144), (160, 160)], [(307, 308), (324, 324)], [(338, 338), (458, 459)], [(466, 466), (477, 477), (502, 503), (514, 514)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 86])\n",
            "predicted_antecedents_float size torch.Size([1, 86])\n",
            "probs_logs size: torch.Size([1, 86, 6])\n",
            "probs size: torch.Size([1, 86, 6])\n",
            "probs_flat size: torch.Size([1, 516])\n",
            "gold_antecedent_labels size: torch.Size([1, 86, 6])\n",
            "clusters (batch_clusters) [[[(11, 15), (31, 33), (69, 71), (116, 117), (131, 131), (138, 140), (143, 143), (234, 236), (342, 345)], [(101, 105), (109, 109), (593, 597), (640, 642), (645, 645), (654, 655), (676, 676)], [(180, 183), (187, 187)], [(226, 227), (265, 266), (310, 311)], [(14, 15), (281, 283), (337, 338), (593, 595)], [(84, 86), (705, 707)], [(728, 730), (761, 761), (824, 824), (1078, 1078)], [(736, 743), (773, 775)], [(831, 831), (852, 852)], [(862, 875), (877, 877)], [(98, 99), (899, 900), (972, 973)], [(980, 985), (988, 988)], [(951, 951), (991, 1007)], [(1057, 1059), (1064, 1064)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7210, coref_recall: 0.2735, coref_f1: 0.3960, mention_recall: 0.4471, batch_loss: 43.1151, loss: 19.7338 ||:  56%|#####6    | 193/343 [00:12<00:09, 16.25it/s]predicted_antecedents size torch.Size([1, 47])\n",
            "predicted_antecedents_float size torch.Size([1, 47])\n",
            "probs_logs size: torch.Size([1, 47, 6])\n",
            "probs size: torch.Size([1, 47, 6])\n",
            "probs_flat size: torch.Size([1, 282])\n",
            "gold_antecedent_labels size: torch.Size([1, 47, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (6, 6), (10, 10), (13, 15), (20, 20), (36, 36), (69, 69), (173, 173), (178, 178), (256, 256), (345, 345), (374, 374), (468, 468)], [(6, 7), (116, 117), (268, 268), (292, 293), (508, 509), (579, 580)], [(208, 210), (238, 239)], [(234, 239), (241, 241), (262, 264)], [(313, 314), (322, 323), (343, 343), (356, 360)], [(316, 317), (325, 326)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(26, 27), (49, 49), (78, 78)], [(129, 129), (136, 136), (169, 169)], [(220, 220), (248, 248), (278, 278)], [(255, 257), (262, 262), (267, 267), (460, 460), (474, 475), (481, 481), (488, 489), (496, 496), (508, 508)], [(293, 300), (302, 302)], [(322, 322), (326, 326), (336, 336), (344, 344)], [(382, 382), (400, 401), (438, 438), (443, 443), (448, 448)], [(423, 423), (430, 430)], [(454, 454), (468, 468), (537, 537), (564, 566)], [(451, 457), (500, 505)], [(553, 556), (560, 560)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7197, coref_recall: 0.2734, coref_f1: 0.3957, mention_recall: 0.4464, batch_loss: 16.8709, loss: 19.8245 ||:  57%|#####6    | 195/343 [00:12<00:09, 15.29it/s]predicted_antecedents size torch.Size([1, 51])\n",
            "predicted_antecedents_float size torch.Size([1, 51])\n",
            "probs_logs size: torch.Size([1, 51, 6])\n",
            "probs size: torch.Size([1, 51, 6])\n",
            "probs_flat size: torch.Size([1, 306])\n",
            "gold_antecedent_labels size: torch.Size([1, 51, 6])\n",
            "clusters (batch_clusters) [[[(17, 18), (26, 26)], [(33, 33), (38, 38), (196, 197), (450, 451)], [(83, 91), (100, 100), (141, 143), (557, 558)], [(167, 167), (168, 168), (176, 176), (186, 186)], [(263, 263), (269, 269)], [(287, 287), (300, 300), (479, 479), (583, 583), (587, 587)], [(476, 479), (487, 495), (503, 503)], [(591, 591), (595, 595), (597, 597), (634, 634)], [(585, 585), (609, 609)], [(614, 614), (620, 620), (622, 622)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(60, 61), (67, 69)], [(0, 3), (77, 80)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 58])\n",
            "predicted_antecedents_float size torch.Size([1, 58])\n",
            "probs_logs size: torch.Size([1, 58, 6])\n",
            "probs size: torch.Size([1, 58, 6])\n",
            "probs_flat size: torch.Size([1, 348])\n",
            "gold_antecedent_labels size: torch.Size([1, 58, 6])\n",
            "clusters (batch_clusters) [[[(9, 9), (15, 15), (25, 25), (34, 34), (47, 47), (61, 61), (78, 78), (84, 84), (217, 217), (237, 237), (248, 248), (251, 251), (260, 260), (281, 281), (310, 310), (315, 315), (318, 318), (343, 343), (393, 393), (403, 403), (416, 416), (440, 440), (452, 452), (459, 459), (463, 463), (498, 498), (524, 524), (539, 539), (549, 549), (570, 570), (573, 573), (584, 584), (622, 622), (637, 637), (655, 655), (697, 697), (700, 700), (713, 713)], [(303, 303), (400, 400), (457, 457), (467, 467), (484, 484), (577, 577)], [(678, 678), (688, 688), (691, 691), (704, 704), (710, 710)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7189, coref_recall: 0.2732, coref_f1: 0.3954, mention_recall: 0.4463, batch_loss: 31.9209, loss: 19.7641 ||:  58%|#####7    | 198/343 [00:12<00:08, 16.39it/s]predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(34, 38), (56, 56), (123, 124)], [(83, 90), (114, 115)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (47, 48), (61, 63), (80, 81)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(45, 46), (48, 48), (70, 71), (89, 89), (113, 113), (134, 134), (136, 136), (151, 151), (158, 158), (171, 171), (192, 192)], [(108, 111), (195, 197)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(13, 33), (63, 63), (71, 72), (97, 98), (198, 200), (229, 229)], [(54, 55), (117, 118), (233, 234), (237, 238), (320, 321), (344, 345), (407, 408)], [(278, 295), (300, 300)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2740, coref_f1: 0.3964, mention_recall: 0.4465, batch_loss: 9.0516, loss: 19.4236 ||:  59%|#####8    | 202/343 [00:12<00:06, 20.16it/s] predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(13, 32), (26, 26), (39, 39), (94, 94), (125, 125), (150, 150), (162, 162), (173, 173)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 87])\n",
            "predicted_antecedents_float size torch.Size([1, 87])\n",
            "probs_logs size: torch.Size([1, 87, 6])\n",
            "probs size: torch.Size([1, 87, 6])\n",
            "probs_flat size: torch.Size([1, 522])\n",
            "gold_antecedent_labels size: torch.Size([1, 87, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (17, 33), (236, 236), (249, 258), (284, 284)], [(31, 33), (61, 61), (101, 123), (138, 138), (172, 172), (195, 195), (202, 204), (217, 217), (232, 232), (302, 303), (337, 338), (346, 347), (352, 352), (415, 415), (438, 438), (451, 451), (469, 470), (504, 506), (695, 696), (720, 721), (925, 925)], [(82, 82), (93, 95), (98, 98), (463, 466), (494, 494), (514, 514), (530, 531), (776, 780), (796, 796), (832, 832), (878, 881), (915, 915), (934, 935), (946, 946), (990, 990), (1000, 1000), (1003, 1003), (1015, 1015), (1018, 1018)], [(426, 432), (460, 461)], [(883, 895), (894, 894)], [(979, 983), (986, 986)], [(1034, 1034), (1038, 1038), (1064, 1064), (1072, 1072), (1080, 1080), (1084, 1084), (1091, 1091)], [(1068, 1076), (1088, 1088)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(86, 99), (92, 92), (128, 129), (214, 215)], [(107, 117), (121, 121)], [(151, 151), (154, 154), (159, 159), (277, 277)], [(168, 169), (175, 175)], [(305, 309), (313, 315), (339, 340), (373, 374), (387, 394), (411, 412)], [(403, 412), (423, 423), (439, 439), (442, 442), (462, 462), (474, 474), (492, 492)], [(454, 455), (477, 477)], [(486, 490), (501, 501)], [(515, 518), (524, 524)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2742, coref_f1: 0.3966, mention_recall: 0.4465, batch_loss: 21.0829, loss: 19.6917 ||:  60%|#####9    | 205/343 [00:13<00:08, 16.23it/s]predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(57, 71), (73, 73)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(15, 17), (19, 19), (80, 87)], [(37, 37), (44, 44), (46, 46)], [(154, 159), (177, 177), (179, 179)], [(204, 205), (213, 214)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(47, 49), (154, 156)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7201, coref_recall: 0.2736, coref_f1: 0.3960, mention_recall: 0.4463, batch_loss: 1.3819, loss: 19.4698 ||:  61%|######    | 208/343 [00:13<00:07, 18.63it/s] predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(2, 2), (49, 49), (69, 69), (76, 76), (88, 88), (116, 116), (133, 133), (151, 151), (155, 155), (197, 197), (231, 231), (236, 236), (264, 264), (274, 274), (289, 289), (294, 294), (349, 349), (364, 364), (392, 392), (417, 417), (422, 422), (450, 450), (498, 498), (502, 502), (504, 504), (515, 515), (523, 523), (532, 532), (537, 537), (550, 550), (556, 556), (558, 558), (565, 565)], [(195, 195), (204, 204), (426, 426)], [(534, 535), (541, 541), (554, 554)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 83])\n",
            "predicted_antecedents_float size torch.Size([1, 83])\n",
            "probs_logs size: torch.Size([1, 83, 6])\n",
            "probs size: torch.Size([1, 83, 6])\n",
            "probs_flat size: torch.Size([1, 498])\n",
            "gold_antecedent_labels size: torch.Size([1, 83, 6])\n",
            "clusters (batch_clusters) [[[(24, 24), (28, 28)], [(53, 54), (69, 69), (135, 136), (208, 210), (238, 240), (279, 279), (289, 292), (324, 325), (663, 664)], [(10, 11), (150, 151), (859, 860), (906, 907)], [(64, 66), (247, 249), (329, 331), (700, 701), (765, 767)], [(259, 260), (282, 283)], [(352, 355), (387, 387), (413, 413), (421, 422), (831, 832), (852, 852)], [(463, 467), (525, 525)], [(554, 554), (575, 575)], [(585, 592), (595, 595)], [(365, 369), (726, 728), (865, 866), (882, 882), (888, 889)], [(456, 459), (796, 799)], [(968, 970), (974, 974), (993, 994), (1010, 1010), (1038, 1038)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (23, 23), (35, 35), (66, 66), (73, 73), (102, 105), (112, 112), (136, 136), (168, 168), (187, 187), (199, 199), (231, 232), (243, 243), (273, 273), (302, 303), (305, 305)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7212, coref_recall: 0.2730, coref_f1: 0.3954, mention_recall: 0.4446, batch_loss: 0.1139, loss: 19.5457 ||:  62%|######1   | 211/343 [00:13<00:08, 16.22it/s]predicted_antecedents size torch.Size([1, 85])\n",
            "predicted_antecedents_float size torch.Size([1, 85])\n",
            "probs_logs size: torch.Size([1, 85, 6])\n",
            "probs size: torch.Size([1, 85, 6])\n",
            "probs_flat size: torch.Size([1, 510])\n",
            "gold_antecedent_labels size: torch.Size([1, 85, 6])\n",
            "clusters (batch_clusters) [[[(45, 52), (58, 58), (63, 68), (73, 74), (107, 107), (298, 299), (337, 338), (424, 424), (473, 473), (478, 478), (484, 484), (488, 488), (672, 672), (711, 711), (723, 723), (746, 746), (828, 828), (902, 902), (909, 909), (953, 953), (961, 961), (970, 970), (984, 984), (986, 986), (1053, 1054)], [(51, 52), (77, 78), (190, 191), (204, 205), (293, 294), (323, 324), (331, 332), (335, 335), (355, 356), (371, 372), (381, 382), (397, 398)], [(89, 91), (100, 100), (116, 116), (119, 119), (121, 122), (127, 127), (138, 138), (143, 143), (151, 151)], [(193, 198), (210, 210)], [(225, 225), (236, 236), (240, 240)], [(276, 278), (280, 280)], [(663, 663), (669, 669)], [(675, 676), (684, 684), (687, 687), (693, 694), (708, 708), (721, 721), (736, 737)], [(766, 766), (773, 773), (789, 789), (795, 795), (799, 799), (838, 838), (948, 948), (956, 956), (964, 964), (974, 974), (1004, 1004)], [(980, 981), (989, 990)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(78, 78), (87, 87), (87, 101), (90, 101)], [(24, 32), (100, 101)], [(130, 130), (140, 141), (146, 146), (159, 162), (178, 178), (192, 192), (196, 198)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7217, coref_recall: 0.2716, coref_f1: 0.3941, mention_recall: 0.4420, batch_loss: 2.7117, loss: 19.7099 ||:  62%|######2   | 213/343 [00:13<00:08, 15.20it/s]predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(148, 151), (162, 162)], [(276, 284), (289, 289), (292, 292), (305, 305), (310, 310)], [(336, 336), (342, 342)], [(350, 355), (359, 359), (381, 381)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(3, 4), (92, 94)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(19, 25), (69, 70), (72, 78), (92, 93), (118, 121), (128, 129)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7222, coref_recall: 0.2716, coref_f1: 0.3942, mention_recall: 0.4418, batch_loss: 5.1629, loss: 19.4871 ||:  63%|######2   | 216/343 [00:13<00:07, 17.00it/s]predicted_antecedents size torch.Size([1, 61])\n",
            "predicted_antecedents_float size torch.Size([1, 61])\n",
            "probs_logs size: torch.Size([1, 61, 6])\n",
            "probs size: torch.Size([1, 61, 6])\n",
            "probs_flat size: torch.Size([1, 366])\n",
            "gold_antecedent_labels size: torch.Size([1, 61, 6])\n",
            "clusters (batch_clusters) [[[(7, 10), (18, 18), (22, 22), (41, 42), (200, 201), (372, 375), (382, 382)], [(86, 87), (136, 137), (145, 145), (171, 172)], [(186, 187), (219, 219), (413, 413), (429, 429), (440, 441)], [(417, 417), (425, 425), (566, 566), (572, 572), (581, 581), (586, 586), (596, 596), (601, 601), (609, 609), (619, 619), (623, 623), (626, 626), (635, 635), (655, 655), (671, 671), (685, 685), (695, 695), (703, 703), (706, 706), (717, 717), (725, 725), (727, 727), (732, 732), (742, 742), (744, 744), (747, 747), (758, 758)], [(541, 547), (577, 577), (589, 589), (606, 606), (633, 633), (637, 637), (652, 652)], [(657, 663), (673, 674)], [(688, 689), (713, 715)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(0, 8), (40, 41), (105, 105), (111, 111), (122, 123), (131, 132), (139, 139), (167, 168)], [(20, 27), (79, 80)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7231, coref_recall: 0.2712, coref_f1: 0.3939, mention_recall: 0.4410, batch_loss: 4.8837, loss: 19.4982 ||:  64%|######3   | 218/343 [00:13<00:07, 16.96it/s]predicted_antecedents size torch.Size([1, 56])\n",
            "predicted_antecedents_float size torch.Size([1, 56])\n",
            "probs_logs size: torch.Size([1, 56, 6])\n",
            "probs size: torch.Size([1, 56, 6])\n",
            "probs_flat size: torch.Size([1, 336])\n",
            "gold_antecedent_labels size: torch.Size([1, 56, 6])\n",
            "clusters (batch_clusters) [[[(79, 82), (84, 84), (93, 93)], [(189, 189), (200, 200), (207, 207), (216, 216)], [(228, 228), (230, 230)], [(232, 232), (239, 239), (246, 246), (252, 252), (272, 272), (435, 436), (460, 460), (465, 465)], [(243, 244), (250, 250), (254, 254), (261, 262), (265, 265)], [(599, 599), (612, 612)], [(652, 660), (667, 667)], [(675, 675), (688, 688)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(57, 59), (78, 79)], [(151, 153), (158, 158), (457, 457)], [(201, 201), (217, 217), (344, 344), (349, 349), (351, 351), (481, 482), (491, 491), (494, 494)], [(247, 248), (254, 254), (260, 260), (265, 265), (273, 273)], [(237, 237), (333, 333), (338, 338), (434, 434), (498, 498), (505, 505)], [(308, 330), (336, 336)], [(450, 454), (475, 475)], [(563, 563), (568, 568), (582, 582)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7229, coref_recall: 0.2713, coref_f1: 0.3940, mention_recall: 0.4409, batch_loss: 23.6738, loss: 19.3721 ||:  64%|######4   | 221/343 [00:14<00:06, 17.50it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(93, 94), (118, 119), (131, 131), (143, 144)], [(163, 164), (183, 183)], [(200, 200), (207, 207)], [(235, 236), (264, 264), (343, 345)], [(257, 257), (277, 277)], [(355, 355), (360, 360)], [(324, 325), (363, 363), (366, 366), (372, 373), (381, 382)], [(409, 413), (415, 415), (427, 427), (435, 435), (440, 440), (460, 460), (469, 469), (473, 473), (484, 484)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (12, 12)], [(138, 143), (149, 150), (174, 175), (210, 210), (213, 213), (408, 409), (426, 428)], [(246, 251), (267, 267)], [(138, 139), (300, 301), (305, 306)], [(141, 143), (319, 321), (334, 336)], [(319, 324), (334, 337)], [(145, 147), (398, 406)], [(436, 437), (439, 439)], [(567, 573), (579, 579)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7241, coref_recall: 0.2711, coref_f1: 0.3940, mention_recall: 0.4406, batch_loss: 42.4025, loss: 19.4236 ||:  65%|######5   | 223/343 [00:14<00:06, 17.19it/s]predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(51, 55), (72, 72), (76, 76), (80, 80)], [(83, 95), (127, 129)], [(165, 165), (170, 170), (175, 175), (185, 187), (191, 191), (205, 205), (263, 263), (313, 313)], [(278, 279), (281, 281), (292, 292)], [(278, 278), (289, 289)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(12, 17), (19, 19), (27, 28), (74, 74), (90, 91), (121, 122), (132, 132), (141, 142), (169, 169), (179, 179), (183, 183), (194, 194), (201, 201), (214, 214), (235, 236)], [(218, 218), (221, 221)], [(284, 284), (296, 296), (305, 305), (310, 310), (316, 316), (324, 324), (333, 333), (343, 345), (353, 353), (360, 360)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(25, 26), (45, 46), (96, 97), (198, 199), (217, 218), (290, 290), (335, 335)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7253, coref_recall: 0.2717, coref_f1: 0.3947, mention_recall: 0.4409, batch_loss: 0.5741, loss: 19.2464 ||:  66%|######5   | 226/343 [00:14<00:06, 18.33it/s] predicted_antecedents size torch.Size([1, 80])\n",
            "predicted_antecedents_float size torch.Size([1, 80])\n",
            "probs_logs size: torch.Size([1, 80, 6])\n",
            "probs size: torch.Size([1, 80, 6])\n",
            "probs_flat size: torch.Size([1, 480])\n",
            "gold_antecedent_labels size: torch.Size([1, 80, 6])\n",
            "clusters (batch_clusters) [[[(19, 20), (35, 36)], [(38, 39), (63, 65)], [(86, 86), (88, 88), (121, 121), (397, 397), (417, 417), (419, 419), (533, 533), (891, 891), (904, 904), (918, 918), (921, 921), (924, 924), (935, 935), (939, 939), (949, 949), (955, 955), (957, 957)], [(136, 147), (170, 170)], [(188, 191), (196, 197)], [(222, 223), (226, 226), (265, 266), (963, 963)], [(509, 509), (511, 511), (516, 516), (523, 523), (531, 531)], [(722, 722), (735, 735)], [(909, 909), (913, 913)], [(902, 902), (928, 928), (931, 931), (946, 946)], [(967, 968), (976, 976), (980, 980), (987, 987), (994, 994)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 34])\n",
            "predicted_antecedents_float size torch.Size([1, 34])\n",
            "probs_logs size: torch.Size([1, 34, 6])\n",
            "probs size: torch.Size([1, 34, 6])\n",
            "probs_flat size: torch.Size([1, 204])\n",
            "gold_antecedent_labels size: torch.Size([1, 34, 6])\n",
            "clusters (batch_clusters) [[[(16, 21), (90, 94), (100, 105), (111, 111), (132, 136), (152, 152), (156, 156), (183, 188), (329, 333)], [(256, 260), (264, 266)], [(67, 68), (274, 280), (278, 280), (292, 296), (308, 312)], [(223, 227), (300, 304)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7231, coref_recall: 0.2707, coref_f1: 0.3934, mention_recall: 0.4409, batch_loss: 51.3531, loss: 19.5463 ||:  66%|######6   | 228/343 [00:14<00:07, 15.34it/s]predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(30, 31), (70, 75)], [(67, 67), (83, 84), (222, 226), (245, 247), (309, 310), (405, 407), (435, 436), (469, 470)], [(113, 113), (117, 120), (189, 191), (199, 199), (218, 219), (265, 266)], [(239, 240), (242, 242)], [(67, 75), (279, 280), (289, 291), (458, 460)], [(328, 330), (390, 391)], [(441, 447), (452, 454)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 81])\n",
            "predicted_antecedents_float size torch.Size([1, 81])\n",
            "probs_logs size: torch.Size([1, 81, 6])\n",
            "probs size: torch.Size([1, 81, 6])\n",
            "probs_flat size: torch.Size([1, 486])\n",
            "gold_antecedent_labels size: torch.Size([1, 81, 6])\n",
            "clusters (batch_clusters) [[[(69, 70), (94, 94), (118, 118), (120, 120), (134, 134), (137, 137), (142, 142), (150, 150), (156, 156), (163, 163), (168, 168), (175, 175), (190, 190), (213, 213), (235, 235), (250, 250), (266, 266), (297, 297)], [(187, 187), (218, 218)], [(257, 258), (260, 260), (556, 559), (824, 825), (849, 865), (872, 893), (933, 934)], [(269, 275), (314, 314), (444, 444), (463, 463), (478, 478), (503, 503), (520, 520)], [(239, 239), (365, 365)], [(581, 583), (599, 601)], [(594, 608), (633, 633), (654, 654)], [(676, 680), (683, 685)], [(930, 934), (961, 961)], [(966, 990), (992, 992), (1008, 1008), (1013, 1013)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7237, coref_recall: 0.2714, coref_f1: 0.3941, mention_recall: 0.4420, batch_loss: 43.3662, loss: 19.6709 ||:  67%|######7   | 230/343 [00:14<00:08, 13.51it/s]predicted_antecedents size torch.Size([1, 41])\n",
            "predicted_antecedents_float size torch.Size([1, 41])\n",
            "probs_logs size: torch.Size([1, 41, 6])\n",
            "probs size: torch.Size([1, 41, 6])\n",
            "probs_flat size: torch.Size([1, 246])\n",
            "gold_antecedent_labels size: torch.Size([1, 41, 6])\n",
            "clusters (batch_clusters) [[[(1, 2), (6, 7), (9, 10), (27, 29)], [(117, 118), (121, 121), (125, 125), (127, 127)], [(145, 151), (154, 154)], [(200, 200), (237, 238), (254, 254), (334, 334), (356, 358)], [(283, 288), (291, 291)], [(372, 372), (392, 393)], [(179, 179), (395, 395), (418, 418), (424, 424), (450, 450), (463, 463), (476, 476), (501, 502)], [(483, 485), (501, 505)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(13, 18), (20, 21), (51, 51)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(0, 5), (7, 7), (27, 27), (58, 64), (75, 75), (94, 94), (136, 136), (161, 162)], [(27, 29), (45, 49)], [(146, 156), (166, 166)], [(176, 176), (181, 182)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7230, coref_recall: 0.2714, coref_f1: 0.3941, mention_recall: 0.4420, batch_loss: 17.9177, loss: 19.6293 ||:  68%|######7   | 233/343 [00:14<00:06, 15.80it/s]predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(22, 25), (46, 49), (64, 67), (131, 134)], [(447, 468), (478, 479), (514, 515)], [(452, 456), (521, 523)], [(192, 194), (545, 547), (600, 602)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 102])\n",
            "predicted_antecedents_float size torch.Size([1, 102])\n",
            "probs_logs size: torch.Size([1, 102, 6])\n",
            "probs size: torch.Size([1, 102, 6])\n",
            "probs_flat size: torch.Size([1, 612])\n",
            "gold_antecedent_labels size: torch.Size([1, 102, 6])\n",
            "clusters (batch_clusters) [[[(50, 50), (84, 86), (203, 206), (221, 221), (225, 225), (1265, 1265)], [(116, 139), (131, 131), (229, 231), (246, 248)], [(97, 109), (240, 242), (345, 346)], [(318, 319), (330, 330)], [(365, 369), (379, 381)], [(453, 455), (458, 458), (482, 484), (495, 495), (523, 524), (1244, 1246), (1260, 1262)], [(591, 591), (599, 599)], [(607, 607), (609, 613)], [(638, 647), (661, 661)], [(684, 693), (698, 698)], [(725, 733), (738, 738), (760, 760), (773, 776), (788, 788), (790, 790), (797, 797), (805, 805), (818, 818)], [(444, 445), (842, 843), (1270, 1271)], [(266, 269), (951, 951)], [(1072, 1076), (1094, 1096), (1187, 1188)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7217, coref_recall: 0.2713, coref_f1: 0.3938, mention_recall: 0.4429, batch_loss: 68.9819, loss: 19.8101 ||:  69%|######8   | 235/343 [00:15<00:08, 12.43it/s]predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(156, 156), (175, 175), (192, 194), (206, 208)], [(122, 123), (201, 204)], [(164, 165), (217, 218), (336, 337)], [(82, 94), (222, 225)], [(233, 246), (249, 249)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 55])\n",
            "predicted_antecedents_float size torch.Size([1, 55])\n",
            "probs_logs size: torch.Size([1, 55, 6])\n",
            "probs size: torch.Size([1, 55, 6])\n",
            "probs_flat size: torch.Size([1, 330])\n",
            "gold_antecedent_labels size: torch.Size([1, 55, 6])\n",
            "clusters (batch_clusters) [[[(53, 58), (63, 65), (327, 329), (349, 351)], [(37, 44), (196, 197), (548, 550)], [(208, 213), (302, 303)], [(411, 411), (412, 412), (414, 414), (418, 418), (435, 435), (439, 439), (445, 445), (465, 465), (478, 478), (495, 495), (589, 589)], [(425, 426), (462, 463)], [(514, 514), (520, 520), (535, 535)], [(597, 599), (603, 603), (624, 626), (640, 640)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7213, coref_recall: 0.2707, coref_f1: 0.3931, mention_recall: 0.4428, batch_loss: 19.8647, loss: 19.8244 ||:  69%|######9   | 237/343 [00:15<00:08, 12.96it/s]predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(11, 12), (19, 19), (24, 24), (27, 29), (37, 37), (45, 46), (156, 157), (200, 200), (211, 212), (321, 321)], [(227, 229), (234, 234)], [(316, 319), (317, 319), (328, 329), (332, 332), (334, 334), (346, 346), (367, 367), (382, 382), (384, 384)], [(104, 104), (339, 339), (344, 344), (370, 370), (388, 388)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 50])\n",
            "predicted_antecedents_float size torch.Size([1, 50])\n",
            "probs_logs size: torch.Size([1, 50, 6])\n",
            "probs size: torch.Size([1, 50, 6])\n",
            "probs_flat size: torch.Size([1, 300])\n",
            "gold_antecedent_labels size: torch.Size([1, 50, 6])\n",
            "clusters (batch_clusters) [[[(0, 5), (9, 9), (13, 13), (148, 148), (160, 161), (160, 167), (212, 212), (215, 216), (249, 250), (269, 272), (284, 284), (333, 333), (375, 375), (380, 380), (390, 390), (401, 401), (404, 404), (458, 458), (496, 496), (503, 503)], [(20, 22), (36, 36), (72, 76), (92, 92), (330, 330), (340, 340), (411, 411), (567, 568), (574, 574), (590, 590), (605, 606), (628, 628)], [(126, 134), (146, 146), (358, 358), (393, 393)], [(222, 232), (234, 234)], [(578, 579), (599, 599)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7206, coref_recall: 0.2697, coref_f1: 0.3919, mention_recall: 0.4414, batch_loss: 77.5278, loss: 20.0502 ||:  70%|######9   | 239/343 [00:15<00:07, 13.40it/s]predicted_antecedents size torch.Size([1, 40])\n",
            "predicted_antecedents_float size torch.Size([1, 40])\n",
            "probs_logs size: torch.Size([1, 40, 6])\n",
            "probs size: torch.Size([1, 40, 6])\n",
            "probs_flat size: torch.Size([1, 240])\n",
            "gold_antecedent_labels size: torch.Size([1, 40, 6])\n",
            "clusters (batch_clusters) [[[(7, 8), (25, 29), (130, 130), (146, 146), (167, 167)], [(132, 139), (151, 151), (162, 162)], [(361, 361), (391, 391), (482, 482), (493, 493), (499, 499), (506, 506), (508, 508)], [(408, 412), (419, 419)], [(429, 433), (440, 440), (442, 442)], [(219, 219), (485, 485)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 38])\n",
            "predicted_antecedents_float size torch.Size([1, 38])\n",
            "probs_logs size: torch.Size([1, 38, 6])\n",
            "probs size: torch.Size([1, 38, 6])\n",
            "probs_flat size: torch.Size([1, 228])\n",
            "gold_antecedent_labels size: torch.Size([1, 38, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (9, 9), (21, 21)], [(56, 58), (81, 82), (113, 115), (178, 179), (196, 197), (209, 209), (224, 225), (425, 427), (441, 442), (456, 457)], [(72, 73), (90, 90), (151, 151), (254, 254), (277, 277), (313, 313), (355, 355), (378, 378), (381, 381), (386, 386), (416, 417)], [(165, 168), (186, 186)], [(240, 240), (245, 245)], [(330, 333), (374, 374)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7217, coref_recall: 0.2700, coref_f1: 0.3924, mention_recall: 0.4414, batch_loss: 27.0081, loss: 20.0575 ||:  70%|#######   | 241/343 [00:15<00:07, 13.59it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(43, 46), (48, 48)], [(93, 93), (99, 99)], [(0, 5), (186, 189)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 43])\n",
            "predicted_antecedents_float size torch.Size([1, 43])\n",
            "probs_logs size: torch.Size([1, 43, 6])\n",
            "probs size: torch.Size([1, 43, 6])\n",
            "probs_flat size: torch.Size([1, 258])\n",
            "gold_antecedent_labels size: torch.Size([1, 43, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (15, 19), (30, 30), (60, 64), (67, 67), (102, 102), (116, 117), (219, 222), (276, 279)], [(21, 24), (38, 39)], [(140, 142), (150, 151), (243, 245)], [(162, 162), (173, 173)], [(302, 303), (325, 326)], [(336, 337), (479, 480), (528, 529)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7222, coref_recall: 0.2701, coref_f1: 0.3926, mention_recall: 0.4414, batch_loss: 15.7579, loss: 19.9954 ||:  71%|#######   | 243/343 [00:15<00:07, 13.85it/s]predicted_antecedents size torch.Size([1, 35])\n",
            "predicted_antecedents_float size torch.Size([1, 35])\n",
            "probs_logs size: torch.Size([1, 35, 6])\n",
            "probs size: torch.Size([1, 35, 6])\n",
            "probs_flat size: torch.Size([1, 210])\n",
            "gold_antecedent_labels size: torch.Size([1, 35, 6])\n",
            "clusters (batch_clusters) [[[(16, 19), (110, 113)], [(165, 165), (193, 193), (208, 208), (225, 225), (238, 238), (251, 251)], [(201, 205), (212, 212), (240, 241), (243, 243)], [(220, 222), (256, 257), (306, 306), (313, 329), (365, 365)], [(386, 386), (389, 392), (408, 408), (413, 413)], [(398, 403), (405, 405)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(0, 2), (6, 6), (38, 40), (67, 68), (91, 92), (94, 94), (103, 116), (120, 120)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(28, 43), (45, 46), (94, 95)], [(17, 19), (97, 100)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7229, coref_recall: 0.2705, coref_f1: 0.3931, mention_recall: 0.4419, batch_loss: 6.2889, loss: 19.7746 ||:  72%|#######2  | 247/343 [00:15<00:05, 18.66it/s] predicted_antecedents size torch.Size([1, 54])\n",
            "predicted_antecedents_float size torch.Size([1, 54])\n",
            "probs_logs size: torch.Size([1, 54, 6])\n",
            "probs size: torch.Size([1, 54, 6])\n",
            "probs_flat size: torch.Size([1, 324])\n",
            "gold_antecedent_labels size: torch.Size([1, 54, 6])\n",
            "clusters (batch_clusters) [[[(28, 43), (59, 59), (61, 61), (172, 172), (179, 179), (183, 183), (301, 301), (308, 308), (555, 555), (557, 557), (588, 588), (621, 621), (625, 625), (650, 650), (655, 656), (663, 664), (667, 667)], [(112, 112), (131, 131)], [(420, 420), (437, 437)], [(443, 444), (447, 447)], [(430, 430), (465, 465), (484, 485)], [(673, 673), (676, 676)], [(681, 681), (684, 684)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(13, 18), (86, 88), (105, 106), (149, 150), (175, 176)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7226, coref_recall: 0.2696, coref_f1: 0.3921, mention_recall: 0.4421, batch_loss: 12.5270, loss: 19.8716 ||:  73%|#######2  | 250/343 [00:15<00:04, 20.09it/s]predicted_antecedents size torch.Size([1, 26])\n",
            "predicted_antecedents_float size torch.Size([1, 26])\n",
            "probs_logs size: torch.Size([1, 26, 6])\n",
            "probs size: torch.Size([1, 26, 6])\n",
            "probs_flat size: torch.Size([1, 156])\n",
            "gold_antecedent_labels size: torch.Size([1, 26, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (36, 36), (48, 48), (59, 59), (65, 65), (309, 312)], [(109, 121), (137, 144), (197, 198), (229, 231)], [(129, 144), (168, 168)], [(181, 188), (190, 190)], [(241, 241), (272, 272), (306, 306)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(4, 11), (35, 35), (51, 51), (65, 65), (121, 121), (146, 146)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 72])\n",
            "predicted_antecedents_float size torch.Size([1, 72])\n",
            "probs_logs size: torch.Size([1, 72, 6])\n",
            "probs size: torch.Size([1, 72, 6])\n",
            "probs_flat size: torch.Size([1, 432])\n",
            "gold_antecedent_labels size: torch.Size([1, 72, 6])\n",
            "clusters (batch_clusters) [[[(41, 42), (62, 64)], [(193, 196), (208, 208)], [(37, 42), (267, 269)], [(323, 342), (366, 368), (398, 399), (420, 420), (438, 439)], [(451, 451), (458, 458)], [(515, 516), (534, 535)], [(550, 552), (564, 568)], [(575, 587), (609, 610), (615, 616), (632, 632), (663, 663)], [(635, 635), (665, 665), (668, 668)], [(724, 725), (780, 783), (859, 859)], [(811, 812), (824, 824)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7222, coref_recall: 0.2702, coref_f1: 0.3926, mention_recall: 0.4433, batch_loss: 39.2544, loss: 19.9071 ||:  74%|#######3  | 253/343 [00:16<00:04, 19.20it/s]predicted_antecedents size torch.Size([1, 79])\n",
            "predicted_antecedents_float size torch.Size([1, 79])\n",
            "probs_logs size: torch.Size([1, 79, 6])\n",
            "probs size: torch.Size([1, 79, 6])\n",
            "probs_flat size: torch.Size([1, 474])\n",
            "gold_antecedent_labels size: torch.Size([1, 79, 6])\n",
            "clusters (batch_clusters) [[[(9, 9), (14, 14)], [(128, 128), (141, 141)], [(225, 225), (228, 228)], [(219, 220), (233, 233)], [(254, 254), (257, 257), (308, 308), (394, 394), (757, 757), (817, 817)], [(317, 320), (328, 328), (335, 335), (356, 357), (372, 372), (375, 375), (378, 378), (387, 387)], [(616, 616), (629, 629), (650, 650)], [(783, 783), (799, 799)], [(845, 845), (856, 856)], [(890, 890), (967, 967), (978, 978)], [(948, 948), (971, 973)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 61])\n",
            "predicted_antecedents_float size torch.Size([1, 61])\n",
            "probs_logs size: torch.Size([1, 61, 6])\n",
            "probs size: torch.Size([1, 61, 6])\n",
            "probs_flat size: torch.Size([1, 366])\n",
            "gold_antecedent_labels size: torch.Size([1, 61, 6])\n",
            "clusters (batch_clusters) [[[(20, 32), (34, 35)], [(7, 8), (75, 75), (104, 104), (110, 110), (168, 168), (207, 207), (219, 219), (224, 224), (231, 231), (297, 297), (299, 299), (313, 313), (316, 316), (327, 327), (332, 332), (335, 335), (338, 338), (356, 356), (385, 385), (499, 499), (528, 528), (531, 531), (546, 546), (553, 553), (600, 600), (628, 628), (638, 639), (668, 669), (680, 680), (693, 693), (744, 744)], [(86, 102), (106, 106), (116, 116), (239, 239), (244, 244), (249, 251), (359, 359), (383, 383), (543, 543), (649, 651)], [(443, 443), (445, 445), (575, 575)], [(420, 420), (478, 478), (592, 592)], [(704, 704), (718, 718), (727, 727)], [(746, 746), (760, 760)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7201, coref_recall: 0.2701, coref_f1: 0.3922, mention_recall: 0.4440, batch_loss: 32.5678, loss: 20.0560 ||:  74%|#######4  | 255/343 [00:16<00:05, 16.35it/s]predicted_antecedents size torch.Size([1, 39])\n",
            "predicted_antecedents_float size torch.Size([1, 39])\n",
            "probs_logs size: torch.Size([1, 39, 6])\n",
            "probs size: torch.Size([1, 39, 6])\n",
            "probs_flat size: torch.Size([1, 234])\n",
            "gold_antecedent_labels size: torch.Size([1, 39, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (6, 6), (28, 28), (44, 44), (78, 78), (92, 92), (95, 95), (101, 101), (159, 159), (441, 442)], [(107, 115), (125, 125), (133, 134)], [(238, 238), (245, 245), (251, 251)], [(406, 407), (417, 417), (423, 423)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 14])\n",
            "predicted_antecedents_float size torch.Size([1, 14])\n",
            "probs_logs size: torch.Size([1, 14, 6])\n",
            "probs size: torch.Size([1, 14, 6])\n",
            "probs_flat size: torch.Size([1, 84])\n",
            "gold_antecedent_labels size: torch.Size([1, 14, 6])\n",
            "clusters (batch_clusters) [[[(17, 18), (29, 31)], [(11, 13), (47, 47)], [(110, 132), (136, 136)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 45])\n",
            "predicted_antecedents_float size torch.Size([1, 45])\n",
            "probs_logs size: torch.Size([1, 45, 6])\n",
            "probs size: torch.Size([1, 45, 6])\n",
            "probs_flat size: torch.Size([1, 270])\n",
            "gold_antecedent_labels size: torch.Size([1, 45, 6])\n",
            "clusters (batch_clusters) [[[(7, 7), (39, 39), (54, 54), (81, 81), (88, 88), (98, 98), (118, 118), (121, 121), (141, 141), (186, 186), (191, 191), (199, 199), (213, 213), (223, 223), (237, 237), (243, 243), (256, 256), (262, 262), (270, 270), (278, 278), (286, 286), (293, 293), (304, 304), (310, 310), (332, 332), (405, 405), (413, 413), (422, 422), (445, 445), (469, 469), (482, 482), (485, 485), (488, 488), (493, 493), (495, 495), (506, 506), (511, 511), (525, 525), (544, 544)], [(262, 264), (270, 272)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2709, coref_f1: 0.3931, mention_recall: 0.4447, batch_loss: 0.8948, loss: 19.9342 ||:  75%|#######5  | 258/343 [00:16<00:04, 17.27it/s] predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(7, 14), (29, 35), (75, 76)], [(4, 14), (100, 101), (129, 131)], [(116, 127), (135, 137)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 24])\n",
            "predicted_antecedents_float size torch.Size([1, 24])\n",
            "probs_logs size: torch.Size([1, 24, 6])\n",
            "probs size: torch.Size([1, 24, 6])\n",
            "probs_flat size: torch.Size([1, 144])\n",
            "gold_antecedent_labels size: torch.Size([1, 24, 6])\n",
            "clusters (batch_clusters) [[[(4, 4), (22, 23), (277, 278)], [(39, 49), (52, 52)], [(22, 29), (63, 64), (82, 84), (107, 108), (113, 119), (142, 143), (183, 184), (210, 211), (227, 229)], [(172, 173), (224, 225)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 31])\n",
            "predicted_antecedents_float size torch.Size([1, 31])\n",
            "probs_logs size: torch.Size([1, 31, 6])\n",
            "probs size: torch.Size([1, 31, 6])\n",
            "probs_flat size: torch.Size([1, 186])\n",
            "gold_antecedent_labels size: torch.Size([1, 31, 6])\n",
            "clusters (batch_clusters) [[[(7, 25), (49, 49), (65, 65), (78, 78), (93, 93), (95, 95), (117, 117), (170, 170), (206, 206), (252, 252), (277, 277)], [(51, 62), (90, 90), (99, 99), (108, 108), (114, 114)], [(213, 213), (227, 227)], [(246, 246), (273, 273)], [(265, 265), (362, 362), (373, 373), (383, 383)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7208, coref_recall: 0.2714, coref_f1: 0.3937, mention_recall: 0.4449, batch_loss: 24.1811, loss: 19.8259 ||:  76%|#######6  | 261/343 [00:16<00:04, 19.40it/s]predicted_antecedents size torch.Size([1, 36])\n",
            "predicted_antecedents_float size torch.Size([1, 36])\n",
            "probs_logs size: torch.Size([1, 36, 6])\n",
            "probs size: torch.Size([1, 36, 6])\n",
            "probs_flat size: torch.Size([1, 216])\n",
            "gold_antecedent_labels size: torch.Size([1, 36, 6])\n",
            "clusters (batch_clusters) [[[(87, 89), (95, 95), (148, 148), (156, 156), (177, 179), (228, 230), (238, 238)], [(57, 58), (118, 119)], [(213, 219), (233, 233), (246, 246), (266, 266), (303, 303)], [(181, 181), (240, 240), (248, 248)], [(358, 360), (391, 391), (398, 398), (401, 401), (404, 404), (411, 411), (414, 414), (416, 416), (418, 418), (450, 450)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 64])\n",
            "predicted_antecedents_float size torch.Size([1, 64])\n",
            "probs_logs size: torch.Size([1, 64, 6])\n",
            "probs size: torch.Size([1, 64, 6])\n",
            "probs_flat size: torch.Size([1, 384])\n",
            "gold_antecedent_labels size: torch.Size([1, 64, 6])\n",
            "clusters (batch_clusters) [[[(9, 10), (15, 15), (220, 221)], [(20, 32), (39, 39)], [(135, 135), (143, 143), (170, 171), (181, 181), (185, 185), (197, 197), (205, 205)], [(51, 52), (213, 214)], [(268, 269), (272, 272), (283, 283), (301, 301), (313, 313), (329, 330), (371, 372), (407, 407), (414, 414), (446, 447), (468, 468), (482, 482)], [(343, 347), (404, 404), (434, 438), (450, 450)], [(395, 398), (409, 409)], [(513, 514), (520, 521), (526, 526)], [(465, 466), (543, 544), (548, 548)], [(599, 599), (612, 612), (618, 618), (637, 638), (646, 646), (684, 684), (690, 690), (704, 705), (715, 715), (733, 733), (744, 744), (766, 767), (775, 775), (780, 780), (797, 797)], [(719, 722), (730, 730)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(88, 88), (93, 93)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7201, coref_recall: 0.2697, coref_f1: 0.3918, mention_recall: 0.4424, batch_loss: 0.0166, loss: 19.8871 ||:  77%|#######6  | 264/343 [00:16<00:04, 19.26it/s] predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(15, 16), (48, 49), (57, 57), (98, 99), (235, 236), (243, 243), (255, 255), (329, 330), (345, 345), (353, 355), (383, 385), (392, 392), (430, 430), (439, 439), (457, 458), (462, 462)], [(144, 150), (189, 190)], [(170, 170), (195, 195)], [(199, 200), (202, 205), (210, 210), (215, 215), (226, 226)], [(284, 284), (287, 287), (306, 306), (317, 317)], [(390, 390), (445, 445), (449, 449)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 22])\n",
            "predicted_antecedents_float size torch.Size([1, 22])\n",
            "probs_logs size: torch.Size([1, 22, 6])\n",
            "probs size: torch.Size([1, 22, 6])\n",
            "probs_flat size: torch.Size([1, 132])\n",
            "gold_antecedent_labels size: torch.Size([1, 22, 6])\n",
            "clusters (batch_clusters) [[[(56, 58), (69, 69)], [(31, 32), (87, 88)], [(202, 204), (225, 227), (247, 247), (254, 254), (265, 265)], [(235, 236), (251, 252)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(68, 72), (91, 91), (118, 120), (176, 178), (288, 292)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7199, coref_recall: 0.2695, coref_f1: 0.3916, mention_recall: 0.4423, batch_loss: 0.4604, loss: 19.8154 ||:  78%|#######7  | 267/343 [00:16<00:03, 19.21it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(12, 14), (86, 88), (162, 164)], [(121, 122), (125, 126), (215, 216)], [(18, 26), (150, 164), (198, 201)], [(183, 185), (189, 191)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 13])\n",
            "predicted_antecedents_float size torch.Size([1, 13])\n",
            "probs_logs size: torch.Size([1, 13, 6])\n",
            "probs size: torch.Size([1, 13, 6])\n",
            "probs_flat size: torch.Size([1, 78])\n",
            "gold_antecedent_labels size: torch.Size([1, 13, 6])\n",
            "clusters (batch_clusters) [[[(6, 9), (16, 17)], [(19, 19), (28, 28), (33, 33)], [(119, 120), (122, 122)], [(114, 114), (126, 126)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(17, 29), (81, 81)], [(49, 52), (116, 117)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 71])\n",
            "predicted_antecedents_float size torch.Size([1, 71])\n",
            "probs_logs size: torch.Size([1, 71, 6])\n",
            "probs size: torch.Size([1, 71, 6])\n",
            "probs_flat size: torch.Size([1, 426])\n",
            "gold_antecedent_labels size: torch.Size([1, 71, 6])\n",
            "clusters (batch_clusters) [[[(14, 14), (29, 29), (33, 33), (40, 40), (52, 52), (57, 57), (75, 75), (87, 87), (116, 116), (213, 213), (241, 241), (250, 250), (261, 261), (279, 279), (350, 350), (353, 353), (390, 390), (393, 393), (399, 399), (403, 403), (411, 411), (419, 419), (423, 423), (429, 429), (465, 465), (486, 486), (502, 502), (510, 510), (513, 513), (556, 556), (586, 586), (650, 650), (681, 681), (703, 703), (716, 716), (727, 727), (735, 735), (795, 795), (803, 803), (819, 819), (841, 841)], [(177, 177), (192, 192)], [(81, 81), (459, 459), (580, 580), (592, 592), (628, 631), (658, 658)], [(545, 545), (567, 567), (605, 605), (641, 641), (696, 696), (706, 706), (778, 778)], [(38, 38), (822, 822), (828, 828)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2701, coref_f1: 0.3922, mention_recall: 0.4428, batch_loss: 39.7336, loss: 19.7182 ||:  79%|#######9  | 271/343 [00:17<00:03, 21.11it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(20, 20), (34, 34), (61, 61), (87, 87), (107, 107), (122, 122), (148, 148), (152, 152), (199, 199), (223, 223), (230, 230), (236, 236)], [(72, 73), (93, 94), (100, 100), (109, 109), (126, 126), (134, 134), (150, 150), (197, 197)], [(245, 245), (248, 248), (269, 269), (291, 291)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(0, 4), (22, 25), (52, 52)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 58])\n",
            "predicted_antecedents_float size torch.Size([1, 58])\n",
            "probs_logs size: torch.Size([1, 58, 6])\n",
            "probs size: torch.Size([1, 58, 6])\n",
            "probs_flat size: torch.Size([1, 348])\n",
            "gold_antecedent_labels size: torch.Size([1, 58, 6])\n",
            "clusters (batch_clusters) [[[(78, 82), (93, 93), (96, 96), (115, 115), (133, 133), (321, 321)], [(150, 154), (172, 176)], [(157, 170), (225, 226), (527, 528), (646, 647)], [(317, 332), (347, 347)], [(364, 390), (387, 387), (406, 406)], [(580, 581), (605, 606)], [(652, 652), (658, 658)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7206, coref_recall: 0.2703, coref_f1: 0.3925, mention_recall: 0.4433, batch_loss: 45.6698, loss: 19.7141 ||:  80%|#######9  | 274/343 [00:17<00:03, 20.18it/s]predicted_antecedents size torch.Size([1, 49])\n",
            "predicted_antecedents_float size torch.Size([1, 49])\n",
            "probs_logs size: torch.Size([1, 49, 6])\n",
            "probs size: torch.Size([1, 49, 6])\n",
            "probs_flat size: torch.Size([1, 294])\n",
            "gold_antecedent_labels size: torch.Size([1, 49, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (111, 133)], [(31, 37), (157, 159), (199, 201), (200, 200), (203, 205), (215, 215)], [(247, 252), (272, 272), (283, 283), (294, 294)], [(325, 327), (471, 472), (496, 496), (504, 506), (515, 515)], [(604, 604), (609, 609)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(0, 9), (33, 33), (61, 64)], [(20, 24), (45, 48)], [(140, 140), (265, 265)], [(344, 345), (349, 349)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 76])\n",
            "predicted_antecedents_float size torch.Size([1, 76])\n",
            "probs_logs size: torch.Size([1, 76, 6])\n",
            "probs size: torch.Size([1, 76, 6])\n",
            "probs_flat size: torch.Size([1, 456])\n",
            "gold_antecedent_labels size: torch.Size([1, 76, 6])\n",
            "clusters (batch_clusters) [[[(36, 38), (43, 43)], [(57, 59), (108, 108), (363, 365), (433, 435), (482, 484), (489, 489), (494, 494)], [(189, 190), (212, 213), (239, 240)], [(260, 262), (321, 323), (342, 344), (348, 350), (373, 375), (380, 382), (416, 419)], [(325, 327), (352, 353), (361, 361)], [(230, 232), (409, 411)], [(513, 517), (531, 531)], [(537, 546), (545, 545), (575, 575)], [(952, 953), (956, 956)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7202, coref_recall: 0.2705, coref_f1: 0.3927, mention_recall: 0.4446, batch_loss: 27.6026, loss: 19.7153 ||:  81%|########  | 277/343 [00:17<00:03, 16.77it/s]predicted_antecedents size torch.Size([1, 12])\n",
            "predicted_antecedents_float size torch.Size([1, 12])\n",
            "probs_logs size: torch.Size([1, 12, 6])\n",
            "probs size: torch.Size([1, 12, 6])\n",
            "probs_flat size: torch.Size([1, 72])\n",
            "gold_antecedent_labels size: torch.Size([1, 12, 6])\n",
            "clusters (batch_clusters) [[[(14, 16), (19, 19)], [(22, 26), (29, 29), (138, 138)], [(82, 84), (109, 111)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(65, 65), (67, 67)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(0, 7), (14, 14), (84, 84)], [(9, 12), (20, 20), (50, 61)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 55])\n",
            "predicted_antecedents_float size torch.Size([1, 55])\n",
            "probs_logs size: torch.Size([1, 55, 6])\n",
            "probs size: torch.Size([1, 55, 6])\n",
            "probs_flat size: torch.Size([1, 330])\n",
            "gold_antecedent_labels size: torch.Size([1, 55, 6])\n",
            "clusters (batch_clusters) [[[(22, 22), (43, 43), (52, 52), (67, 67)], [(93, 96), (108, 108), (139, 142), (152, 152), (260, 263), (278, 279), (584, 586), (601, 601), (607, 607), (614, 614)], [(190, 193), (202, 202)], [(366, 368), (439, 440), (445, 446), (450, 450), (462, 462), (465, 465), (596, 598)], [(509, 509), (515, 515)], [(123, 125), (645, 647)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7208, coref_recall: 0.2708, coref_f1: 0.3931, mention_recall: 0.4445, batch_loss: 9.7057, loss: 19.4742 ||:  82%|########1 | 281/343 [00:17<00:03, 18.72it/s] predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(1, 1), (3, 3)], [(150, 166), (163, 163)], [(103, 106), (173, 174)], [(63, 65), (190, 192), (195, 195), (211, 213), (277, 279)], [(216, 218), (221, 221), (237, 237)], [(16, 31), (254, 257)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(5, 9), (14, 17), (36, 38), (63, 65), (82, 83), (134, 136), (162, 163), (166, 166), (200, 204)], [(47, 47), (49, 49), (189, 204)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 90])\n",
            "predicted_antecedents_float size torch.Size([1, 90])\n",
            "probs_logs size: torch.Size([1, 90, 6])\n",
            "probs size: torch.Size([1, 90, 6])\n",
            "probs_flat size: torch.Size([1, 540])\n",
            "gold_antecedent_labels size: torch.Size([1, 90, 6])\n",
            "clusters (batch_clusters) [[[(9, 13), (49, 53), (67, 67), (87, 87)], [(258, 261), (277, 277)], [(116, 121), (290, 294)], [(246, 247), (323, 325), (521, 522), (550, 552), (559, 561), (616, 617), (689, 691), (958, 959)], [(307, 311), (348, 348)], [(376, 397), (406, 408)], [(317, 320), (459, 461)], [(470, 474), (540, 544), (813, 819), (847, 853)], [(696, 696), (712, 714), (725, 725), (813, 815), (847, 849), (882, 884), (893, 894)], [(567, 570), (720, 723)], [(980, 992), (1039, 1042)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7186, coref_recall: 0.2702, coref_f1: 0.3922, mention_recall: 0.4438, batch_loss: 59.5397, loss: 19.6467 ||:  83%|########2 | 284/343 [00:17<00:03, 16.99it/s]predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (7, 7), (27, 27), (78, 81), (90, 93), (165, 167), (182, 182), (201, 204)], [(72, 81), (103, 105), (257, 261), (281, 282), (293, 295)], [(334, 336), (349, 349)], [(243, 245), (360, 360)], [(54, 55), (392, 393)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 8])\n",
            "predicted_antecedents_float size torch.Size([1, 8])\n",
            "probs_logs size: torch.Size([1, 8, 6])\n",
            "probs size: torch.Size([1, 8, 6])\n",
            "probs_flat size: torch.Size([1, 48])\n",
            "gold_antecedent_labels size: torch.Size([1, 8, 6])\n",
            "clusters (batch_clusters) [[[(12, 15), (38, 39), (57, 64), (84, 84)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 70])\n",
            "predicted_antecedents_float size torch.Size([1, 70])\n",
            "probs_logs size: torch.Size([1, 70, 6])\n",
            "probs size: torch.Size([1, 70, 6])\n",
            "probs_flat size: torch.Size([1, 420])\n",
            "gold_antecedent_labels size: torch.Size([1, 70, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (3, 4), (7, 7), (16, 16), (64, 64), (122, 122), (131, 131), (137, 137), (161, 161), (171, 171), (181, 181), (246, 246), (285, 285), (286, 286), (308, 308), (312, 312), (324, 324), (333, 333), (354, 354), (382, 382), (423, 423), (451, 451), (456, 456), (463, 463), (467, 467), (478, 478), (492, 492), (498, 498), (557, 557), (564, 564), (574, 574), (585, 585), (600, 600), (609, 609), (624, 624), (645, 645), (651, 651), (661, 661), (669, 669), (677, 677), (696, 696), (705, 705), (747, 747), (760, 760), (778, 778), (802, 802), (858, 858), (871, 871)], [(13, 13), (21, 21), (24, 24), (32, 32), (36, 36), (40, 40)], [(66, 66), (76, 76), (79, 79), (82, 82), (86, 86), (90, 90), (94, 94), (114, 114)], [(820, 820), (826, 826), (833, 833)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7189, coref_recall: 0.2705, coref_f1: 0.3925, mention_recall: 0.4441, batch_loss: 46.9876, loss: 19.6776 ||:  84%|########3 | 287/343 [00:18<00:03, 17.64it/s]predicted_antecedents size torch.Size([1, 48])\n",
            "predicted_antecedents_float size torch.Size([1, 48])\n",
            "probs_logs size: torch.Size([1, 48, 6])\n",
            "probs size: torch.Size([1, 48, 6])\n",
            "probs_flat size: torch.Size([1, 288])\n",
            "gold_antecedent_labels size: torch.Size([1, 48, 6])\n",
            "clusters (batch_clusters) [[[(31, 38), (40, 42), (210, 211), (214, 214), (253, 255)], [(102, 102), (124, 125), (232, 232)], [(129, 129), (133, 133)], [(64, 76), (205, 205), (383, 383)], [(218, 221), (229, 230)], [(193, 195), (407, 410)], [(421, 423), (454, 455)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(18, 22), (38, 41)], [(58, 60), (93, 95), (132, 134), (164, 166), (264, 266)], [(75, 81), (115, 117)], [(31, 33), (174, 176), (208, 210), (223, 223), (333, 335), (344, 346), (349, 352)], [(183, 187), (361, 364), (386, 389)], [(306, 308), (391, 393)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7188, coref_recall: 0.2705, coref_f1: 0.3925, mention_recall: 0.4443, batch_loss: 7.3294, loss: 19.6736 ||:  84%|########4 | 289/343 [00:18<00:03, 16.36it/s] predicted_antecedents size torch.Size([1, 32])\n",
            "predicted_antecedents_float size torch.Size([1, 32])\n",
            "probs_logs size: torch.Size([1, 32, 6])\n",
            "probs size: torch.Size([1, 32, 6])\n",
            "probs_flat size: torch.Size([1, 192])\n",
            "gold_antecedent_labels size: torch.Size([1, 32, 6])\n",
            "clusters (batch_clusters) [[[(103, 115), (156, 156), (178, 178), (184, 184), (195, 195), (215, 215), (362, 363)], [(250, 250), (270, 270)], [(368, 368), (372, 372)], [(352, 352), (387, 387)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 80])\n",
            "predicted_antecedents_float size torch.Size([1, 80])\n",
            "probs_logs size: torch.Size([1, 80, 6])\n",
            "probs size: torch.Size([1, 80, 6])\n",
            "probs_flat size: torch.Size([1, 480])\n",
            "gold_antecedent_labels size: torch.Size([1, 80, 6])\n",
            "clusters (batch_clusters) [[[(27, 27), (65, 65), (72, 72), (718, 724)], [(87, 91), (113, 113)], [(116, 116), (127, 127), (263, 263)], [(200, 202), (204, 204)], [(271, 275), (281, 281), (320, 321)], [(316, 321), (325, 325), (342, 342)], [(418, 418), (426, 426), (459, 459), (541, 541), (584, 584), (610, 610), (615, 615), (629, 629), (636, 636), (678, 678)], [(516, 516), (521, 521), (527, 527), (604, 604), (613, 613)], [(555, 555), (558, 558)], [(432, 432), (593, 594), (606, 606)], [(504, 504), (617, 617), (633, 633)], [(784, 786), (905, 906), (945, 946)], [(964, 965), (969, 969), (976, 976)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7191, coref_recall: 0.2714, coref_f1: 0.3934, mention_recall: 0.4450, batch_loss: 51.0292, loss: 19.7244 ||:  85%|########4 | 291/343 [00:18<00:03, 15.20it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(27, 28), (45, 46), (101, 102), (108, 108), (149, 150), (164, 164), (189, 190)], [(105, 106), (128, 129), (173, 174), (193, 194)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(0, 3), (31, 32), (50, 55)], [(11, 22), (69, 70), (74, 75), (97, 98)], [(34, 42), (150, 155), (170, 184)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 120])\n",
            "predicted_antecedents_float size torch.Size([1, 120])\n",
            "probs_logs size: torch.Size([1, 120, 6])\n",
            "probs size: torch.Size([1, 120, 6])\n",
            "probs_flat size: torch.Size([1, 720])\n",
            "gold_antecedent_labels size: torch.Size([1, 120, 6])\n",
            "clusters (batch_clusters) [[[(8, 12), (36, 40), (64, 65), (95, 96), (110, 110), (146, 147), (326, 328), (338, 338), (379, 380), (499, 500), (534, 535), (540, 540), (599, 600), (613, 614), (619, 619), (769, 770), (850, 852), (942, 944), (1007, 1008), (1146, 1147), (1232, 1233), (1242, 1242), (1293, 1294), (1306, 1307), (1469, 1472), (1478, 1478)], [(215, 219), (230, 232), (237, 239)], [(226, 243), (247, 248)], [(260, 260), (267, 267), (303, 304), (564, 564), (717, 717), (812, 812), (933, 934)], [(156, 172), (291, 292), (319, 323), (411, 412), (447, 448), (521, 522), (569, 570), (1277, 1278), (1326, 1327)], [(134, 135), (340, 341), (374, 375), (644, 645), (1314, 1315)], [(203, 206), (844, 844), (960, 960), (1347, 1347), (1403, 1403), (1409, 1409)], [(976, 978), (1106, 1108)], [(1116, 1118), (1131, 1132)], [(1165, 1166), (1196, 1196)], [(1389, 1390), (1393, 1393), (1417, 1418)], [(1454, 1458), (1475, 1475)], [(1439, 1440), (1483, 1484)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7186, coref_recall: 0.2706, coref_f1: 0.3926, mention_recall: 0.4443, batch_loss: 94.1865, loss: 19.8821 ||:  86%|########5 | 294/343 [00:18<00:03, 13.76it/s]predicted_antecedents size torch.Size([1, 18])\n",
            "predicted_antecedents_float size torch.Size([1, 18])\n",
            "probs_logs size: torch.Size([1, 18, 6])\n",
            "probs size: torch.Size([1, 18, 6])\n",
            "probs_flat size: torch.Size([1, 108])\n",
            "gold_antecedent_labels size: torch.Size([1, 18, 6])\n",
            "clusters (batch_clusters) [[[(14, 26), (46, 49), (141, 143), (179, 181), (213, 214)], [(174, 176), (211, 211)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[[(7, 10), (26, 26)], [(45, 52), (66, 67)], [(81, 81), (95, 95)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 91])\n",
            "predicted_antecedents_float size torch.Size([1, 91])\n",
            "probs_logs size: torch.Size([1, 91, 6])\n",
            "probs size: torch.Size([1, 91, 6])\n",
            "probs_flat size: torch.Size([1, 546])\n",
            "gold_antecedent_labels size: torch.Size([1, 91, 6])\n",
            "clusters (batch_clusters) [[[(17, 17), (45, 46), (69, 70), (105, 107), (139, 139), (148, 148), (158, 158), (166, 166), (184, 184), (194, 194), (270, 270), (294, 294), (308, 308), (319, 319), (322, 322), (344, 344), (361, 361), (367, 367), (395, 395), (402, 402), (409, 409), (443, 443), (450, 450), (552, 552), (558, 558), (571, 571), (574, 574), (583, 583), (639, 640), (658, 658), (668, 669), (688, 689), (714, 715), (723, 723), (731, 737), (774, 774), (785, 785), (795, 795), (797, 797), (802, 802), (816, 816), (818, 818), (837, 837), (849, 849), (879, 879), (893, 893), (905, 905), (910, 910), (930, 930), (970, 970), (979, 979), (983, 983), (993, 993), (1029, 1029), (1049, 1049), (1057, 1057), (1074, 1074), (1095, 1095), (1105, 1105), (1118, 1118), (1125, 1125), (1139, 1139)], [(258, 261), (287, 290), (381, 382)], [(327, 339), (346, 346), (353, 353)], [(377, 377), (389, 389), (391, 391)], [(488, 488), (498, 498), (504, 504), (535, 535)], [(995, 997), (999, 999), (1008, 1008), (1039, 1039), (1065, 1065)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7204, coref_recall: 0.2715, coref_f1: 0.3938, mention_recall: 0.4451, batch_loss: 27.2126, loss: 19.7761 ||:  87%|########6 | 297/343 [00:18<00:03, 14.59it/s]predicted_antecedents size torch.Size([1, 19])\n",
            "predicted_antecedents_float size torch.Size([1, 19])\n",
            "probs_logs size: torch.Size([1, 19, 6])\n",
            "probs size: torch.Size([1, 19, 6])\n",
            "probs_flat size: torch.Size([1, 114])\n",
            "gold_antecedent_labels size: torch.Size([1, 19, 6])\n",
            "clusters (batch_clusters) [[[(3, 4), (95, 96), (138, 139), (166, 168), (212, 214)], [(56, 59), (103, 106)], [(117, 119), (132, 132)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 151])\n",
            "predicted_antecedents_float size torch.Size([1, 151])\n",
            "probs_logs size: torch.Size([1, 151, 6])\n",
            "probs size: torch.Size([1, 151, 6])\n",
            "probs_flat size: torch.Size([1, 906])\n",
            "gold_antecedent_labels size: torch.Size([1, 151, 6])\n",
            "clusters (batch_clusters) [[[(72, 72), (83, 83), (86, 86)], [(43, 45), (115, 117)], [(253, 261), (260, 260)], [(234, 244), (276, 276), (280, 280), (300, 300), (1541, 1543), (1565, 1566)], [(280, 294), (288, 288)], [(5, 7), (334, 341), (741, 741), (769, 770), (816, 816), (898, 918), (937, 937), (1669, 1670)], [(565, 565), (575, 575)], [(396, 397), (632, 634)], [(781, 781), (871, 871), (921, 921), (955, 955)], [(1164, 1165), (1168, 1168)], [(1204, 1212), (1240, 1241), (1252, 1252)], [(1270, 1274), (1276, 1276)], [(1358, 1361), (1368, 1370)], [(1406, 1409), (1419, 1420), (1434, 1437), (1439, 1440), (1449, 1449)], [(1506, 1509), (1518, 1518), (1561, 1562), (1586, 1587)], [(1528, 1529), (1535, 1536)], [(1364, 1365), (1590, 1592)], [(1673, 1684), (1686, 1688), (1714, 1715), (1734, 1735), (1793, 1795)], [(1729, 1738), (1762, 1762)], [(1797, 1801), (1808, 1808)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7202, coref_recall: 0.2710, coref_f1: 0.3932, mention_recall: 0.4458, batch_loss: 78.0345, loss: 19.9767 ||:  87%|########7 | 299/343 [00:19<00:03, 11.91it/s]predicted_antecedents size torch.Size([1, 81])\n",
            "predicted_antecedents_float size torch.Size([1, 81])\n",
            "probs_logs size: torch.Size([1, 81, 6])\n",
            "probs size: torch.Size([1, 81, 6])\n",
            "probs_flat size: torch.Size([1, 486])\n",
            "gold_antecedent_labels size: torch.Size([1, 81, 6])\n",
            "clusters (batch_clusters) [[[(332, 332), (342, 342), (387, 394), (504, 504), (585, 586)], [(439, 440), (456, 470), (456, 471), (530, 531), (552, 553), (837, 839), (848, 848), (859, 860)], [(669, 681), (683, 686)], [(724, 727), (743, 744)], [(789, 796), (809, 809)], [(862, 870), (875, 875)], [(890, 891), (905, 905)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(34, 41), (53, 53)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2713, coref_f1: 0.3936, mention_recall: 0.4460, batch_loss: 0.5578, loss: 19.9163 ||:  88%|########7 | 301/343 [00:19<00:03, 12.50it/s] predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(42, 44), (54, 57)], [(16, 18), (71, 81)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 79])\n",
            "predicted_antecedents_float size torch.Size([1, 79])\n",
            "probs_logs size: torch.Size([1, 79, 6])\n",
            "probs size: torch.Size([1, 79, 6])\n",
            "probs_flat size: torch.Size([1, 474])\n",
            "gold_antecedent_labels size: torch.Size([1, 79, 6])\n",
            "clusters (batch_clusters) [[[(5, 5), (19, 19), (320, 320), (477, 477)], [(44, 44), (76, 76), (93, 93)], [(110, 111), (130, 130), (139, 139)], [(212, 212), (219, 219), (222, 222), (238, 238), (244, 244)], [(263, 266), (282, 282), (288, 288)], [(1, 3), (316, 318)], [(333, 336), (340, 340), (352, 352)], [(373, 376), (389, 389)], [(359, 359), (417, 417), (503, 503), (718, 718), (760, 760), (802, 802)], [(442, 443), (459, 459)], [(526, 526), (529, 529), (536, 536)], [(858, 858), (863, 863)], [(912, 912), (922, 922)], [(947, 961), (967, 967)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7192, coref_recall: 0.2713, coref_f1: 0.3934, mention_recall: 0.4461, batch_loss: 47.8615, loss: 19.9645 ||:  88%|########8 | 303/343 [00:19<00:03, 12.61it/s]predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(36, 36), (86, 86), (155, 158)], [(189, 190), (201, 202)], [(121, 133), (231, 234)], [(267, 268), (283, 283)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 2])\n",
            "predicted_antecedents_float size torch.Size([1, 2])\n",
            "probs_logs size: torch.Size([1, 2, 3])\n",
            "probs size: torch.Size([1, 2, 3])\n",
            "probs_flat size: torch.Size([1, 6])\n",
            "gold_antecedent_labels size: torch.Size([1, 2, 3])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 29])\n",
            "predicted_antecedents_float size torch.Size([1, 29])\n",
            "probs_logs size: torch.Size([1, 29, 6])\n",
            "probs size: torch.Size([1, 29, 6])\n",
            "probs_flat size: torch.Size([1, 174])\n",
            "gold_antecedent_labels size: torch.Size([1, 29, 6])\n",
            "clusters (batch_clusters) [[[(0, 4), (43, 46), (62, 65), (96, 97), (219, 222), (278, 280), (299, 301), (323, 329)], [(8, 9), (48, 49), (165, 166), (229, 230), (308, 309)], [(119, 121), (196, 198), (268, 270)], [(209, 211), (214, 214)], [(77, 79), (339, 340)], [(306, 306), (368, 368)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7197, coref_recall: 0.2711, coref_f1: 0.3933, mention_recall: 0.4457, batch_loss: 1.0845, loss: 19.8210 ||:  89%|########9 | 306/343 [00:19<00:02, 14.42it/s] predicted_antecedents size torch.Size([1, 58])\n",
            "predicted_antecedents_float size torch.Size([1, 58])\n",
            "probs_logs size: torch.Size([1, 58, 6])\n",
            "probs size: torch.Size([1, 58, 6])\n",
            "probs_flat size: torch.Size([1, 348])\n",
            "gold_antecedent_labels size: torch.Size([1, 58, 6])\n",
            "clusters (batch_clusters) [[[(0, 0), (11, 11), (14, 14), (21, 21), (52, 52), (76, 76), (85, 85), (87, 87), (102, 102), (111, 111), (135, 135), (137, 137), (139, 139), (150, 150), (187, 187), (205, 205), (209, 209), (222, 222), (225, 225), (230, 230), (238, 238), (243, 243), (250, 250), (318, 318), (324, 324), (329, 329), (341, 341), (352, 352), (395, 395), (403, 403), (406, 406), (412, 412), (424, 424), (434, 434), (437, 437), (450, 450), (461, 461), (474, 474), (477, 477), (483, 483), (496, 496), (506, 506), (513, 513), (521, 521), (534, 534), (539, 539), (546, 546), (554, 554), (561, 561), (628, 628), (642, 642), (654, 654), (669, 669), (676, 676), (706, 706), (721, 721), (726, 726)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 58])\n",
            "predicted_antecedents_float size torch.Size([1, 58])\n",
            "probs_logs size: torch.Size([1, 58, 6])\n",
            "probs size: torch.Size([1, 58, 6])\n",
            "probs_flat size: torch.Size([1, 348])\n",
            "gold_antecedent_labels size: torch.Size([1, 58, 6])\n",
            "clusters (batch_clusters) [[[(13, 17), (91, 96), (151, 151), (157, 157)], [(263, 265), (327, 329)], [(365, 387), (393, 393), (398, 398), (557, 559)], [(476, 477), (479, 479)], [(535, 536), (546, 547)], [(566, 574), (576, 576)], [(514, 517), (629, 632)], [(41, 44), (701, 702)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7203, coref_recall: 0.2719, coref_f1: 0.3941, mention_recall: 0.4461, batch_loss: 9.3035, loss: 19.8126 ||:  90%|########9 | 308/343 [00:19<00:02, 13.94it/s]predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (10, 10), (32, 34)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 9])\n",
            "predicted_antecedents_float size torch.Size([1, 9])\n",
            "probs_logs size: torch.Size([1, 9, 6])\n",
            "probs size: torch.Size([1, 9, 6])\n",
            "probs_flat size: torch.Size([1, 54])\n",
            "gold_antecedent_labels size: torch.Size([1, 9, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (9, 13), (21, 23), (39, 40), (67, 69), (77, 78), (80, 80)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(3, 6), (35, 39)], [(32, 39), (59, 60)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 87])\n",
            "predicted_antecedents_float size torch.Size([1, 87])\n",
            "probs_logs size: torch.Size([1, 87, 6])\n",
            "probs size: torch.Size([1, 87, 6])\n",
            "probs_flat size: torch.Size([1, 522])\n",
            "gold_antecedent_labels size: torch.Size([1, 87, 6])\n",
            "clusters (batch_clusters) [[[(0, 1), (26, 26), (48, 48), (55, 55), (90, 90), (101, 101)], [(130, 131), (142, 142), (146, 146), (158, 158)], [(259, 259), (272, 272), (280, 280)], [(392, 392), (434, 435), (444, 444), (508, 509), (526, 526), (540, 540), (543, 543), (575, 576), (636, 637)], [(439, 439), (480, 480)], [(595, 595), (600, 600), (643, 643)], [(678, 679), (689, 689)], [(732, 732), (761, 761), (770, 770)], [(798, 799), (804, 804)], [(845, 847), (854, 854)], [(865, 871), (897, 897), (912, 912)], [(945, 947), (950, 950)], [(945, 945), (967, 967)], [(1009, 1010), (1036, 1036), (1044, 1044), (1054, 1054), (1060, 1060), (1076, 1076)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7212, coref_recall: 0.2721, coref_f1: 0.3945, mention_recall: 0.4461, batch_loss: 34.3720, loss: 19.6693 ||:  91%|######### | 312/343 [00:19<00:01, 16.16it/s]predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(71, 80), (84, 84), (92, 92), (101, 101), (105, 107), (140, 140)], [(39, 44), (118, 119), (137, 138)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 25])\n",
            "predicted_antecedents_float size torch.Size([1, 25])\n",
            "probs_logs size: torch.Size([1, 25, 6])\n",
            "probs size: torch.Size([1, 25, 6])\n",
            "probs_flat size: torch.Size([1, 150])\n",
            "gold_antecedent_labels size: torch.Size([1, 25, 6])\n",
            "clusters (batch_clusters) [[[(115, 116), (124, 124)], [(204, 205), (237, 237), (242, 243), (250, 251)], [(47, 53), (280, 285), (313, 315)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(48, 49), (60, 60)], [(76, 78), (81, 81)], [(100, 115), (143, 146), (163, 163), (178, 178)], [(326, 331), (347, 351)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7211, coref_recall: 0.2721, coref_f1: 0.3945, mention_recall: 0.4462, batch_loss: 16.8670, loss: 19.6004 ||:  92%|#########1| 315/343 [00:20<00:01, 17.03it/s]predicted_antecedents size torch.Size([1, 15])\n",
            "predicted_antecedents_float size torch.Size([1, 15])\n",
            "probs_logs size: torch.Size([1, 15, 6])\n",
            "probs size: torch.Size([1, 15, 6])\n",
            "probs_flat size: torch.Size([1, 90])\n",
            "gold_antecedent_labels size: torch.Size([1, 15, 6])\n",
            "clusters (batch_clusters) [[[(46, 47), (58, 58), (67, 69)], [(7, 9), (90, 95)], [(188, 188), (192, 192)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 185])\n",
            "predicted_antecedents_float size torch.Size([1, 185])\n",
            "probs_logs size: torch.Size([1, 185, 6])\n",
            "probs size: torch.Size([1, 185, 6])\n",
            "probs_flat size: torch.Size([1, 1110])\n",
            "gold_antecedent_labels size: torch.Size([1, 185, 6])\n",
            "clusters (batch_clusters) [[[(17, 20), (33, 35), (83, 100), (871, 876)], [(27, 29), (117, 119), (213, 215), (234, 236), (670, 672), (706, 708), (899, 900), (928, 929)], [(138, 140), (157, 157), (980, 981), (991, 993), (2173, 2175)], [(166, 171), (185, 185), (185, 186), (186, 186), (308, 310)], [(227, 237), (244, 246), (259, 259), (891, 895), (907, 907), (945, 948)], [(121, 123), (315, 316), (409, 412), (1258, 1260), (1280, 1281)], [(409, 417), (422, 423), (1014, 1014), (1766, 1771), (1799, 1801), (1830, 1830)], [(506, 507), (513, 513)], [(525, 527), (530, 530)], [(74, 75), (536, 538)], [(556, 557), (607, 612)], [(622, 630), (636, 637)], [(649, 650), (653, 653)], [(917, 920), (956, 957)], [(1096, 1100), (1125, 1126)], [(1329, 1329), (1347, 1347), (1382, 1382)], [(1376, 1378), (1393, 1395)], [(1469, 1472), (1480, 1481), (1486, 1486), (1557, 1557), (1565, 1565)], [(1599, 1603), (1604, 1605), (1612, 1614), (1642, 1643), (1656, 1657)], [(1668, 1672), (1675, 1675)], [(1694, 1698), (1724, 1726)], [(1737, 1739), (1758, 1758)], [(1774, 1780), (1793, 1793), (1818, 1819)], [(1850, 1859), (1870, 1870)], [(1894, 1895), (1898, 1898)], [(1917, 1920), (1929, 1930), (1932, 1932), (1951, 1952), (1968, 1968)], [(1951, 1954), (1989, 1990)], [(2070, 2073), (2130, 2131)], [(2124, 2125), (2208, 2209)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7198, coref_recall: 0.2726, coref_f1: 0.3949, mention_recall: 0.4472, batch_loss: 152.0104, loss: 19.9940 ||:  92%|#########2| 317/343 [00:20<00:02, 12.43it/s]predicted_antecedents size torch.Size([1, 60])\n",
            "predicted_antecedents_float size torch.Size([1, 60])\n",
            "probs_logs size: torch.Size([1, 60, 6])\n",
            "probs size: torch.Size([1, 60, 6])\n",
            "probs_flat size: torch.Size([1, 360])\n",
            "gold_antecedent_labels size: torch.Size([1, 60, 6])\n",
            "clusters (batch_clusters) [[[(22, 25), (32, 32), (42, 42)], [(56, 57), (75, 77), (109, 110), (126, 126), (192, 193), (247, 248), (351, 352), (373, 374), (479, 480), (671, 672), (742, 744), (757, 758)], [(169, 171), (178, 178)], [(202, 202), (221, 221)], [(410, 412), (424, 424)], [(439, 444), (465, 465)], [(475, 480), (511, 511), (543, 543), (549, 549)], [(629, 630), (652, 652), (661, 661)], [(729, 729), (734, 734)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 28])\n",
            "predicted_antecedents_float size torch.Size([1, 28])\n",
            "probs_logs size: torch.Size([1, 28, 6])\n",
            "probs size: torch.Size([1, 28, 6])\n",
            "probs_flat size: torch.Size([1, 168])\n",
            "gold_antecedent_labels size: torch.Size([1, 28, 6])\n",
            "clusters (batch_clusters) [[[(56, 57), (78, 78), (194, 195)], [(224, 225), (247, 248)], [(68, 71), (307, 310), (321, 321), (338, 338), (351, 351)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7204, coref_recall: 0.2728, coref_f1: 0.3952, mention_recall: 0.4474, batch_loss: 2.1987, loss: 19.9443 ||:  93%|#########3| 319/343 [00:20<00:01, 12.64it/s]  predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[[(27, 28), (45, 45)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 51])\n",
            "predicted_antecedents_float size torch.Size([1, 51])\n",
            "probs_logs size: torch.Size([1, 51, 6])\n",
            "probs size: torch.Size([1, 51, 6])\n",
            "probs_flat size: torch.Size([1, 306])\n",
            "gold_antecedent_labels size: torch.Size([1, 51, 6])\n",
            "clusters (batch_clusters) [[[(20, 42), (63, 69), (439, 443), (526, 528)], [(92, 93), (106, 106), (110, 110), (372, 374), (384, 384), (604, 605)], [(192, 200), (204, 204), (207, 207)], [(236, 257), (272, 273)], [(17, 18), (335, 336), (339, 339), (492, 493), (514, 514), (531, 531)], [(594, 595), (613, 614)], [(589, 592), (636, 637)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7205, coref_recall: 0.2727, coref_f1: 0.3951, mention_recall: 0.4474, batch_loss: 23.2168, loss: 19.8958 ||:  94%|#########3| 321/343 [00:20<00:01, 13.67it/s]predicted_antecedents size torch.Size([1, 37])\n",
            "predicted_antecedents_float size torch.Size([1, 37])\n",
            "probs_logs size: torch.Size([1, 37, 6])\n",
            "probs size: torch.Size([1, 37, 6])\n",
            "probs_flat size: torch.Size([1, 222])\n",
            "gold_antecedent_labels size: torch.Size([1, 37, 6])\n",
            "clusters (batch_clusters) [[[(38, 43), (84, 85), (144, 145), (226, 227), (230, 230), (251, 252)], [(261, 263), (279, 281), (340, 342)], [(277, 277), (294, 294), (300, 300), (321, 321), (328, 328), (336, 336), (348, 348)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 7])\n",
            "predicted_antecedents_float size torch.Size([1, 7])\n",
            "probs_logs size: torch.Size([1, 7, 6])\n",
            "probs size: torch.Size([1, 7, 6])\n",
            "probs_flat size: torch.Size([1, 42])\n",
            "gold_antecedent_labels size: torch.Size([1, 7, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7206, coref_recall: 0.2723, coref_f1: 0.3947, mention_recall: 0.4473, batch_loss: 0.0018, loss: 19.9168 ||:  94%|#########4| 323/343 [00:20<00:01, 14.87it/s] predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(44, 45), (69, 70), (124, 125), (198, 199), (324, 326), (328, 330), (347, 349), (371, 372), (456, 457), (518, 519)], [(37, 40), (93, 98)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 68])\n",
            "predicted_antecedents_float size torch.Size([1, 68])\n",
            "probs_logs size: torch.Size([1, 68, 6])\n",
            "probs size: torch.Size([1, 68, 6])\n",
            "probs_flat size: torch.Size([1, 408])\n",
            "gold_antecedent_labels size: torch.Size([1, 68, 6])\n",
            "clusters (batch_clusters) [[[(74, 74), (80, 80), (92, 92), (101, 107), (125, 126), (176, 177), (188, 189), (211, 211), (228, 229), (242, 243), (276, 276), (335, 336)], [(68, 68), (96, 97)], [(112, 121), (157, 159), (266, 268), (293, 294), (342, 343)], [(239, 240), (251, 252)], [(283, 283), (298, 298), (304, 304), (322, 322)], [(494, 496), (500, 500), (505, 505)], [(483, 487), (513, 513), (541, 541), (546, 546), (552, 552)], [(247, 247), (584, 584)], [(561, 563), (587, 587), (620, 620)], [(676, 692), (705, 705)], [(742, 743), (757, 757), (760, 760), (824, 824)], [(792, 792), (799, 799)], [(630, 637), (815, 816)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7205, coref_recall: 0.2726, coref_f1: 0.3949, mention_recall: 0.4479, batch_loss: 30.9600, loss: 19.9363 ||:  95%|#########4| 325/343 [00:20<00:01, 13.56it/s]predicted_antecedents size torch.Size([1, 57])\n",
            "predicted_antecedents_float size torch.Size([1, 57])\n",
            "probs_logs size: torch.Size([1, 57, 6])\n",
            "probs size: torch.Size([1, 57, 6])\n",
            "probs_flat size: torch.Size([1, 342])\n",
            "gold_antecedent_labels size: torch.Size([1, 57, 6])\n",
            "clusters (batch_clusters) [[[(8, 22), (40, 43), (166, 170), (172, 172), (176, 176), (201, 202), (209, 209), (287, 288), (295, 295), (312, 312), (482, 482), (488, 488)], [(80, 80), (88, 88), (103, 103), (189, 189), (193, 193), (212, 212), (269, 269), (533, 533), (541, 541)], [(131, 131), (137, 137)], [(435, 435), (439, 439)], [(561, 561), (569, 569), (574, 574), (578, 578), (581, 581), (593, 593), (596, 596), (607, 607), (613, 613), (623, 623), (644, 644)], [(649, 649), (651, 651)], [(662, 667), (675, 675)], [(653, 653), (694, 694), (715, 715)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 17])\n",
            "predicted_antecedents_float size torch.Size([1, 17])\n",
            "probs_logs size: torch.Size([1, 17, 6])\n",
            "probs size: torch.Size([1, 17, 6])\n",
            "probs_flat size: torch.Size([1, 102])\n",
            "gold_antecedent_labels size: torch.Size([1, 17, 6])\n",
            "clusters (batch_clusters) [[[(3, 9), (11, 11)], [(61, 61), (65, 65), (69, 69), (79, 79), (86, 86), (96, 96), (127, 128), (130, 130)], [(92, 92), (105, 105)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 11])\n",
            "predicted_antecedents_float size torch.Size([1, 11])\n",
            "probs_logs size: torch.Size([1, 11, 6])\n",
            "probs size: torch.Size([1, 11, 6])\n",
            "probs_flat size: torch.Size([1, 66])\n",
            "gold_antecedent_labels size: torch.Size([1, 11, 6])\n",
            "clusters (batch_clusters) [[[(135, 137), (144, 144)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7209, coref_recall: 0.2728, coref_f1: 0.3952, mention_recall: 0.4481, batch_loss: 1.1107, loss: 19.8708 ||:  96%|#########5| 328/343 [00:20<00:00, 16.40it/s] predicted_antecedents size torch.Size([1, 67])\n",
            "predicted_antecedents_float size torch.Size([1, 67])\n",
            "probs_logs size: torch.Size([1, 67, 6])\n",
            "probs size: torch.Size([1, 67, 6])\n",
            "probs_flat size: torch.Size([1, 402])\n",
            "gold_antecedent_labels size: torch.Size([1, 67, 6])\n",
            "clusters (batch_clusters) [[[(20, 21), (46, 46), (229, 229), (496, 496)], [(9, 9), (57, 57), (97, 97), (112, 112), (177, 177), (181, 181), (187, 187), (216, 216), (237, 237), (239, 239), (263, 263), (279, 279)], [(185, 185), (201, 201), (213, 213), (219, 219), (247, 247), (252, 252), (254, 254), (329, 329), (334, 334), (364, 364), (370, 370), (383, 383), (388, 388), (395, 395), (399, 399), (409, 409), (419, 419), (435, 435), (440, 440), (448, 448), (457, 457), (474, 474), (476, 476), (492, 492), (498, 498), (540, 540), (554, 554), (567, 567), (572, 572), (664, 664), (675, 675), (684, 684), (687, 687), (690, 690), (703, 703), (710, 710), (727, 727), (738, 738), (744, 744), (753, 753), (764, 764), (792, 792), (798, 798), (820, 820)], [(805, 805), (815, 815)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 57])\n",
            "predicted_antecedents_float size torch.Size([1, 57])\n",
            "probs_logs size: torch.Size([1, 57, 6])\n",
            "probs size: torch.Size([1, 57, 6])\n",
            "probs_flat size: torch.Size([1, 342])\n",
            "gold_antecedent_labels size: torch.Size([1, 57, 6])\n",
            "clusters (batch_clusters) [[[(19, 19), (26, 26), (37, 37), (44, 44), (51, 51), (59, 59), (67, 67), (86, 86), (100, 100), (111, 111), (118, 118), (123, 123), (130, 130), (135, 135), (146, 146), (168, 168), (180, 180), (182, 182), (194, 194), (202, 202), (208, 208), (213, 213), (215, 215), (217, 217), (233, 233), (288, 288), (319, 319), (331, 331), (338, 338), (342, 342), (356, 356), (367, 367), (401, 401), (453, 453), (472, 472), (482, 482), (525, 525), (575, 575), (582, 582), (586, 586), (621, 621), (634, 634), (650, 650), (657, 657)], [(8, 8), (205, 205), (636, 636)], [(290, 290), (314, 314), (418, 418), (444, 444), (468, 468)], [(592, 592), (597, 597), (603, 603)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7211, coref_recall: 0.2732, coref_f1: 0.3957, mention_recall: 0.4481, batch_loss: 22.0311, loss: 19.9198 ||:  96%|#########6| 330/343 [00:21<00:00, 15.90it/s]predicted_antecedents size torch.Size([1, 73])\n",
            "predicted_antecedents_float size torch.Size([1, 73])\n",
            "probs_logs size: torch.Size([1, 73, 6])\n",
            "probs size: torch.Size([1, 73, 6])\n",
            "probs_flat size: torch.Size([1, 438])\n",
            "gold_antecedent_labels size: torch.Size([1, 73, 6])\n",
            "clusters (batch_clusters) [[[(54, 54), (70, 70), (303, 303), (318, 318)], [(38, 48), (78, 80), (93, 95), (124, 124), (195, 195), (204, 205), (222, 223)], [(309, 309), (313, 313), (361, 361), (396, 396), (402, 402), (444, 444), (476, 476), (481, 481), (487, 487), (492, 492)], [(371, 378), (398, 398)], [(309, 310), (438, 438)], [(468, 469), (472, 472), (484, 484), (495, 495), (504, 505), (508, 508), (520, 520), (537, 538), (548, 548), (563, 563), (572, 572), (584, 584), (592, 593), (598, 598), (612, 612), (627, 627), (638, 638), (649, 649), (667, 667), (762, 763), (774, 774), (787, 787), (796, 796), (894, 894), (913, 913)], [(481, 485), (492, 496), (836, 840)], [(695, 697), (702, 702), (707, 707)], [(723, 723), (736, 736), (745, 745), (754, 754), (807, 807), (817, 817), (852, 852)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 30])\n",
            "predicted_antecedents_float size torch.Size([1, 30])\n",
            "probs_logs size: torch.Size([1, 30, 6])\n",
            "probs size: torch.Size([1, 30, 6])\n",
            "probs_flat size: torch.Size([1, 180])\n",
            "gold_antecedent_labels size: torch.Size([1, 30, 6])\n",
            "clusters (batch_clusters) [[[(6, 6), (10, 10), (27, 27), (40, 40), (104, 104), (111, 111), (112, 112), (210, 210), (221, 221), (232, 232), (239, 239), (244, 244), (247, 249), (254, 254), (265, 265), (273, 273), (280, 280)], [(123, 129), (134, 134)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7208, coref_recall: 0.2725, coref_f1: 0.3949, mention_recall: 0.4465, batch_loss: 27.0420, loss: 19.9855 ||:  97%|#########6| 332/343 [00:21<00:00, 15.79it/s]predicted_antecedents size torch.Size([1, 47])\n",
            "predicted_antecedents_float size torch.Size([1, 47])\n",
            "probs_logs size: torch.Size([1, 47, 6])\n",
            "probs size: torch.Size([1, 47, 6])\n",
            "probs_flat size: torch.Size([1, 282])\n",
            "gold_antecedent_labels size: torch.Size([1, 47, 6])\n",
            "clusters (batch_clusters) [[[(96, 106), (110, 110), (136, 136), (209, 223), (222, 223)], [(228, 230), (243, 243), (378, 378), (384, 384), (395, 395)], [(71, 71), (285, 286)], [(252, 260), (341, 342)], [(120, 130), (416, 418)], [(565, 571), (578, 578)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 46])\n",
            "predicted_antecedents_float size torch.Size([1, 46])\n",
            "probs_logs size: torch.Size([1, 46, 6])\n",
            "probs size: torch.Size([1, 46, 6])\n",
            "probs_flat size: torch.Size([1, 276])\n",
            "gold_antecedent_labels size: torch.Size([1, 46, 6])\n",
            "clusters (batch_clusters) [[[(27, 27), (50, 53), (224, 224)], [(158, 158), (165, 165), (184, 184), (214, 214), (222, 222), (238, 238), (278, 278), (303, 303), (310, 310), (337, 337), (344, 344), (356, 356), (368, 368), (379, 379), (417, 417), (425, 425), (451, 451), (464, 464), (472, 472), (482, 482), (489, 489), (499, 499), (533, 533), (539, 539), (548, 548), (558, 558), (560, 560)], [(281, 289), (313, 313)], [(394, 394), (402, 402), (407, 407), (427, 427), (437, 437), (449, 449)], [(569, 571), (574, 574)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7206, coref_recall: 0.2723, coref_f1: 0.3947, mention_recall: 0.4468, batch_loss: 8.4685, loss: 19.9640 ||:  97%|#########7| 334/343 [00:21<00:00, 14.44it/s] predicted_antecedents size torch.Size([1, 52])\n",
            "predicted_antecedents_float size torch.Size([1, 52])\n",
            "probs_logs size: torch.Size([1, 52, 6])\n",
            "probs size: torch.Size([1, 52, 6])\n",
            "probs_flat size: torch.Size([1, 312])\n",
            "gold_antecedent_labels size: torch.Size([1, 52, 6])\n",
            "clusters (batch_clusters) [[[(16, 16), (45, 45), (94, 94), (112, 112), (131, 131), (178, 178), (185, 185), (191, 191), (209, 209), (236, 236), (241, 241), (302, 302), (311, 312), (317, 317), (386, 386), (395, 395), (399, 399), (430, 431), (458, 458), (461, 461), (502, 502), (530, 530), (560, 560), (578, 578), (584, 584), (596, 596), (624, 624), (657, 657)], [(54, 54), (62, 62), (100, 100)], [(89, 89), (98, 98)], [(154, 155), (157, 157), (164, 165)], [(274, 278), (290, 290)], [(182, 182), (392, 392), (475, 475)], [(630, 630), (646, 646)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 5])\n",
            "predicted_antecedents_float size torch.Size([1, 5])\n",
            "probs_logs size: torch.Size([1, 5, 6])\n",
            "probs size: torch.Size([1, 5, 6])\n",
            "probs_flat size: torch.Size([1, 30])\n",
            "gold_antecedent_labels size: torch.Size([1, 5, 6])\n",
            "clusters (batch_clusters) [[]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7209, coref_recall: 0.2718, coref_f1: 0.3942, mention_recall: 0.4461, batch_loss: 0.0120, loss: 19.8942 ||:  98%|#########7| 336/343 [00:21<00:00, 15.68it/s]predicted_antecedents size torch.Size([1, 56])\n",
            "predicted_antecedents_float size torch.Size([1, 56])\n",
            "probs_logs size: torch.Size([1, 56, 6])\n",
            "probs size: torch.Size([1, 56, 6])\n",
            "probs_flat size: torch.Size([1, 336])\n",
            "gold_antecedent_labels size: torch.Size([1, 56, 6])\n",
            "clusters (batch_clusters) [[[(131, 131), (167, 167), (305, 306), (314, 314), (335, 335), (391, 395), (411, 411), (432, 432), (454, 454)], [(183, 188), (199, 199), (204, 204)], [(3, 5), (212, 212)], [(338, 338), (343, 343)], [(353, 364), (366, 366), (379, 379)], [(486, 487), (512, 514), (584, 584)], [(522, 525), (562, 562)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 4])\n",
            "predicted_antecedents_float size torch.Size([1, 4])\n",
            "probs_logs size: torch.Size([1, 4, 5])\n",
            "probs size: torch.Size([1, 4, 5])\n",
            "probs_flat size: torch.Size([1, 20])\n",
            "gold_antecedent_labels size: torch.Size([1, 4, 5])\n",
            "clusters (batch_clusters) [[[(0, 3), (30, 33)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7209, coref_recall: 0.2717, coref_f1: 0.3941, mention_recall: 0.4462, batch_loss: 0.0008, loss: 19.8718 ||:  99%|#########8| 338/343 [00:21<00:00, 16.64it/s]predicted_antecedents size torch.Size([1, 6])\n",
            "predicted_antecedents_float size torch.Size([1, 6])\n",
            "probs_logs size: torch.Size([1, 6, 6])\n",
            "probs size: torch.Size([1, 6, 6])\n",
            "probs_flat size: torch.Size([1, 36])\n",
            "gold_antecedent_labels size: torch.Size([1, 6, 6])\n",
            "clusters (batch_clusters) [[[(16, 20), (29, 31), (51, 51), (58, 58)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 33])\n",
            "predicted_antecedents_float size torch.Size([1, 33])\n",
            "probs_logs size: torch.Size([1, 33, 6])\n",
            "probs size: torch.Size([1, 33, 6])\n",
            "probs_flat size: torch.Size([1, 198])\n",
            "gold_antecedent_labels size: torch.Size([1, 33, 6])\n",
            "clusters (batch_clusters) [[[(25, 25), (75, 75), (80, 80), (87, 87), (96, 96), (102, 102), (111, 111), (146, 146), (213, 213), (222, 222), (224, 224), (229, 229), (236, 236), (243, 243), (260, 260), (269, 269), (285, 285), (297, 297), (324, 324), (326, 326), (342, 342)], [(307, 307), (319, 319), (331, 331)], [(397, 397), (412, 412)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 27])\n",
            "predicted_antecedents_float size torch.Size([1, 27])\n",
            "probs_logs size: torch.Size([1, 27, 6])\n",
            "probs size: torch.Size([1, 27, 6])\n",
            "probs_flat size: torch.Size([1, 162])\n",
            "gold_antecedent_labels size: torch.Size([1, 27, 6])\n",
            "clusters (batch_clusters) [[[(29, 29), (41, 41), (74, 74), (98, 98), (112, 112), (183, 183), (202, 202), (235, 235), (291, 292), (320, 320)], [(291, 295), (306, 306)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7216, coref_recall: 0.2715, coref_f1: 0.3939, mention_recall: 0.4459, batch_loss: 3.6953, loss: 19.7418 ||:  99%|#########9| 341/343 [00:21<00:00, 19.79it/s]predicted_antecedents size torch.Size([1, 16])\n",
            "predicted_antecedents_float size torch.Size([1, 16])\n",
            "probs_logs size: torch.Size([1, 16, 6])\n",
            "probs size: torch.Size([1, 16, 6])\n",
            "probs_flat size: torch.Size([1, 96])\n",
            "gold_antecedent_labels size: torch.Size([1, 16, 6])\n",
            "clusters (batch_clusters) [[[(39, 45), (81, 91), (158, 160)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "predicted_antecedents size torch.Size([1, 10])\n",
            "predicted_antecedents_float size torch.Size([1, 10])\n",
            "probs_logs size: torch.Size([1, 10, 6])\n",
            "probs size: torch.Size([1, 10, 6])\n",
            "probs_flat size: torch.Size([1, 60])\n",
            "gold_antecedent_labels size: torch.Size([1, 10, 6])\n",
            "clusters (batch_clusters) [[[(2, 17), (19, 19), (44, 47)], [(60, 62), (65, 65)], [(78, 82), (98, 100)]]]\n",
            "clusters size (batch_clusters) 1\n",
            "coref_precision: 0.7217, coref_recall: 0.2714, coref_f1: 0.3939, mention_recall: 0.4459, batch_loss: 1.5055, loss: 19.6628 ||: 100%|##########| 343/343 [00:21<00:00, 15.73it/s]\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger -                        Training |  Validation\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - coref_f1           |     0.447  |     0.394\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - coref_precision    |     0.899  |     0.722\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - coref_recall       |     0.298  |     0.271\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - gpu_0_memory_MB    |  8978.047  |       N/A\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - loss               |     2.361  |    19.663\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - mention_recall     |     0.446  |     0.446\n",
            "2021-09-30 14:34:51,265 - INFO - allennlp.training.callbacks.console_logger - worker_0_memory_MB |  6806.969  |       N/A\n",
            "2021-09-30 14:34:54,444 - INFO - allennlp.training.gradient_descent_trainer - Epoch duration: 0:05:35.808318\n",
            "2021-09-30 14:34:57,488 - INFO - allennlp.commands.train - To evaluate on the test set after training, pass the 'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\n",
            "2021-09-30 14:34:57,489 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 3,\n",
            "  \"peak_worker_0_memory_MB\": 6806.96875,\n",
            "  \"peak_gpu_0_memory_MB\": 8978.046875,\n",
            "  \"training_duration\": \"0:55:58.034267\",\n",
            "  \"epoch\": 9,\n",
            "  \"training_coref_precision\": 0.8985807208611286,\n",
            "  \"training_coref_recall\": 0.29833533824035596,\n",
            "  \"training_coref_f1\": 0.4467745794351865,\n",
            "  \"training_mention_recall\": 0.4459914664271278,\n",
            "  \"training_loss\": 2.3612547188166317,\n",
            "  \"training_worker_0_memory_MB\": 6806.96875,\n",
            "  \"training_gpu_0_memory_MB\": 8978.046875,\n",
            "  \"validation_coref_precision\": 0.7216503026091163,\n",
            "  \"validation_coref_recall\": 0.2714271953764123,\n",
            "  \"validation_coref_f1\": 0.3939034451744034,\n",
            "  \"validation_mention_recall\": 0.44588880187940483,\n",
            "  \"validation_loss\": 19.662838598640228,\n",
            "  \"best_validation_coref_precision\": 0.7561439485273431,\n",
            "  \"best_validation_coref_recall\": 0.312317769475945,\n",
            "  \"best_validation_coref_f1\": 0.440726571262468,\n",
            "  \"best_validation_mention_recall\": 0.5003915426781519,\n",
            "  \"best_validation_loss\": 17.302982382528725\n",
            "}\n",
            "2021-09-30 14:34:57,653 - INFO - allennlp.models.archival - archiving weights and vocabulary to /content/coref_model/model.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9f85Wzw0hVr",
        "outputId": "266a6756-fc9c-40d9-a4d1-5db338ceb48c"
      },
      "source": [
        "# training using WinoBias \n",
        "\n",
        "!allennlp train -f /content/coref_spanbert_base_winobias.jsonnet.txt --include-package coref_adv --include-package coref_mine -s /content/coref_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-09-30 12:19:50,197 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2021-09-30 12:19:50,235 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2021-09-30 12:19:50,236 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2021-09-30 12:19:50,236 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2021-09-30 12:19:50,236 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2021-09-30 12:19:50,237 - INFO - allennlp.common.checks - Pytorch version: 1.9.0+cu102\n",
            "2021-09-30 12:19:50,237 - INFO - allennlp.common.params - type = default\n",
            "2021-09-30 12:19:50,237 - INFO - allennlp.common.params - dataset_reader.type = coref_mine\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.type = coref_mine\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-09-30 12:19:50,238 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-30 12:19:50,241 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-30 12:19:50,242 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-30 12:19:54,697 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-30 12:19:54,697 - INFO - allennlp.common.params - dataset_reader.max_sentences = None\n",
            "2021-09-30 12:19:54,697 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-30 12:19:54,697 - INFO - allennlp.common.params - train_data_path = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_anti_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,697 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_anti_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,699 - INFO - allennlp.common.params - model.type = coref_adv\n",
            "2021-09-30 12:19:54,699 - INFO - allennlp.common.params - data_loader.type = ref\n",
            "2021-09-30 12:19:54,701 - INFO - allennlp.common.params - trainer.type = ref\n",
            "2021-09-30 12:19:54,702 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7efd47b47e50>\n",
            "2021-09-30 12:19:54,702 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.type = coref_mine\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.type = coref_mine\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-30 12:19:54,703 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-09-30 12:19:54,704 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-30 12:19:54,705 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-30 12:19:54,706 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - validation_data_path = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_pro_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_pro_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - test_data_path = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type2_anti_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,707 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type2_anti_stereotype.v4_auto_conll\n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-30 12:19:54,708 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - type = text\n",
            "2021-09-30 12:19:54,709 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-30 12:19:54,710 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7efd4fca5690>\n",
            "loading instances: 0it [00:00, ?it/s]2021-09-30 12:19:54,736 - INFO - allennlp.common.file_utils - cache of https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_anti_stereotype.v4_auto_conll is up-to-date\n",
            "loading instances: 396it [00:00, 2042.10it/s]\n",
            "2021-09-30 12:19:54,905 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-30 12:19:54,905 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-30 12:19:54,905 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-30 12:19:54,905 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-30 12:19:54,906 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:54,906 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:54,906 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-30 12:19:54,906 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-30 12:19:54,906 - INFO - allennlp.common.params - type = text\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-30 12:19:54,907 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7efd4fca5690>\n",
            "loading instances: 0it [00:00, ?it/s]2021-09-30 12:19:55,156 - INFO - allennlp.common.file_utils - https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type1_pro_stereotype.v4_auto_conll not found in cache, downloading to /root/.allennlp/cache/3d5d58b0097438a8038bf6f2aa7450d6fff0c2f44a0b3b0058e92c2f700c908e.e71e7f7ff87173b0e80e88c282131846a91b1122aa00bc68211aa0e8d40025b9\n",
            "\n",
            "downloading: 407542B [00:00, 182156334.27B/s]        \n",
            "loading instances: 396it [00:00, 894.66it/s]\n",
            "2021-09-30 12:19:55,350 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-30 12:19:55,351 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-30 12:19:55,351 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-30 12:19:55,351 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-30 12:19:55,351 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:55,352 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-30 12:19:55,352 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-30 12:19:55,352 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - type = text\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-30 12:19:55,353 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-30 12:19:55,354 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7efd4fca5690>\n",
            "loading instances: 0it [00:00, ?it/s]2021-09-30 12:19:55,681 - INFO - allennlp.common.file_utils - https://raw.githubusercontent.com/uclanlp/corefBias/master/WinoBias/wino/data/conll_format/dev_type2_anti_stereotype.v4_auto_conll not found in cache, downloading to /root/.allennlp/cache/a85d890b5e4b22d75d76da268f2cbdfd1718e15618acfeac05fbdb36c6dd989c.7b756c7d971e3f74c6f35afda21bde2f5cef224b95b878bb522924f4aaf3c88f\n",
            "\n",
            "downloading: 416645B [00:00, 191657796.67B/s]        \n",
            "loading instances: 396it [00:00, 526.58it/s]\n",
            "2021-09-30 12:19:56,106 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - min_count = None\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-09-30 12:19:56,107 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 1188it [00:00, 65845.17it/s]\n",
            "2021-09-30 12:19:56,126 - INFO - allennlp.common.params - model.type = coref_adv\n",
            "2021-09-30 12:19:56,126 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-09-30 12:19:56,127 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-09-30 12:19:56,128 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-09-30 12:19:56,129 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-09-30 12:19:56,130 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:56,130 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-30 12:19:56,130 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:56,130 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-09-30 12:19:56,131 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
            "Downloading: 100% 215M/215M [00:05<00:00, 42.0MB/s]\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2021-09-30 12:20:04,313 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-30 12:20:04,314 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-30 12:20:04,314 - INFO - allennlp.common.params - model.context_layer.input_dim = 768\n",
            "2021-09-30 12:20:04,314 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
            "2021-09-30 12:20:04,316 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 2324\n",
            "2021-09-30 12:20:04,317 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
            "2021-09-30 12:20:04,317 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
            "2021-09-30 12:20:04,317 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
            "2021-09-30 12:20:04,317 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,318 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,318 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,318 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
            "2021-09-30 12:20:04,363 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
            "2021-09-30 12:20:04,365 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 6992\n",
            "2021-09-30 12:20:04,365 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
            "2021-09-30 12:20:04,365 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
            "2021-09-30 12:20:04,365 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
            "2021-09-30 12:20:04,365 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,366 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,366 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-30 12:20:04,366 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
            "2021-09-30 12:20:04,460 - INFO - allennlp.common.params - model.feature_size = 20\n",
            "2021-09-30 12:20:04,460 - INFO - allennlp.common.params - model.max_span_width = 30\n",
            "2021-09-30 12:20:04,461 - INFO - allennlp.common.params - model.spans_per_word = 0.08\n",
            "2021-09-30 12:20:04,461 - INFO - allennlp.common.params - model.max_antecedents = 5\n",
            "2021-09-30 12:20:04,461 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
            "2021-09-30 12:20:04,461 - INFO - allennlp.common.params - model.inference_order = 2\n",
            "2021-09-30 12:20:04,461 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2021-09-30 12:20:04,462 - INFO - allennlp.common.params - model.initializer.type = ref\n",
            "2021-09-30 12:20:04,464 - INFO - allennlp.common.params - type = .*_span_updating_gated_sum.*weight\n",
            "2021-09-30 12:20:04,465 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,465 - INFO - allennlp.common.params - model.initializer.regexes.0.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,465 - INFO - allennlp.common.params - model.initializer.regexes.0.1.gain = 1.0\n",
            "2021-09-30 12:20:04,465 - INFO - allennlp.common.params - type = .*linear_layers.*weight\n",
            "2021-09-30 12:20:04,465 - INFO - allennlp.common.params - model.initializer.regexes.1.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,466 - INFO - allennlp.common.params - model.initializer.regexes.1.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,466 - INFO - allennlp.common.params - model.initializer.regexes.1.1.gain = 1.0\n",
            "2021-09-30 12:20:04,466 - INFO - allennlp.common.params - type = .*scorer.*weight\n",
            "2021-09-30 12:20:04,466 - INFO - allennlp.common.params - model.initializer.regexes.2.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,466 - INFO - allennlp.common.params - model.initializer.regexes.2.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,467 - INFO - allennlp.common.params - model.initializer.regexes.2.1.gain = 1.0\n",
            "2021-09-30 12:20:04,467 - INFO - allennlp.common.params - type = _distance_embedding.weight\n",
            "2021-09-30 12:20:04,467 - INFO - allennlp.common.params - model.initializer.regexes.3.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,468 - INFO - allennlp.common.params - model.initializer.regexes.3.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,468 - INFO - allennlp.common.params - model.initializer.regexes.3.1.gain = 1.0\n",
            "2021-09-30 12:20:04,468 - INFO - allennlp.common.params - type = _span_width_embedding.weight\n",
            "2021-09-30 12:20:04,469 - INFO - allennlp.common.params - model.initializer.regexes.4.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,469 - INFO - allennlp.common.params - model.initializer.regexes.4.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,469 - INFO - allennlp.common.params - model.initializer.regexes.4.1.gain = 1.0\n",
            "2021-09-30 12:20:04,469 - INFO - allennlp.common.params - type = _context_layer._module.weight_ih.*\n",
            "2021-09-30 12:20:04,470 - INFO - allennlp.common.params - model.initializer.regexes.5.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,470 - INFO - allennlp.common.params - model.initializer.regexes.5.1.type = xavier_normal\n",
            "2021-09-30 12:20:04,470 - INFO - allennlp.common.params - model.initializer.regexes.5.1.gain = 1.0\n",
            "2021-09-30 12:20:04,470 - INFO - allennlp.common.params - type = _context_layer._module.weight_hh.*\n",
            "2021-09-30 12:20:04,471 - INFO - allennlp.common.params - model.initializer.regexes.6.1.type = orthogonal\n",
            "2021-09-30 12:20:04,471 - INFO - allennlp.common.params - model.initializer.regexes.6.1.type = orthogonal\n",
            "2021-09-30 12:20:04,471 - INFO - allennlp.common.params - model.initializer.regexes.6.1.gain = 1.0\n",
            "2021-09-30 12:20:04,471 - INFO - allennlp.common.params - model.initializer.prevent_regexes = None\n",
            "2021-09-30 12:20:04,512 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-09-30 12:20:04,523 - INFO - allennlp.nn.initializers - Initializing _mention_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
            "2021-09-30 12:20:04,546 - INFO - allennlp.nn.initializers - Initializing _mention_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
            "2021-09-30 12:20:04,561 - INFO - allennlp.nn.initializers - Initializing _mention_scorer._module.weight using .*scorer.*weight initializer\n",
            "2021-09-30 12:20:04,561 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.0.weight using .*linear_layers.*weight initializer\n",
            "2021-09-30 12:20:04,629 - INFO - allennlp.nn.initializers - Initializing _antecedent_feedforward._module._linear_layers.1.weight using .*linear_layers.*weight initializer\n",
            "2021-09-30 12:20:04,644 - INFO - allennlp.nn.initializers - Initializing _antecedent_scorer._module.weight using .*scorer.*weight initializer\n",
            "2021-09-30 12:20:04,644 - INFO - allennlp.nn.initializers - Initializing _endpoint_span_extractor._span_width_embedding.weight using _span_width_embedding.weight initializer\n",
            "2021-09-30 12:20:04,645 - INFO - allennlp.nn.initializers - Initializing _distance_embedding.weight using _distance_embedding.weight initializer\n",
            "2021-09-30 12:20:04,645 - INFO - allennlp.nn.initializers - Initializing _coarse2fine_scorer.weight using .*scorer.*weight initializer\n",
            "2021-09-30 12:20:04,679 - INFO - allennlp.nn.initializers - Initializing _span_updating_gated_sum._gate.weight using .*_span_updating_gated_sum.*weight initializer\n",
            "2021-09-30 12:20:04,680 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
            "2021-09-30 12:20:04,680 - WARNING - allennlp.nn.initializers - Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
            "2021-09-30 12:20:04,680 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-09-30 12:20:04,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-09-30 12:20:04,682 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-09-30 12:20:04,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-09-30 12:20:04,684 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-09-30 12:20:04,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-09-30 12:20:04,745 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,746 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,747 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,748 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,749 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-09-30 12:20:04,750 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,751 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-09-30 12:20:04,752 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,753 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-09-30 12:20:04,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-09-30 12:20:04,755 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-09-30 12:20:04,755 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-09-30 12:20:04,755 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-09-30 12:20:04,755 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-09-30 12:20:04,849 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-09-30 12:20:04,849 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-09-30 12:20:06,201 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2021-09-30 12:20:06,201 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - trainer.patience = 10\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - trainer.validation_metric = +coref_f1\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-09-30 12:20:06,202 - INFO - allennlp.common.params - type = +coref_f1\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.num_epochs = 10\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.grad_norm = False\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2021-09-30 12:20:06,203 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2021-09-30 12:20:06,204 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-09-30 12:20:06,204 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-09-30 12:20:06,204 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2021-09-30 12:20:06,204 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2021-09-30 12:20:06,204 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7efd47b8f2d0>\n",
            "2021-09-30 12:20:06,205 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2021-09-30 12:20:06,205 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2021-09-30 12:20:06,205 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2021-09-30 12:20:06,205 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2021-09-30 12:20:13,507 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-09-30 12:20:13,508 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.0003\n",
            "2021-09-30 12:20:13,508 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2021-09-30 12:20:13,508 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2021-09-30 12:20:13,508 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0\n",
            "2021-09-30 12:20:13,509 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n",
            "2021-09-30 12:20:13,509 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2021-09-30 12:20:13,509 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias'], {'lr': 1e-05}\n",
            "2021-09-30 12:20:13,584 - INFO - allennlp.training.optimizers - Group 1: ['_mention_scorer._module.bias', '_attentive_span_extractor._global_attention._module.bias', '_coarse2fine_scorer.weight', '_antecedent_feedforward._module._linear_layers.1.bias', '_antecedent_feedforward._module._linear_layers.0.weight', '_mention_scorer._module.weight', '_coarse2fine_scorer.bias', '_antecedent_scorer._module.bias', '_span_updating_gated_sum._gate.weight', '_mention_feedforward._module._linear_layers.1.bias', '_endpoint_span_extractor._span_width_embedding.weight', '_mention_feedforward._module._linear_layers.1.weight', '_antecedent_feedforward._module._linear_layers.1.weight', '_span_updating_gated_sum._gate.bias', '_antecedent_scorer._module.weight', '_mention_feedforward._module._linear_layers.0.weight', '_mention_feedforward._module._linear_layers.0.bias', '_attentive_span_extractor._global_attention._module.weight', '_antecedent_feedforward._module._linear_layers.0.bias', '_distance_embedding.weight'], {}\n",
            "2021-09-30 12:20:13,584 - INFO - allennlp.training.optimizers - Number of trainable parameters: 132202792\n",
            "2021-09-30 12:20:13,585 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2021-09-30 12:20:13,586 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,587 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-09-30 12:20:13,588 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,589 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,590 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-09-30 12:20:13,591 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-09-30 12:20:13,591 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-09-30 12:20:13,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-09-30 12:20:13,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-09-30 12:20:13,686 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-09-30 12:20:13,687 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,688 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-09-30 12:20:13,689 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,690 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,691 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,692 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-09-30 12:20:13,693 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,694 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-09-30 12:20:13,695 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_feedforward._module._linear_layers.0.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_feedforward._module._linear_layers.1.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_scorer._module.weight\n",
            "2021-09-30 12:20:13,696 - INFO - allennlp.common.util - _mention_scorer._module.bias\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_scorer._module.weight\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _antecedent_scorer._module.bias\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _attentive_span_extractor._global_attention._module.weight\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _attentive_span_extractor._global_attention._module.bias\n",
            "2021-09-30 12:20:13,697 - INFO - allennlp.common.util - _distance_embedding.weight\n",
            "2021-09-30 12:20:13,698 - INFO - allennlp.common.util - _coarse2fine_scorer.weight\n",
            "2021-09-30 12:20:13,698 - INFO - allennlp.common.util - _coarse2fine_scorer.bias\n",
            "2021-09-30 12:20:13,698 - INFO - allennlp.common.util - _span_updating_gated_sum._gate.weight\n",
            "2021-09-30 12:20:13,698 - INFO - allennlp.common.util - _span_updating_gated_sum._gate.bias\n",
            "2021-09-30 12:20:13,698 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - type = default\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - save_completed_epochs = True\n",
            "2021-09-30 12:20:13,699 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
            "2021-09-30 12:20:13,700 - INFO - allennlp.common.params - save_every_num_batches = None\n",
            "2021-09-30 12:20:13,700 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
            "2021-09-30 12:20:13,700 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
            "2021-09-30 12:20:13,702 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2021-09-30 12:20:13,702 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/9\n",
            "2021-09-30 12:20:13,702 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 4.6G\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  FutureWarning)\n",
            "2021-09-30 12:20:13,703 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 505M\n",
            "2021-09-30 12:20:13,704 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "  0%|          | 0/396 [00:00<?, ?it/s]\n",
            "2021-09-30 12:20:13,837 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/__main__.py\", line 46, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/__init__.py\", line 122, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 121, in train_model_from_args\n",
            "    file_friendly_logging=args.file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 187, in train_model_from_file\n",
            "    return_model=return_model,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 260, in train_model\n",
            "    file_friendly_logging=file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 504, in _train_worker\n",
            "    metrics = train_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 577, in run\n",
            "    return self.trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 750, in train\n",
            "    metrics, epoch = self._try_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 773, in _try_train\n",
            "    train_metrics = self._train_epoch(epoch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 490, in _train_epoch\n",
            "    batch_outputs = self.batch_outputs(batch, for_training=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 383, in batch_outputs\n",
            "    output_dict = self._pytorch_model(**batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1071, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/content/coref_adv.py\", line 231, in forward\n",
            "    span_mention_scores, span_mask, num_spans_to_keep\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/nn/util.py\", line 2043, in masked_topk\n",
            "    fill_value, _ = top_indices.max(dim=1, keepdim=True)\n",
            "IndexError: max(): Expected reduction dim 1 to have non-zero size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJIe9D1U6Nx"
      },
      "source": [
        "### **3.2 Training coreference model with adversary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE7SQ-VmJIpv",
        "outputId": "464e1f89-92e6-4d13-9414-11feab99d3b8"
      },
      "source": [
        "# loading the trained model \n",
        "!ls \"/content/gdrive/MyDrive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'ALS - AllenNLP.ipynb'\n",
            "'ALS - BERT Attention Head Weights.ipynb'\n",
            "'ALS - Fine-tuning BERT.ipynb'\n",
            "'ALS - OntoNotes BERT.ipynb'\n",
            " anti_stereotyped_type1.txt.dev\n",
            " BMA_model.tar.gz\n",
            " conll_format\n",
            " conll-formatted-ontonotes-5.0\n",
            " coref_adv.py\n",
            " coref_model.tar.gz\n",
            " coref_spanbert_base_mine.jsonnet.txt\n",
            " coref_spanbert_large_mine.jsonnet.txt\n",
            " dev2.gdoc\n",
            " dev2.txt\n",
            " dev.gdoc\n",
            " dev.txt\n",
            " dev.v4_gold_conll\n",
            "'FromOnline: External data: Local Files, Drive, Sheets, and Cloud Storage'\n",
            " pro_stereotyped_type1.txt.dev\n",
            " __pycache__\n",
            " test.v4_gold_conll\n",
            " train.v4_gold_conll\n",
            " Untitled0.ipynb\n",
            " WinoBias.v4_auto_conll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGzcvqY3MsiJ"
      },
      "source": [
        "# coref_model.tar.gz\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/coref_model.tar.gz\" \"/content/coref_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqMInAvWyTpp",
        "outputId": "afb7781b-a1df-460d-91bb-ab12a19c2c46"
      },
      "source": [
        "!allennlp train adversarial_coref_spanbert.jsonnet.txt --include-package coref_adv -s /content/AdversarialBiasMitigator2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-09-29 19:29:30,227 - INFO - allennlp.common.plugins - Plugin allennlp_models available\n",
            "2021-09-29 19:29:30,261 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2021-09-29 19:29:30,262 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2021-09-29 19:29:30,262 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2021-09-29 19:29:30,262 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2021-09-29 19:29:30,263 - INFO - allennlp.common.checks - Pytorch version: 1.9.0+cu102\n",
            "2021-09-29 19:29:30,263 - INFO - allennlp.common.params - type = default\n",
            "2021-09-29 19:29:30,263 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-09-29 19:29:30,264 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-09-29 19:29:30,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:29:30,266 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-29 19:29:30,267 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-29 19:29:34,656 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-29 19:29:34,656 - INFO - allennlp.common.params - dataset_reader.max_sentences = None\n",
            "2021-09-29 19:29:34,656 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-29 19:29:34,657 - INFO - allennlp.common.params - train_data_path = /content/train\n",
            "2021-09-29 19:29:34,657 - INFO - allennlp.common.params - type = /content/train\n",
            "2021-09-29 19:29:34,657 - INFO - allennlp.common.params - model.type = adversarial_bias_mitigator\n",
            "2021-09-29 19:29:34,657 - INFO - allennlp.common.params - data_loader.type = ref\n",
            "2021-09-29 19:29:34,659 - INFO - allennlp.common.params - trainer.type = ref\n",
            "2021-09-29 19:29:34,660 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f8dc10e8890>\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-29 19:29:34,661 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-09-29 19:29:34,662 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-29 19:29:34,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-29 19:29:34,664 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - validation_data_path = /content/dev\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - type = /content/dev\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - test_data_path = None\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2021-09-29 19:29:34,665 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-29 19:29:34,666 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - type = text\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-29 19:29:34,667 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-29 19:29:34,668 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-29 19:29:34,668 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-29 19:29:34,668 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f8dc92c7150>\n",
            "loading instances: 2802it [01:19, 35.15it/s]\n",
            "2021-09-29 19:30:54,376 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-09-29 19:30:54,376 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-09-29 19:30:54,376 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-09-29 19:30:54,376 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-09-29 19:30:54,376 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 1\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = ['text']\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - type = text\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-09-29 19:30:54,377 - INFO - allennlp.common.params - data_loader.batch_sampler.shuffle = True\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2021-09-29 19:30:54,378 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7f8dc92c7150>\n",
            "loading instances: 343it [00:11, 29.69it/s]\n",
            "2021-09-29 19:31:05,932 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-09-29 19:31:05,932 - INFO - allennlp.common.params - min_count = None\n",
            "2021-09-29 19:31:05,932 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-09-29 19:31:05,932 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-09-29 19:31:05,933 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 3145it [00:02, 1484.18it/s]\n",
            "2021-09-29 19:31:08,053 - INFO - allennlp.common.params - model.type = adversarial_bias_mitigator\n",
            "2021-09-29 19:31:08,053 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-09-29 19:31:08,053 - INFO - allennlp.common.params - model.predictor._pretrained.archive_file = /content/coref_model2/model.tar.gz\n",
            "2021-09-29 19:31:08,053 - INFO - allennlp.common.params - model.predictor._pretrained.module_path = \n",
            "2021-09-29 19:31:08,053 - INFO - allennlp.common.params - model.predictor._pretrained.freeze = False\n",
            "2021-09-29 19:31:08,054 - INFO - allennlp.models.archival - loading archive file /content/coref_model2/model.tar.gz\n",
            "2021-09-29 19:31:08,054 - INFO - allennlp.models.archival - extracting archive file /content/coref_model2/model.tar.gz to temp dir /tmp/tmpa1oi92zc\n",
            "2021-09-29 19:31:12,525 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-09-29 19:31:12,526 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-09-29 19:31:12,526 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-29 19:31:12,526 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-29 19:31:12,526 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-09-29 19:31:12,526 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-09-29 19:31:12,528 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,528 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,528 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-29 19:31:12,529 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,529 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,529 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-29 19:31:12,529 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-29 19:31:12,529 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-29 19:31:12,530 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-29 19:31:12,530 - INFO - allennlp.common.params - dataset_reader.max_sentences = 20\n",
            "2021-09-29 19:31:12,530 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-09-29 19:31:12,531 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-09-29 19:31:12,532 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,532 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-09-29 19:31:12,533 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-09-29 19:31:12,534 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-09-29 19:31:12,534 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-09-29 19:31:12,534 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-09-29 19:31:12,535 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-09-29 19:31:12,535 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpa1oi92zc/vocabulary.\n",
            "2021-09-29 19:31:12,535 - INFO - allennlp.common.params - model.type = coref_adv\n",
            "2021-09-29 19:31:12,535 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-09-29 19:31:12,535 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2021-09-29 19:31:12,536 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-09-29 19:31:12,537 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-09-29 19:31:12,537 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-09-29 19:31:12,539 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-09-29 19:31:12,540 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
            "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2021-09-29 19:31:15,081 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-29 19:31:15,081 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-09-29 19:31:15,081 - INFO - allennlp.common.params - model.context_layer.input_dim = 768\n",
            "2021-09-29 19:31:15,082 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
            "2021-09-29 19:31:15,083 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 2324\n",
            "2021-09-29 19:31:15,084 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
            "2021-09-29 19:31:15,084 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
            "2021-09-29 19:31:15,084 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
            "2021-09-29 19:31:15,084 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,084 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,085 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,085 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
            "2021-09-29 19:31:15,120 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
            "2021-09-29 19:31:15,121 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 6992\n",
            "2021-09-29 19:31:15,121 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - type = relu\n",
            "2021-09-29 19:31:15,122 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
            "2021-09-29 19:31:15,201 - INFO - allennlp.common.params - model.feature_size = 20\n",
            "2021-09-29 19:31:15,201 - INFO - allennlp.common.params - model.max_span_width = 30\n",
            "2021-09-29 19:31:15,201 - INFO - allennlp.common.params - model.spans_per_word = 0.08\n",
            "2021-09-29 19:31:15,201 - INFO - allennlp.common.params - model.max_antecedents = 5\n",
            "2021-09-29 19:31:15,201 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
            "2021-09-29 19:31:15,202 - INFO - allennlp.common.params - model.inference_order = 2\n",
            "2021-09-29 19:31:15,202 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2021-09-29 19:31:15,202 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f8da58e4310>\n",
            "2021-09-29 19:31:15,242 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _distance_embedding.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
            "2021-09-29 19:31:15,243 - INFO - allennlp.nn.initializers -    _mention_scorer._module.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,244 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,245 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,246 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,324 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,324 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,325 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-09-29 19:31:15,326 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,327 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-09-29 19:31:15,328 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-09-29 19:31:15,329 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-09-29 19:31:15,332 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-09-29 19:31:15,332 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-09-29 19:31:15,700 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpa1oi92zc\n",
            "2021-09-29 19:31:15,754 - INFO - allennlp.common.params - model.adversary.type = feedforward_regression_adversary\n",
            "2021-09-29 19:31:15,755 - INFO - allennlp.common.params - model.adversary.type = feedforward_regression_adversary\n",
            "2021-09-29 19:31:15,755 - INFO - allennlp.common.params - model.adversary.regularizer = None\n",
            "2021-09-29 19:31:15,755 - INFO - allennlp.common.params - model.adversary.feedforward.type = ref\n",
            "2021-09-29 19:31:15,757 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/params.py\", line 238, in pop\n",
            "    value = self.params.pop(key)\n",
            "KeyError: 'input_dim'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/__main__.py\", line 46, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/__init__.py\", line 122, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 121, in train_model_from_args\n",
            "    file_friendly_logging=args.file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 187, in train_model_from_file\n",
            "    return_model=return_model,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 260, in train_model\n",
            "    file_friendly_logging=file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 494, in _train_worker\n",
            "    ddp_accelerator=ddp_accelerator,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 656, in from_params\n",
            "    **extras,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 686, in from_params\n",
            "    return constructor_to_call(**kwargs)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 767, in from_partial_objects\n",
            "    vocab=vocabulary_, serialization_dir=serialization_dir, ddp_accelerator=ddp_accelerator\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/lazy.py\", line 82, in construct\n",
            "    return self.constructor(**contructor_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/lazy.py\", line 68, in constructor_to_use\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 656, in from_params\n",
            "    **extras,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 684, in from_params\n",
            "    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 208, in create_kwargs\n",
            "    cls.__name__, param_name, annotation, param.default, params, **extras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 315, in pop_and_construct_arg\n",
            "    return construct_arg(class_name, name, popped_params, annotation, default, **extras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 374, in construct_arg\n",
            "    result = annotation.from_params(params=popped_params, **subextras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 656, in from_params\n",
            "    **extras,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 684, in from_params\n",
            "    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 208, in create_kwargs\n",
            "    cls.__name__, param_name, annotation, param.default, params, **extras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 315, in pop_and_construct_arg\n",
            "    return construct_arg(class_name, name, popped_params, annotation, default, **extras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 374, in construct_arg\n",
            "    result = annotation.from_params(params=popped_params, **subextras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 684, in from_params\n",
            "    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 208, in create_kwargs\n",
            "    cls.__name__, param_name, annotation, param.default, params, **extras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/from_params.py\", line 311, in pop_and_construct_arg\n",
            "    popped_params = params.pop(name, default) if default != _NO_DEFAULT else params.pop(name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/common/params.py\", line 243, in pop\n",
            "    raise ConfigurationError(msg)\n",
            "allennlp.common.checks.ConfigurationError: key \"input_dim\" is required at location \"model.adversary.feedforward.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDSTtUEnmbHS"
      },
      "source": [
        "### **Training MLM with Adversary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oBvR6AbHmdvv",
        "outputId": "8e42ec0b-3781-48b9-c768-b5c7dc090c58"
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install allennlp-models\n",
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.7.0-py3-none-any.whl (738 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 245 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 266 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 286 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 296 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 307 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 317 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 327 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 337 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 358 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 368 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 378 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 389 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 399 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 409 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 419 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 430 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 450 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 460 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 471 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 481 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 491 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 501 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 512 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 522 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 532 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 542 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 552 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 563 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 573 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 583 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 593 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 604 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 614 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 624 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 634 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 645 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 655 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 665 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 675 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 686 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 696 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 706 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 716 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 727 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 737 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 738 kB 12.4 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting huggingface-hub>=0.0.8\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu111)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 80.0 MB/s \n",
            "\u001b[?25hCollecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 90.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.10.0)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting google-cloud-storage<1.43.0,>=1.38.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 88.9 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 103.6 MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 89.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.10.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu111)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.18.57-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 83.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.4-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Collecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.0 MB/s \n",
            "\u001b[?25hCollecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.22.0,>=1.21.57\n",
            "  Downloading botocore-1.21.57-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 62.0 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 93.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.57->boto3<2.0,>=1.14->allennlp) (2.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 85.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.1 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 84.3 MB/s \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.0.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting google-api-core<3.0dev,>=1.29.0\n",
            "  Downloading google_api_core-2.1.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.35.0)\n",
            "Collecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.1.0-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (1.53.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (4.7.2)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.13)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 82.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Collecting transformers<4.10,>=4.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 64.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 60.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 57.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 54.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 58.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp) (2019.12.20)\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 52.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 54.1 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 88.9 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 92.8 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 89.6 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 89.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 128 kB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 53.5 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 73.9 MB/s \n",
            "\u001b[?25hCollecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.7 MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading portend-3.0.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.3.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.5.1-py3-none-any.whl (8.1 kB)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=6f9b7d594b82ff512f1bd14d5e01c15733805a372eea7006e7cb22c7c37cd84d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=a29d161393bcabdb228a41e59b4e72889222e18353f531949e1e153a87b9b423\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=9636da76dc43d8efa3a42edc0254919ce8942c61728bf3a38416561b06253701\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388661 sha256=19cef93f9e91e742b3678351f0cdaef132fb622de211a0f9acccb82b3640a678\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=1bfa53f6916cb686d3b531dcd6b7d3e7267025a383f9e66192d8805fd719a26e\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=815c7c7231c7fc7ebe5fe992e19c086c31c83a0cc3fb93b3d5be44d28880baa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=b9ac71c81ded0a51a74770f50fdb4128890912b51927a769263b39de7e0ecb13\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=694b3f0196234c6049cafbb9b13f758287164f4c4890644645183ccaa60f3970\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=927e7700960b94249ddd9e611d9f4c43721bf82530f12b34a01429e318cff36c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=6df0f4a8cb4996f09926adc66a63dc5e65cd9685573753345bd767e0f629e9e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=d26503ce28dc47c59bd3b25b70371b568ace2197f0b7cb683582b72f8178b980\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: jaraco.functools, urllib3, tempora, multidict, jaraco.text, jaraco.classes, zc.lockfile, yarl, smmap, sgmllib3k, portend, jmespath, jaraco.collections, cryptography, cheroot, async-timeout, tokenizers, sacremoses, python-docx, pdfminer.six, google-crc32c, google-api-core, gitdb, fsspec, filelock, feedparser, cherrypy, botocore, backports.csv, aiohttp, yaspin, xxhash, transformers, subprocess32, shortuuid, sentry-sdk, s3transfer, patternfork-nosql, pathtools, munch, iso-639, huggingface-hub, google-resumable-media, google-cloud-core, GitPython, docker-pycreds, configparser, wandb, tensorboardX, sqlitedict, sentencepiece, overrides, jsonnet, google-cloud-storage, fairscale, datasets, checklist, boto3, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 1.26.3\n",
            "    Uninstalling google-api-core-1.26.3:\n",
            "      Successfully uninstalled google-api-core-1.26.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.3.0\n",
            "    Uninstalling filelock-3.3.0:\n",
            "      Successfully uninstalled filelock-3.3.0\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.0.3 which is incompatible.\n",
            "google-cloud-bigquery-storage 1.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires google-api-core<2dev,>=1.21.0, but you have google-api-core 2.1.0 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.1.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 allennlp-2.7.0 async-timeout-3.0.1 backports.csv-1.0.7 base58-2.1.0 boto3-1.18.57 botocore-1.21.57 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.0.2 cryptography-35.0.0 datasets-1.12.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 filelock-3.0.12 fsspec-2021.10.0 gitdb-4.0.7 google-api-core-2.1.0 google-cloud-core-2.1.0 google-cloud-storage-1.42.3 google-crc32c-1.3.0 google-resumable-media-2.0.3 huggingface-hub-0.0.19 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.3.0 jaraco.text-3.5.1 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.2.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20201018 portend-3.0.0 python-docx-0.8.11 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.3 sgmllib3k-1.0.0 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.2 tensorboardX-2.4 tokenizers-0.10.3 transformers-4.5.1 urllib3-1.25.11 wandb-0.12.4 xxhash-2.0.2 yarl-1.7.0 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting allennlp-models\n",
            "  Downloading allennlp_models-2.7.0-py3-none-any.whl (461 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 36.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 42.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 45.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 92 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 122 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 133 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 153 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 163 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 184 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 194 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 204 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 215 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 225 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 235 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 245 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 256 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 266 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 276 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 286 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 296 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 307 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 317 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 327 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 337 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 348 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 358 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 368 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 378 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 389 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 399 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 409 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 419 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 430 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 440 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 450 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 460 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 461 kB 13.0 MB/s \n",
            "\u001b[?25hCollecting conllu==4.4.1\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (3.2.5)\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: allennlp<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (2.7.0)\n",
            "Requirement already satisfied: torch<1.10.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models) (1.9.0+cu111)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (8.10.0)\n",
            "Requirement already satisfied: transformers<4.10,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.5.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.19)\n",
            "Requirement already satisfied: google-cloud-storage<1.43.0,>=1.38.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.42.3)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0+cu111)\n",
            "Requirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.12.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (4.62.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.3.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.1.96)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.0.11)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.1.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.99)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.19.5)\n",
            "Requirement already satisfied: fairscale==0.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: datasets<2.0,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.12.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.18.57)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.7/dist-packages (from allennlp<2.8,>=2.7.0->allennlp-models) (1.7.0)\n",
            "Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.6)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (7.6.5)\n",
            "Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.5)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.5.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.57 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.21.57)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.57->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.57->boto3<2.0,>=1.14->allennlp<2.8,>=2.7.0->allennlp-models) (2.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2021.10.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.post0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.35.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.3)\n",
            "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp<2.8,>=2.7.0->allennlp-models) (3.13)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.18)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (5.1.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<1.43.0,>=1.38.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.11.0,>=0.8.1->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (0.0.46)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.0.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (3.1.24)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (5.4.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.3)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.1.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp<2.8,>=2.7.0->allennlp-models) (4.0.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.7.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (21.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets<2.0,>=1.2.1->allennlp<2.8,>=2.7.0->allennlp-models) (2018.9)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.2.6)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.8.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (0.16.0)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (18.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.6.3)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (6.0.8)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (20201018)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.7)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.0.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.0)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.4.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (8.5.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.3.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (4.1.2)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (3.5.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (35.0.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (2.20)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp<2.8,>=2.7.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp<2.8,>=2.7.0->allennlp-models) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.10,>=4.1->allennlp<2.8,>=2.7.0->allennlp-models) (1.0.1)\n",
            "Building wheels for collected packages: word2number, ftfy\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=24db6aa9e792d0d5865db58065b32bd40c8d3a8fd1f14d876d41abfe09cf2110\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=5245b0500e5c263dbf1253cd9c35c21f5faddc5dd877874a42ab75be2142d2f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built word2number ftfy\n",
            "Installing collected packages: word2number, py-rouge, ftfy, conllu, allennlp-models\n",
            "Successfully installed allennlp-models-2.7.0 conllu-4.4.1 ftfy-6.0.3 py-rouge-1.1 word2number-1.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc0kjoeDNnt4",
        "outputId": "ac7d03f3-8043-49b3-eead-63c7d411e60d"
      },
      "source": [
        "# training mlm using my own script \n",
        "\n",
        "!allennlp train -f mlm.jsonnet.txt --include-package masked_language_modelling_ALS --include-package masked_language_model_ALS -s /content/MLM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], if, the, law, could, make, people, perfect, those, sacrifices, would, have, already,\n",
            "\t\tstopped, ., [SEP], they, would, already, be, clean, from, their, sins, and, they, would, not, still,\n",
            "\t\tfeel, guilty, ., [SEP], but, that, ', s, not, what, happens, ., [SEP], their, sacrifices, make,\n",
            "\t\tthem, remember, their, sins, every, year, because, it, is, not, possible, for, the, blood, of, bull,\n",
            "\t\t##s, and, goats, to, take, away, sins, ., [SEP], so, when, E, ##7, ##23, ##2, came, into, the,\n",
            "\t\tworld, [MASK], said, `, `, you, do, n, ', t, want, sacrifices, and, offerings, but, you, have,\n",
            "\t\tprepared, a, body, for, me, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 80. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then i said ' here i am god . [SEP] it is written about me in the book of the law . [SEP] i have come to do what you want . ' '' [SEP] E7232 first said `` you do n't want sacrifices and offerings . [SEP] you are not pleased with animals killed and burned or with sacrifices to take away sin . '' [SEP] -lrb- these are all sacrifices that the law commands . -rrb- [SEP] then [MASK] said `` here i am god . [SEP] [[CLS], then, i, said, ', here, i, am, god, ., [SEP], it, is, written, about, me, in, the, book, of, the, law, ., [SEP], i, have, come, to, do, what, you, want, ., ', ', ', [SEP], E, ##7, ##23, ##2, first, said, `, `, you, do, n, ', t, want, sacrifices, and, offerings, ., [SEP], you, are, not, pleased, with, animals, killed, and, burned, or, with, sacrifices, to, take, away, sin, ., ', ', [SEP], -, l, ##rb, -, these, are, all, sacrifices, that, the, law, commands, ., -, r, ##rb, -, [SEP], then, [MASK], said, `, `, here, i, am, god, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], then, i, said, ', here, i, am, god, ., [SEP], it, is, written, about, me, in, the, book, of,\n",
            "\t\tthe, law, ., [SEP], i, have, come, to, do, what, you, want, ., ', ', ', [SEP], E, ##7, ##23, ##2,\n",
            "\t\tfirst, said, `, `, you, do, n, ', t, want, sacrifices, and, offerings, ., [SEP], you, are, not,\n",
            "\t\tpleased, with, animals, killed, and, burned, or, with, sacrifices, to, take, away, sin, ., ', ',\n",
            "\t\t[SEP], -, l, ##rb, -, these, are, all, sacrifices, that, the, law, commands, ., -, r, ##rb, -,\n",
            "\t\t[SEP], then, [MASK], said, `, `, here, i, am, god, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 95. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i have come to do what you want . '' [SEP] so god ends that first system of sacrifices and starts [MASK] new way . [SEP] [[CLS], i, have, come, to, do, what, you, want, ., ', ', [SEP], so, god, ends, that, first, system, of, sacrifices, and, starts, [MASK], new, way, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], i, have, come, to, do, what, you, want, ., ', ', [SEP], so, god, ends, that, first, system,\n",
            "\t\tof, sacrifices, and, starts, [MASK], new, way, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E4301 E7232 did the things god wanted [MASK] to do . [SEP] [[CLS], E, ##43, ##01, E, ##7, ##23, ##2, did, the, things, god, wanted, [MASK], to, do, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], E, ##43, ##01, E, ##7, ##23, ##2, did, the, things, god, wanted, [MASK], to, do, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and because of that we are made holy through the sacrifice of E7232 's body . [SEP] E7232 made that sacrifice one time -- enough for all time . [SEP] every day the priests stand and do their religious service . [SEP] again and again they offer the same sacrifices which can never take away sins . [SEP] but E7232 offered only one sacrifice for sins and that sacrifice is good for all time . [SEP] then [MASK] sat down at the right side of god . [SEP] [[CLS], and, because, of, that, we, are, made, holy, through, the, sacrifice, of, E, ##7, ##23, ##2, ', s, body, ., [SEP], E, ##7, ##23, ##2, made, that, sacrifice, one, time, -, -, enough, for, all, time, ., [SEP], every, day, the, priests, stand, and, do, their, religious, service, ., [SEP], again, and, again, they, offer, the, same, sacrifices, which, can, never, take, away, sins, ., [SEP], but, E, ##7, ##23, ##2, offered, only, one, sacrifice, for, sins, and, that, sacrifice, is, good, for, all, time, ., [SEP], then, [MASK], sat, down, at, the, right, side, of, god, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], and, because, of, that, we, are, made, holy, through, the, sacrifice, of, E, ##7, ##23, ##2,\n",
            "\t\t', s, body, ., [SEP], E, ##7, ##23, ##2, made, that, sacrifice, one, time, -, -, enough, for, all,\n",
            "\t\ttime, ., [SEP], every, day, the, priests, stand, and, do, their, religious, service, ., [SEP],\n",
            "\t\tagain, and, again, they, offer, the, same, sacrifices, which, can, never, take, away, sins, .,\n",
            "\t\t[SEP], but, E, ##7, ##23, ##2, offered, only, one, sacrifice, for, sins, and, that, sacrifice, is,\n",
            "\t\tgood, for, all, time, ., [SEP], then, [MASK], sat, down, at, the, right, side, of, god, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 89. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] with one sacrifice E7232 made [MASK] people perfect forever . [SEP] [[CLS], with, one, sacrifice, E, ##7, ##23, ##2, made, [MASK], people, perfect, forever, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], with, one, sacrifice, E, ##7, ##23, ##2, made, [MASK], people, perfect, forever, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they are the ones who are being made holy . [SEP] the holy spirit also tells us about this . [SEP] first [MASK] says `` this is the agreement i will make with my people in the future says the lord . [SEP] [[CLS], they, are, the, ones, who, are, being, made, holy, ., [SEP], the, holy, spirit, also, tells, us, about, this, ., [SEP], first, [MASK], says, `, `, this, is, the, agreement, i, will, make, with, my, people, in, the, future, says, the, lord, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], they, are, the, ones, who, are, being, made, holy, ., [SEP], the, holy, spirit, also, tells,\n",
            "\t\tus, about, this, ., [SEP], first, [MASK], says, `, `, this, is, the, agreement, i, will, make, with,\n",
            "\t\tmy, people, in, the, future, says, the, lord, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i will put my laws in their hearts . [SEP] i will write my laws in their minds . '' [SEP] then [MASK] says `` i will forget their sins and never again remember the evil they have done . '' [SEP] [[CLS], i, will, put, my, laws, in, their, hearts, ., [SEP], i, will, write, my, laws, in, their, minds, ., ', ', [SEP], then, [MASK], says, `, `, i, will, forget, their, sins, and, never, again, remember, the, evil, they, have, done, ., ', ', [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 46 with text: \n",
            " \t\t[[CLS], i, will, put, my, laws, in, their, hearts, ., [SEP], i, will, write, my, laws, in, their,\n",
            "\t\tminds, ., ', ', [SEP], then, [MASK], says, `, `, i, will, forget, their, sins, and, never, again,\n",
            "\t\tremember, the, evil, they, have, done, ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and we have a great priest who rules the house of god . [SEP] sprinkled with the blood of E7232 our hearts have been made free from a guilty conscience and our bodies have been washed with pure water . [SEP] so come near to god with a sincere heart full of confidence because of our faith in E7232 . [SEP] we must hold on to the hope we have never hesitating to tell people about it . [SEP] we can trust god to do what [MASK] promised . [SEP] [[CLS], and, we, have, a, great, priest, who, rules, the, house, of, god, ., [SEP], s, ##p, ##rinkled, with, the, blood, of, E, ##7, ##23, ##2, our, hearts, have, been, made, free, from, a, guilty, conscience, and, our, bodies, have, been, washed, with, pure, water, ., [SEP], so, come, near, to, god, with, a, sincere, heart, full, of, confidence, because, of, our, faith, in, E, ##7, ##23, ##2, ., [SEP], we, must, hold, on, to, the, hope, we, have, never, he, ##si, ##tating, to, tell, people, about, it, ., [SEP], we, can, trust, god, to, do, what, [MASK], promised, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 101 with text: \n",
            " \t\t[[CLS], and, we, have, a, great, priest, who, rules, the, house, of, god, ., [SEP], s, ##p,\n",
            "\t\t##rinkled, with, the, blood, of, E, ##7, ##23, ##2, our, hearts, have, been, made, free, from, a,\n",
            "\t\tguilty, conscience, and, our, bodies, have, been, washed, with, pure, water, ., [SEP], so, come,\n",
            "\t\tnear, to, god, with, a, sincere, heart, full, of, confidence, because, of, our, faith, in, E, ##7,\n",
            "\t\t##23, ##2, ., [SEP], we, must, hold, on, to, the, hope, we, have, never, he, ##si, ##tating, to,\n",
            "\t\ttell, people, about, it, ., [SEP], we, can, trust, god, to, do, what, [MASK], promised, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 97. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes you helped them in prison and shared in their suffering . [SEP] and you were still happy when everything you owned was taken away from you . [SEP] you continued to be happy because you knew that you had something much better -- something that would continue forever . [SEP] so do n't lose the courage that you had in the past . [SEP] your courage will be rewarded richly . [SEP] you must be patient . [SEP] after you have done what god wants you will get what [MASK] promised you . [SEP] [[CLS], yes, you, helped, them, in, prison, and, shared, in, their, suffering, ., [SEP], and, you, were, still, happy, when, everything, you, owned, was, taken, away, from, you, ., [SEP], you, continued, to, be, happy, because, you, knew, that, you, had, something, much, better, -, -, something, that, would, continue, forever, ., [SEP], so, do, n, ', t, lose, the, courage, that, you, had, in, the, past, ., [SEP], your, courage, will, be, rewarded, rich, ##ly, ., [SEP], you, must, be, patient, ., [SEP], after, you, have, done, what, god, wants, you, will, get, what, [MASK], promised, you, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], yes, you, helped, them, in, prison, and, shared, in, their, suffering, ., [SEP], and, you,\n",
            "\t\twere, still, happy, when, everything, you, owned, was, taken, away, from, you, ., [SEP], you,\n",
            "\t\tcontinued, to, be, happy, because, you, knew, that, you, had, something, much, better, -, -,\n",
            "\t\tsomething, that, would, continue, forever, ., [SEP], so, do, n, ', t, lose, the, courage, that, you,\n",
            "\t\thad, in, the, past, ., [SEP], your, courage, will, be, rewarded, rich, ##ly, ., [SEP], you, must,\n",
            "\t\tbe, patient, ., [SEP], after, you, have, done, what, god, wants, you, will, get, what, [MASK],\n",
            "\t\tpromised, you, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 95. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] says `` very soon now the one who is coming will come and will not be late . [SEP] [[CLS], [MASK], says, `, `, very, soon, now, the, one, who, is, coming, will, come, and, will, not, be, late, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], [MASK], says, `, `, very, soon, now, the, one, who, is, coming, will, come, and, will, not,\n",
            "\t\tbe, late, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the angel 's face was like the sun and [MASK] legs were like poles of fire . [SEP] [[CLS], the, angel, ', s, face, was, like, the, sun, and, [MASK], legs, were, like, poles, of, fire, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], the, angel, ', s, face, was, like, the, sun, and, [MASK], legs, were, like, poles, of, fire,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the angel was holding a small scroll . [SEP] the scroll was open in [MASK] hand . [SEP] [[CLS], the, angel, was, holding, a, small, scroll, ., [SEP], the, scroll, was, open, in, [MASK], hand, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], the, angel, was, holding, a, small, scroll, ., [SEP], the, scroll, was, open, in, [MASK],\n",
            "\t\thand, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] shouted loudly like the roaring of a lion . [SEP] [[CLS], [MASK], shouted, loudly, like, the, roaring, of, a, lion, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], shouted, loudly, like, the, roaring, of, a, lion, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 826it [00:06, 162.59it/s][CLS] after [MASK] shouted the voices of seven thunders spoke . [SEP] [[CLS], after, [MASK], shouted, the, voices, of, seven, thunder, ##s, spoke, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], after, [MASK], shouted, the, voices, of, seven, thunder, ##s, spoke, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the seven thunders spoke and i started to write . [SEP] but then i heard a voice from heaven that said `` do n't write what the seven thunders said . [SEP] keep those things secret . '' [SEP] then the angel i saw standing on the sea and on the land raised [MASK] right hand to heaven . [SEP] [[CLS], the, seven, thunder, ##s, spoke, and, i, started, to, write, ., [SEP], but, then, i, heard, a, voice, from, heaven, that, said, `, `, do, n, ', t, write, what, the, seven, thunder, ##s, said, ., [SEP], keep, those, things, secret, ., ', ', [SEP], then, the, angel, i, saw, standing, on, the, sea, and, on, the, land, raised, [MASK], right, hand, to, heaven, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 67 with text: \n",
            " \t\t[[CLS], the, seven, thunder, ##s, spoke, and, i, started, to, write, ., [SEP], but, then, i, heard,\n",
            "\t\ta, voice, from, heaven, that, said, `, `, do, n, ', t, write, what, the, seven, thunder, ##s, said,\n",
            "\t\t., [SEP], keep, those, things, secret, ., ', ', [SEP], then, the, angel, i, saw, standing, on, the,\n",
            "\t\tsea, and, on, the, land, raised, [MASK], right, hand, to, heaven, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 60. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the angel made a promise by the power of the one who lives forever and ever . [SEP] [MASK] is the one who made the skies and all that is in them . [SEP] [[CLS], the, angel, made, a, promise, by, the, power, of, the, one, who, lives, forever, and, ever, ., [SEP], [MASK], is, the, one, who, made, the, skies, and, all, that, is, in, them, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 35 with text: \n",
            " \t\t[[CLS], the, angel, made, a, promise, by, the, power, of, the, one, who, lives, forever, and, ever,\n",
            "\t\t., [SEP], [MASK], is, the, one, who, made, the, skies, and, all, that, is, in, them, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then i heard the same voice from heaven again . [SEP] it said to me `` go and take the open scroll that is in the angel 's hand . [SEP] this is the angel who is standing on the sea and on the land . '' [SEP] so i went to the angel and asked [MASK] to give me the little scroll . [SEP] [[CLS], then, i, heard, the, same, voice, from, heaven, again, ., [SEP], it, said, to, me, `, `, go, and, take, the, open, scroll, that, is, in, the, angel, ', s, hand, ., [SEP], this, is, the, angel, who, is, standing, on, the, sea, and, on, the, land, ., ', ', [SEP], so, i, went, to, the, angel, and, asked, [MASK], to, give, me, the, little, scroll, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 69 with text: \n",
            " \t\t[[CLS], then, i, heard, the, same, voice, from, heaven, again, ., [SEP], it, said, to, me, `, `, go,\n",
            "\t\tand, take, the, open, scroll, that, is, in, the, angel, ', s, hand, ., [SEP], this, is, the, angel,\n",
            "\t\twho, is, standing, on, the, sea, and, on, the, land, ., ', ', [SEP], so, i, went, to, the, angel,\n",
            "\t\tand, asked, [MASK], to, give, me, the, little, scroll, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 60. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] said to me `` take the scroll and eat it . [SEP] [[CLS], [MASK], said, to, me, `, `, take, the, scroll, and, eat, it, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], [MASK], said, to, me, `, `, take, the, scroll, and, eat, it, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so i took the little scroll from the angel 's hand and ate it . [SEP] in my mouth it tasted sweet like honey but after i ate it it was sour in my stomach . [SEP] then i was told `` you must prophesy again about many races of people many nations languages and rulers . '' [SEP] i saw an angel coming down out of heaven . [SEP] the angel had the key to the bottomless pit . [SEP] the angel also held a large chain in [MASK] hand . [SEP] [[CLS], so, i, took, the, little, scroll, from, the, angel, ', s, hand, and, ate, it, ., [SEP], in, my, mouth, it, tasted, sweet, like, honey, but, after, i, ate, it, it, was, sour, in, my, stomach, ., [SEP], then, i, was, told, `, `, you, must, prop, ##hes, ##y, again, about, many, races, of, people, many, nations, languages, and, rulers, ., ', ', [SEP], i, saw, an, angel, coming, down, out, of, heaven, ., [SEP], the, angel, had, the, key, to, the, bottom, ##less, pit, ., [SEP], the, angel, also, held, a, large, chain, in, [MASK], hand, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], so, i, took, the, little, scroll, from, the, angel, ', s, hand, and, ate, it, ., [SEP], in,\n",
            "\t\tmy, mouth, it, tasted, sweet, like, honey, but, after, i, ate, it, it, was, sour, in, my, stomach,\n",
            "\t\t., [SEP], then, i, was, told, `, `, you, must, prop, ##hes, ##y, again, about, many, races, of,\n",
            "\t\tpeople, many, nations, languages, and, rulers, ., ', ', [SEP], i, saw, an, angel, coming, down, out,\n",
            "\t\tof, heaven, ., [SEP], the, angel, had, the, key, to, the, bottom, ##less, pit, ., [SEP], the, angel,\n",
            "\t\talso, held, a, large, chain, in, [MASK], hand, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 96. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they came back to life and ruled with E7232 for 1000 years . [SEP] -lrb- the rest of the dead did not live again until the 1000 years were ended . -rrb- [SEP] this is the first resurrection . [SEP] great blessings belong to those who share in this first resurrection . [SEP] they are god 's holy people . [SEP] the second death has no power over them . [SEP] they will be priests for god and for E7232 . [SEP] they will rule with [MASK] for 1000 years . [SEP] [[CLS], they, came, back, to, life, and, ruled, with, E, ##7, ##23, ##2, for, 1000, years, ., [SEP], -, l, ##rb, -, the, rest, of, the, dead, did, not, live, again, until, the, 1000, years, were, ended, ., -, r, ##rb, -, [SEP], this, is, the, first, resurrection, ., [SEP], great, blessing, ##s, belong, to, those, who, share, in, this, first, resurrection, ., [SEP], they, are, god, ', s, holy, people, ., [SEP], the, second, death, has, no, power, over, them, ., [SEP], they, will, be, priests, for, god, and, for, E, ##7, ##23, ##2, ., [SEP], they, will, rule, with, [MASK], for, 1000, years, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 107 with text: \n",
            " \t\t[[CLS], they, came, back, to, life, and, ruled, with, E, ##7, ##23, ##2, for, 1000, years, ., [SEP],\n",
            "\t\t-, l, ##rb, -, the, rest, of, the, dead, did, not, live, again, until, the, 1000, years, were,\n",
            "\t\tended, ., -, r, ##rb, -, [SEP], this, is, the, first, resurrection, ., [SEP], great, blessing, ##s,\n",
            "\t\tbelong, to, those, who, share, in, this, first, resurrection, ., [SEP], they, are, god, ', s, holy,\n",
            "\t\tpeople, ., [SEP], the, second, death, has, no, power, over, them, ., [SEP], they, will, be, priests,\n",
            "\t\tfor, god, and, for, E, ##7, ##23, ##2, ., [SEP], they, will, rule, with, [MASK], for, 1000, years,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 101. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] when the 1000 years are ended satan will be made free from [MASK] prison . [SEP] [[CLS], when, the, 1000, years, are, ended, sat, ##an, will, be, made, free, from, [MASK], prison, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], when, the, 1000, years, are, ended, sat, ##an, will, be, made, free, from, [MASK], prison,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] will go out to trick the nations in all the earth the nations known as gog and magog . [SEP] [[CLS], [MASK], will, go, out, to, trick, the, nations, in, all, the, earth, the, nations, known, as, go, ##g, and, ma, ##go, ##g, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], [MASK], will, go, out, to, trick, the, nations, in, all, the, earth, the, nations, known,\n",
            "\t\tas, go, ##g, and, ma, ##go, ##g, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] satan will gather the people for battle . [SEP] there will be more people than anyone can count like sand on the seashore . [SEP] i saw satan 's army march across the earth and gather around the camp of god 's people and the city that god loves . [SEP] but fire came down from heaven and destroyed satan 's army . [SEP] and [MASK] -lrb- the one who tricked these people -rrb- was thrown into the lake of burning sulfur with the beast and the false prophet . [SEP] [[CLS], sat, ##an, will, gather, the, people, for, battle, ., [SEP], there, will, be, more, people, than, anyone, can, count, like, sand, on, the, seas, ##hore, ., [SEP], i, saw, sat, ##an, ', s, army, march, across, the, earth, and, gather, around, the, camp, of, god, ', s, people, and, the, city, that, god, loves, ., [SEP], but, fire, came, down, from, heaven, and, destroyed, sat, ##an, ', s, army, ., [SEP], and, [MASK], -, l, ##rb, -, the, one, who, trick, ##ed, these, people, -, r, ##rb, -, was, thrown, into, the, lake, of, burning, sulfur, with, the, beast, and, the, false, prophet, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 106 with text: \n",
            " \t\t[[CLS], sat, ##an, will, gather, the, people, for, battle, ., [SEP], there, will, be, more, people,\n",
            "\t\tthan, anyone, can, count, like, sand, on, the, seas, ##hore, ., [SEP], i, saw, sat, ##an, ', s,\n",
            "\t\tarmy, march, across, the, earth, and, gather, around, the, camp, of, god, ', s, people, and, the,\n",
            "\t\tcity, that, god, loves, ., [SEP], but, fire, came, down, from, heaven, and, destroyed, sat, ##an, ',\n",
            "\t\ts, army, ., [SEP], and, [MASK], -, l, ##rb, -, the, one, who, trick, ##ed, these, people, -, r,\n",
            "\t\t##rb, -, was, thrown, into, the, lake, of, burning, sulfur, with, the, beast, and, the, false,\n",
            "\t\tprophet, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 73. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] there they would be tortured day and night forever and ever . [SEP] then i saw a large white throne . [SEP] i saw the one who was sitting on the throne . [SEP] earth and sky ran away from [MASK] and disappeared . [SEP] [[CLS], there, they, would, be, tortured, day, and, night, forever, and, ever, ., [SEP], then, i, saw, a, large, white, throne, ., [SEP], i, saw, the, one, who, was, sitting, on, the, throne, ., [SEP], earth, and, sky, ran, away, from, [MASK], and, disappeared, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 46 with text: \n",
            " \t\t[[CLS], there, they, would, be, tortured, day, and, night, forever, and, ever, ., [SEP], then, i,\n",
            "\t\tsaw, a, large, white, throne, ., [SEP], i, saw, the, one, who, was, sitting, on, the, throne, .,\n",
            "\t\t[SEP], earth, and, sky, ran, away, from, [MASK], and, disappeared, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 41. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes . [SEP] sure . [SEP] y- [SEP] %um [SEP] could be it 's actually [SEP] i ca n't believe this . [SEP] this is a german friend that i [SEP] [MASK] worked here at nasa for like two years and was in mhm our german conversation club . [SEP] [[CLS], yes, ., [SEP], sure, ., [SEP], y, -, [SEP], %, um, [SEP], could, be, it, ', s, actually, [SEP], i, ca, n, ', t, believe, this, ., [SEP], this, is, a, g, ##erman, friend, that, i, [SEP], [MASK], worked, here, at, na, ##sa, for, like, two, years, and, was, in, m, ##hm, our, g, ##erman, conversation, club, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 60 with text: \n",
            " \t\t[[CLS], yes, ., [SEP], sure, ., [SEP], y, -, [SEP], %, um, [SEP], could, be, it, ', s, actually,\n",
            "\t\t[SEP], i, ca, n, ', t, believe, this, ., [SEP], this, is, a, g, ##erman, friend, that, i, [SEP],\n",
            "\t\t[MASK], worked, here, at, na, ##sa, for, like, two, years, and, was, in, m, ##hm, our, g, ##erman,\n",
            "\t\tconversation, club, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 38. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] and [MASK] 's marrying a german woman . [SEP] [[CLS], yeah, ., [SEP], and, [MASK], ', s, marrying, a, g, ##erman, woman, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], and, [MASK], ', s, marrying, a, g, ##erman, woman, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no [MASK] 's an american [SEP] [[CLS], no, [MASK], ', s, an, am, ##eric, ##an, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], no, [MASK], ', s, an, am, ##eric, ##an, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] 's marrying a german woman . [SEP] [[CLS], but, [MASK], ', s, marrying, a, g, ##erman, woman, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], but, [MASK], ', s, marrying, a, g, ##erman, woman, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh [MASK] 's a german . [SEP] [[CLS], oh, [MASK], ', s, a, g, ##erman, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], oh, [MASK], ', s, a, g, ##erman, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's german too . [SEP] [[CLS], [MASK], ', s, g, ##erman, too, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], ', s, g, ##erman, too, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 843it [00:06, 163.45it/s][CLS] yeah . [SEP] they 're both german . [SEP] so this is a real mhm . german wedding . [SEP] but [SEP] and so [MASK] was saying we should come from our [SEP] [[CLS], yeah, ., [SEP], they, ', re, both, g, ##erman, ., [SEP], so, this, is, a, real, m, ##hm, ., g, ##erman, wedding, ., [SEP], but, [SEP], and, so, [MASK], was, saying, we, should, come, from, our, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], they, ', re, both, g, ##erman, ., [SEP], so, this, is, a, real, m, ##hm, .,\n",
            "\t\tg, ##erman, wedding, ., [SEP], but, [SEP], and, so, [MASK], was, saying, we, should, come, from,\n",
            "\t\tour, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 29. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so [MASK] 's going to be there . [SEP] [[CLS], so, [MASK], ', s, going, to, be, there, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], so, [MASK], ', s, going, to, be, there, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] ja ja . [SEP] ball dresses and you know tuxedos and things . [SEP] yeah i know . [SEP] i could n't believe it [SEP] because i met [SEP] when i was there on the other trip i had n't read the invitation before . [SEP] i did n't think i was going . [SEP] and so i had met %uh [MASK] name is heiko for one day in hamburg . [SEP] [[CLS], j, ##a, j, ##a, ., [SEP], ball, dresses, and, you, know, t, ##uxe, ##dos, and, things, ., [SEP], yeah, i, know, ., [SEP], i, could, n, ', t, believe, it, [SEP], because, i, met, [SEP], when, i, was, there, on, the, other, trip, i, had, n, ', t, read, the, invitation, before, ., [SEP], i, did, n, ', t, think, i, was, going, ., [SEP], and, so, i, had, met, %, uh, [MASK], name, is, he, ##iko, for, one, day, in, ha, ##mb, ##urg, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 87 with text: \n",
            " \t\t[[CLS], j, ##a, j, ##a, ., [SEP], ball, dresses, and, you, know, t, ##uxe, ##dos, and, things, .,\n",
            "\t\t[SEP], yeah, i, know, ., [SEP], i, could, n, ', t, believe, it, [SEP], because, i, met, [SEP], when,\n",
            "\t\ti, was, there, on, the, other, trip, i, had, n, ', t, read, the, invitation, before, ., [SEP], i,\n",
            "\t\tdid, n, ', t, think, i, was, going, ., [SEP], and, so, i, had, met, %, uh, [MASK], name, is, he,\n",
            "\t\t##iko, for, one, day, in, ha, ##mb, ##urg, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 73. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i said you 're going to get married in a tuxedo ? [SEP] [MASK] said no no no . [SEP] [[CLS], and, i, said, you, ', re, going, to, get, married, in, a, t, ##uxe, ##do, ?, [SEP], [MASK], said, no, no, no, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], and, i, said, you, ', re, going, to, get, married, in, a, t, ##uxe, ##do, ?, [SEP], [MASK],\n",
            "\t\tsaid, no, no, no, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] aside from we can talk [SEP] and they should be nice people . [SEP] it 'll be [SEP] and then i 'll know one more person there . [SEP] how old are they ? [SEP] oh . [SEP] a little bit younger than us . [SEP] oh [MASK] 's probably [SEP] [[CLS], aside, from, we, can, talk, [SEP], and, they, should, be, nice, people, ., [SEP], it, ', ll, be, [SEP], and, then, i, ', ll, know, one, more, person, there, ., [SEP], how, old, are, they, ?, [SEP], oh, ., [SEP], a, little, bit, younger, than, us, ., [SEP], oh, [MASK], ', s, probably, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 55 with text: \n",
            " \t\t[[CLS], aside, from, we, can, talk, [SEP], and, they, should, be, nice, people, ., [SEP], it, ', ll,\n",
            "\t\tbe, [SEP], and, then, i, ', ll, know, one, more, person, there, ., [SEP], how, old, are, they, ?,\n",
            "\t\t[SEP], oh, ., [SEP], a, little, bit, younger, than, us, ., [SEP], oh, [MASK], ', s, probably, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] good question . [SEP] i do n't know . [SEP] maybe [MASK] 's thirty - two ? [SEP] [[CLS], good, question, ., [SEP], i, do, n, ', t, know, ., [SEP], maybe, [MASK], ', s, thirty, -, two, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], good, question, ., [SEP], i, do, n, ', t, know, ., [SEP], maybe, [MASK], ', s, thirty, -,\n",
            "\t\ttwo, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] that 's okay . [SEP] as long as they 're not twenty - one . [SEP] oh no no no no . [SEP] no %uh spring chicken [SEP] this is herr doktor heiko hecht and E6695 whatever [MASK] name is . [SEP] [[CLS], that, ', s, okay, ., [SEP], as, long, as, they, ', re, not, twenty, -, one, ., [SEP], oh, no, no, no, no, ., [SEP], no, %, uh, spring, chicken, [SEP], this, is, her, ##r, do, ##kt, ##or, he, ##iko, he, ##cht, and, E, ##6, ##6, ##9, ##5, whatever, [MASK], name, is, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 55 with text: \n",
            " \t\t[[CLS], that, ', s, okay, ., [SEP], as, long, as, they, ', re, not, twenty, -, one, ., [SEP], oh,\n",
            "\t\tno, no, no, no, ., [SEP], no, %, uh, spring, chicken, [SEP], this, is, her, ##r, do, ##kt, ##or, he,\n",
            "\t\t##iko, he, ##cht, and, E, ##6, ##6, ##9, ##5, whatever, [MASK], name, is, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but you 're afraid to say no [SEP] or you have to think about it . [SEP] but it 's the twelfth . [SEP] and otherwise what i 'd like to try to do is stay with you the twelfth and thirteenth . because it 's weekend . [SEP] mhm . [SEP] like we could see each other . [SEP] and then i 'll go monday -- down to bonn or wherever those relatives are because i have n't seen my uncle in like three or four years and [MASK] 's just turning eighty and i like them a lot [SEP] [[CLS], but, you, ', re, afraid, to, say, no, [SEP], or, you, have, to, think, about, it, ., [SEP], but, it, ', s, the, twelfth, ., [SEP], and, otherwise, what, i, ', d, like, to, try, to, do, is, stay, with, you, the, twelfth, and, thirteenth, ., because, it, ', s, weekend, ., [SEP], m, ##hm, ., [SEP], like, we, could, see, each, other, ., [SEP], and, then, i, ', ll, go, mon, ##day, -, -, down, to, b, ##on, ##n, or, wherever, those, relatives, are, because, i, have, n, ', t, seen, my, uncle, in, like, three, or, four, years, and, [MASK], ', s, just, turning, eighty, and, i, like, them, a, lot, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 115 with text: \n",
            " \t\t[[CLS], but, you, ', re, afraid, to, say, no, [SEP], or, you, have, to, think, about, it, ., [SEP],\n",
            "\t\tbut, it, ', s, the, twelfth, ., [SEP], and, otherwise, what, i, ', d, like, to, try, to, do, is,\n",
            "\t\tstay, with, you, the, twelfth, and, thirteenth, ., because, it, ', s, weekend, ., [SEP], m, ##hm, .,\n",
            "\t\t[SEP], like, we, could, see, each, other, ., [SEP], and, then, i, ', ll, go, mon, ##day, -, -, down,\n",
            "\t\tto, b, ##on, ##n, or, wherever, those, relatives, are, because, i, have, n, ', t, seen, my, uncle,\n",
            "\t\tin, like, three, or, four, years, and, [MASK], ', s, just, turning, eighty, and, i, like, them, a,\n",
            "\t\tlot, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 102. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh . [SEP] in bad muenstereifel ? [SEP] yeah [SEP] i 'll [SEP] guess it 'll be summer [SEP] so [MASK] 'll be in bad munstereifel . [SEP] [[CLS], uh, -, huh, ., [SEP], in, bad, m, ##uen, ##ster, ##ei, ##fe, ##l, ?, [SEP], yeah, [SEP], i, ', ll, [SEP], guess, it, ', ll, be, summer, [SEP], so, [MASK], ', ll, be, in, bad, m, ##uns, ##ter, ##ei, ##fe, ##l, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], uh, -, huh, ., [SEP], in, bad, m, ##uen, ##ster, ##ei, ##fe, ##l, ?, [SEP], yeah, [SEP], i,\n",
            "\t\t', ll, [SEP], guess, it, ', ll, be, summer, [SEP], so, [MASK], ', ll, be, in, bad, m, ##uns, ##ter,\n",
            "\t\t##ei, ##fe, ##l, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 30. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mhm [SEP] they 've actually moved . [SEP] they do n't live in bonn now . [SEP] they live in -- %um bad neuena . which is i guess [SEP] bad neuena ja . [SEP] yeah [SEP] just a little bit down . [SEP] but i think they 'll be in munstereifel . [SEP] and then my cousin [MASK] still does live in bonn [SEP] [[CLS], m, ##hm, [SEP], they, ', ve, actually, moved, ., [SEP], they, do, n, ', t, live, in, b, ##on, ##n, now, ., [SEP], they, live, in, -, -, %, um, bad, ne, ##uen, ##a, ., which, is, i, guess, [SEP], bad, ne, ##uen, ##a, j, ##a, ., [SEP], yeah, [SEP], just, a, little, bit, down, ., [SEP], but, i, think, they, ', ll, be, in, m, ##uns, ##ter, ##ei, ##fe, ##l, ., [SEP], and, then, my, cousin, [MASK], still, does, live, in, b, ##on, ##n, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 87 with text: \n",
            " \t\t[[CLS], m, ##hm, [SEP], they, ', ve, actually, moved, ., [SEP], they, do, n, ', t, live, in, b,\n",
            "\t\t##on, ##n, now, ., [SEP], they, live, in, -, -, %, um, bad, ne, ##uen, ##a, ., which, is, i, guess,\n",
            "\t\t[SEP], bad, ne, ##uen, ##a, j, ##a, ., [SEP], yeah, [SEP], just, a, little, bit, down, ., [SEP],\n",
            "\t\tbut, i, think, they, ', ll, be, in, m, ##uns, ##ter, ##ei, ##fe, ##l, ., [SEP], and, then, my,\n",
            "\t\tcousin, [MASK], still, does, live, in, b, ##on, ##n, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 78. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so i 'll get to see [MASK] . [SEP] [[CLS], so, i, ', ll, get, to, see, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], so, i, ', ll, get, to, see, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know your mother 's being critical you know . [SEP] be you see that because you 're critical to yourself too much [SEP] or you 're filtering the information you get from your mother [SEP] yeah [SEP] but this is ridiculous . [SEP] i mean %uh [MASK] 's never let up on that . [SEP] [[CLS], you, know, your, mother, ', s, being, critical, you, know, ., [SEP], be, you, see, that, because, you, ', re, critical, to, yourself, too, much, [SEP], or, you, ', re, filtering, the, information, you, get, from, your, mother, [SEP], yeah, [SEP], but, this, is, ridiculous, ., [SEP], i, mean, %, uh, [MASK], ', s, never, let, up, on, that, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], you, know, your, mother, ', s, being, critical, you, know, ., [SEP], be, you, see, that,\n",
            "\t\tbecause, you, ', re, critical, to, yourself, too, much, [SEP], or, you, ', re, filtering, the,\n",
            "\t\tinformation, you, get, from, your, mother, [SEP], yeah, [SEP], but, this, is, ridiculous, ., [SEP],\n",
            "\t\ti, mean, %, uh, [MASK], ', s, never, let, up, on, that, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i mean it 's crazy . you know . [SEP] i do n't i mean [SEP] and i think no not the last time the last time on the phone i bugged [MASK] about something else that i been trying to not take anymore [SEP] [[CLS], i, mean, it, ', s, crazy, ., you, know, ., [SEP], i, do, n, ', t, i, mean, [SEP], and, i, think, no, not, the, last, time, the, last, time, on, the, phone, i, bug, ##ged, [MASK], about, something, else, that, i, been, trying, to, not, take, anymore, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], i, mean, it, ', s, crazy, ., you, know, ., [SEP], i, do, n, ', t, i, mean, [SEP], and, i,\n",
            "\t\tthink, no, not, the, last, time, the, last, time, on, the, phone, i, bug, ##ged, [MASK], about,\n",
            "\t\tsomething, else, that, i, been, trying, to, not, take, anymore, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 37. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but then [MASK] cries [SEP] [[CLS], but, then, [MASK], cries, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], but, then, [MASK], cries, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but that does n't work with me . [SEP] uh-huh . [SEP] that 's good . [SEP] then [MASK] says i 'm being mean . [SEP] [[CLS], but, that, does, n, ', t, work, with, me, ., [SEP], uh, -, huh, ., [SEP], that, ', s, good, ., [SEP], then, [MASK], says, i, ', m, being, mean, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], but, that, does, n, ', t, work, with, me, ., [SEP], uh, -, huh, ., [SEP], that, ', s, good,\n",
            "\t\t., [SEP], then, [MASK], says, i, ', m, being, mean, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i ignore the fact that [MASK] 's crying . [SEP] [[CLS], and, i, ignore, the, fact, that, [MASK], ', s, crying, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], and, i, ignore, the, fact, that, [MASK], ', s, crying, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i 'm pretty tough . [SEP] yeah [SEP] yeah [SEP] speaking of crying i have a colleague . an american girl a dancer . mhm at a school in cologne mhm who somehow or other has fallen in love with me . [SEP] mhm [SEP] and now that i 'm going back to duesseldorf [MASK] 's all going crazy . %um and thinks %eh [SEP] [[CLS], i, ', m, pretty, tough, ., [SEP], yeah, [SEP], yeah, [SEP], speaking, of, crying, i, have, a, colleague, ., an, am, ##eric, ##an, girl, a, dancer, ., m, ##hm, at, a, school, in, co, ##log, ##ne, m, ##hm, who, somehow, or, other, has, fallen, in, love, with, me, ., [SEP], m, ##hm, [SEP], and, now, that, i, ', m, going, back, to, due, ##ssel, ##dorf, [MASK], ', s, all, going, crazy, ., %, um, and, thinks, %, eh, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 80 with text: \n",
            " \t\t[[CLS], i, ', m, pretty, tough, ., [SEP], yeah, [SEP], yeah, [SEP], speaking, of, crying, i, have,\n",
            "\t\ta, colleague, ., an, am, ##eric, ##an, girl, a, dancer, ., m, ##hm, at, a, school, in, co, ##log,\n",
            "\t\t##ne, m, ##hm, who, somehow, or, other, has, fallen, in, love, with, me, ., [SEP], m, ##hm, [SEP],\n",
            "\t\tand, now, that, i, ', m, going, back, to, due, ##ssel, ##dorf, [MASK], ', s, all, going, crazy, .,\n",
            "\t\t%, um, and, thinks, %, eh, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 66. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 860it [00:06, 164.22it/s][CLS] oho [SEP] but it is [SEP] being stalked here ? [SEP] pardon ? [SEP] in america we would say are you being stalked ? [SEP] so [MASK] said to let your wife do it you know [SEP] [[CLS], oh, ##o, [SEP], but, it, is, [SEP], being, stalked, here, ?, [SEP], pardon, ?, [SEP], in, am, ##eric, ##a, we, would, say, are, you, being, stalked, ?, [SEP], so, [MASK], said, to, let, your, wife, do, it, you, know, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 41 with text: \n",
            " \t\t[[CLS], oh, ##o, [SEP], but, it, is, [SEP], being, stalked, here, ?, [SEP], pardon, ?, [SEP], in,\n",
            "\t\tam, ##eric, ##a, we, would, say, are, you, being, stalked, ?, [SEP], so, [MASK], said, to, let,\n",
            "\t\tyour, wife, do, it, you, know, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 30. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %uh right [SEP] [MASK] says %uh you know no i 'm going to [SEP] [[CLS], %, uh, right, [SEP], [MASK], says, %, uh, you, know, no, i, ', m, going, to, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], %, uh, right, [SEP], [MASK], says, %, uh, you, know, no, i, ', m, going, to, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %eh so now i 'm at a point that it 's very strange [SEP] like i wo n't complain about anything [SEP] or i ca n't complain about anything [SEP] and i ca n't ask [MASK] to come help me in the middle of the night when i forgot to take out something they put in the freezer [SEP] [[CLS], %, eh, so, now, i, ', m, at, a, point, that, it, ', s, very, strange, [SEP], like, i, w, ##o, n, ', t, complain, about, anything, [SEP], or, i, ca, n, ', t, complain, about, anything, [SEP], and, i, ca, n, ', t, ask, [MASK], to, come, help, me, in, the, middle, of, the, night, when, i, forgot, to, take, out, something, they, put, in, the, freeze, ##r, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 71 with text: \n",
            " \t\t[[CLS], %, eh, so, now, i, ', m, at, a, point, that, it, ', s, very, strange, [SEP], like, i, w,\n",
            "\t\t##o, n, ', t, complain, about, anything, [SEP], or, i, ca, n, ', t, complain, about, anything,\n",
            "\t\t[SEP], and, i, ca, n, ', t, ask, [MASK], to, come, help, me, in, the, middle, of, the, night, when,\n",
            "\t\ti, forgot, to, take, out, something, they, put, in, the, freeze, ##r, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 46. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so we 'll see how it goes [SEP] hopefully [SEP] i mean they promise me you know everybody that they 're going to help me and that whatever i say [SEP] but you know everybody gives you %uh these %uh pro- [SEP] yeah [SEP] so you 've already started [SEP] well i 've started working with du [SEP] like i do n't cook anymore [SEP] i have n't worked in the kitchen for a week already [SEP] i 've been just doing with [MASK] learning how to do the orders and meeting all the %um sales people [SEP] [[CLS], so, we, ', ll, see, how, it, goes, [SEP], hopefully, [SEP], i, mean, they, promise, me, you, know, everybody, that, they, ', re, going, to, help, me, and, that, whatever, i, say, [SEP], but, you, know, everybody, gives, you, %, uh, these, %, uh, pro, -, [SEP], yeah, [SEP], so, you, ', ve, already, started, [SEP], well, i, ', ve, started, working, with, du, [SEP], like, i, do, n, ', t, cook, anymore, [SEP], i, have, n, ', t, worked, in, the, kitchen, for, a, week, already, [SEP], i, ', ve, been, just, doing, with, [MASK], learning, how, to, do, the, orders, and, meeting, all, the, %, um, sales, people, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 112 with text: \n",
            " \t\t[[CLS], so, we, ', ll, see, how, it, goes, [SEP], hopefully, [SEP], i, mean, they, promise, me, you,\n",
            "\t\tknow, everybody, that, they, ', re, going, to, help, me, and, that, whatever, i, say, [SEP], but,\n",
            "\t\tyou, know, everybody, gives, you, %, uh, these, %, uh, pro, -, [SEP], yeah, [SEP], so, you, ', ve,\n",
            "\t\talready, started, [SEP], well, i, ', ve, started, working, with, du, [SEP], like, i, do, n, ', t,\n",
            "\t\tcook, anymore, [SEP], i, have, n, ', t, worked, in, the, kitchen, for, a, week, already, [SEP], i,\n",
            "\t\t', ve, been, just, doing, with, [MASK], learning, how, to, do, the, orders, and, meeting, all, the,\n",
            "\t\t%, um, sales, people, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 96. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %um i 've learned how to work the computer which is the bigges- [SEP] i said i do n't know how to turn it on even [SEP] really [SEP] so i 'm not [SEP] i do n't know if i could do it yet by myself [SEP] but [MASK] 's showing me teaching me [SEP] [[CLS], and, %, um, i, ', ve, learned, how, to, work, the, computer, which, is, the, big, ##ges, -, [SEP], i, said, i, do, n, ', t, know, how, to, turn, it, on, even, [SEP], really, [SEP], so, i, ', m, not, [SEP], i, do, n, ', t, know, if, i, could, do, it, yet, by, myself, [SEP], but, [MASK], ', s, showing, me, teaching, me, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 67 with text: \n",
            " \t\t[[CLS], and, %, um, i, ', ve, learned, how, to, work, the, computer, which, is, the, big, ##ges, -,\n",
            "\t\t[SEP], i, said, i, do, n, ', t, know, how, to, turn, it, on, even, [SEP], really, [SEP], so, i, ',\n",
            "\t\tm, not, [SEP], i, do, n, ', t, know, if, i, could, do, it, yet, by, myself, [SEP], but, [MASK], ',\n",
            "\t\ts, showing, me, teaching, me, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 59. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i think like the middle of this week [MASK] 'll be there a few days with me [SEP] [[CLS], and, i, think, like, the, middle, of, this, week, [MASK], ', ll, be, there, a, few, days, with, me, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], and, i, think, like, the, middle, of, this, week, [MASK], ', ll, be, there, a, few, days,\n",
            "\t\twith, me, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and then anoth- [SEP] [MASK] %um [SEP] [[CLS], and, then, an, ##oth, -, [SEP], [MASK], %, um, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, then, an, ##oth, -, [SEP], [MASK], %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] maybe here and there [SEP] but [MASK] 's really basically [SEP] [[CLS], maybe, here, and, there, [SEP], but, [MASK], ', s, really, basically, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], maybe, here, and, there, [SEP], but, [MASK], ', s, really, basically, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] what is [MASK] going to do [SEP] [[CLS], what, is, [MASK], going, to, do, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], what, is, [MASK], going, to, do, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's going to be a teacher for %um high school computers [SEP] [[CLS], [MASK], ', s, going, to, be, a, teacher, for, %, um, high, school, computers, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], [MASK], ', s, going, to, be, a, teacher, for, %, um, high, school, computers, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh okay [SEP] yeah [SEP] so [MASK] 's only teaching three days a week [SEP] [[CLS], oh, okay, [SEP], yeah, [SEP], so, [MASK], ', s, only, teaching, three, days, a, week, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], oh, okay, [SEP], yeah, [SEP], so, [MASK], ', s, only, teaching, three, days, a, week, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the truth is [MASK] 's been teaching and very patient and very good [SEP] [[CLS], the, truth, is, [MASK], ', s, been, teaching, and, very, patient, and, very, good, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], the, truth, is, [MASK], ', s, been, teaching, and, very, patient, and, very, good, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i 'm sure [MASK] 's thrilled to get out of there [SEP] [[CLS], i, ', m, sure, [MASK], ', s, thrilled, to, get, out, of, there, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], i, ', m, sure, [MASK], ', s, thrilled, to, get, out, of, there, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i mean [MASK] has a different head than everybody else [SEP] [[CLS], i, mean, [MASK], has, a, different, head, than, everybody, else, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], i, mean, [MASK], has, a, different, head, than, everybody, else, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] like i know that [MASK] wants everything to be nice and to be good and [SEP] [[CLS], like, i, know, that, [MASK], wants, everything, to, be, nice, and, to, be, good, and, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], like, i, know, that, [MASK], wants, everything, to, be, nice, and, to, be, good, and, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] 's very separate [SEP] [[CLS], but, [MASK], ', s, very, separate, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], but, [MASK], ', s, very, separate, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] does n't like to listen to other people 's %um ideas [SEP] [[CLS], and, [MASK], does, n, ', t, like, to, listen, to, other, people, ', s, %, um, ideas, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], and, [MASK], does, n, ', t, like, to, listen, to, other, people, ', s, %, um, ideas, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 877it [00:07, 164.73it/s][CLS] yeah [SEP] because [MASK] 's also [SEP] [[CLS], yeah, [SEP], because, [MASK], ', s, also, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], yeah, [SEP], because, [MASK], ', s, also, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i 'll tell you [SEP] i worked with [MASK] this whole week [SEP] [[CLS], i, ', ll, tell, you, [SEP], i, worked, with, [MASK], this, whole, week, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], i, ', ll, tell, you, [SEP], i, worked, with, [MASK], this, whole, week, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] does n't eat a thing ever [SEP] [[CLS], [MASK], does, n, ', t, eat, a, thing, ever, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], [MASK], does, n, ', t, eat, a, thing, ever, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] like all morning long [SEP] so [MASK] does n't eat [SEP] [[CLS], like, all, morning, long, [SEP], so, [MASK], does, n, ', t, eat, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], like, all, morning, long, [SEP], so, [MASK], does, n, ', t, eat, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] does n't drink coffee [SEP] [[CLS], [MASK], does, n, ', t, drink, coffee, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], does, n, ', t, drink, coffee, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i did n't see [MASK] drink anything water or anything else [SEP] [[CLS], i, did, n, ', t, see, [MASK], drink, anything, water, or, anything, else, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], i, did, n, ', t, see, [MASK], drink, anything, water, or, anything, else, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] one day [MASK] said i better drink something i have a headache because it was really hot [SEP] [[CLS], one, day, [MASK], said, i, better, drink, something, i, have, a, headache, because, it, was, really, hot, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], one, day, [MASK], said, i, better, drink, something, i, have, a, headache, because, it, was,\n",
            "\t\treally, hot, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but i mean you know [MASK] does n't eat anything [SEP] [[CLS], but, i, mean, you, know, [MASK], does, n, ', t, eat, anything, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], but, i, mean, you, know, [MASK], does, n, ', t, eat, anything, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and then for lunch when we finally went to go get something to eat also [MASK] eats like a little what we would eat maybe half for a meal [SEP] [[CLS], and, then, for, lunch, when, we, finally, went, to, go, get, something, to, eat, also, [MASK], eats, like, a, little, what, we, would, eat, maybe, half, for, a, meal, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 31 with text: \n",
            " \t\t[[CLS], and, then, for, lunch, when, we, finally, went, to, go, get, something, to, eat, also,\n",
            "\t\t[MASK], eats, like, a, little, what, we, would, eat, maybe, half, for, a, meal, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 16. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] right [SEP] that 's so funny [SEP] but anyway [SEP] no [SEP] so [MASK] had agreed to do it for a year [SEP] [[CLS], right, [SEP], that, ', s, so, funny, [SEP], but, anyway, [SEP], no, [SEP], so, [MASK], had, agreed, to, do, it, for, a, year, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], right, [SEP], that, ', s, so, funny, [SEP], but, anyway, [SEP], no, [SEP], so, [MASK], had,\n",
            "\t\tagreed, to, do, it, for, a, year, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] year is over [SEP] [[CLS], and, [MASK], year, is, over, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], and, [MASK], year, is, over, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] tha- [SEP] and they 're going to notice that the cooking is different [SEP] well that 's what even **ruthie** says you know [SEP] everyone thinks that the economie is so important [SEP] but there 's a lot of things that are very important of running the kitchen [SEP] and we 'll see [SEP] %uh right now E1214 %uh [SEP] you know cocoa [MASK] got married [SEP] [[CLS], th, ##a, -, [SEP], and, they, ', re, going, to, notice, that, the, cooking, is, different, [SEP], well, that, ', s, what, even, *, *, r, ##uth, ##ie, *, *, says, you, know, [SEP], everyone, thinks, that, the, e, ##con, ##omi, ##e, is, so, important, [SEP], but, there, ', s, a, lot, of, things, that, are, very, important, of, running, the, kitchen, [SEP], and, we, ', ll, see, [SEP], %, uh, right, now, E, ##12, ##14, %, uh, [SEP], you, know, co, ##coa, [MASK], got, married, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 88 with text: \n",
            " \t\t[[CLS], th, ##a, -, [SEP], and, they, ', re, going, to, notice, that, the, cooking, is, different,\n",
            "\t\t[SEP], well, that, ', s, what, even, *, *, r, ##uth, ##ie, *, *, says, you, know, [SEP], everyone,\n",
            "\t\tthinks, that, the, e, ##con, ##omi, ##e, is, so, important, [SEP], but, there, ', s, a, lot, of,\n",
            "\t\tthings, that, are, very, important, of, running, the, kitchen, [SEP], and, we, ', ll, see, [SEP], %,\n",
            "\t\tuh, right, now, E, ##12, ##14, %, uh, [SEP], you, know, co, ##coa, [MASK], got, married, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 84. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] married that guy [SEP] [[CLS], [MASK], married, that, guy, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], [MASK], married, that, guy, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] not the israeli guy not the sephardic guy but no you 're kidding that little feminine kind of guy [SEP] no [SEP] but reme- [SEP] when i was there in october [MASK] had that guy from yerushalaim that guy [SEP] [[CLS], not, the, is, ##rae, ##li, guy, not, the, se, ##pha, ##rdi, ##c, guy, but, no, you, ', re, kidding, that, little, feminine, kind, of, guy, [SEP], no, [SEP], but, re, ##me, -, [SEP], when, i, was, there, in, o, ##ct, ##obe, ##r, [MASK], had, that, guy, from, yer, ##ush, ##ala, ##im, that, guy, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 55 with text: \n",
            " \t\t[[CLS], not, the, is, ##rae, ##li, guy, not, the, se, ##pha, ##rdi, ##c, guy, but, no, you, ', re,\n",
            "\t\tkidding, that, little, feminine, kind, of, guy, [SEP], no, [SEP], but, re, ##me, -, [SEP], when, i,\n",
            "\t\twas, there, in, o, ##ct, ##obe, ##r, [MASK], had, that, guy, from, yer, ##ush, ##ala, ##im, that,\n",
            "\t\tguy, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] 820.26_822.65_b: no [SEP] but %um it was over already around then [SEP] but they were still friends [SEP] and this guy was a here or something drawer [SEP] i think [MASK] family is romanian or something [SEP] [[CLS], yeah, [SEP], 82, ##0, ., 26, _, 82, ##2, ., 65, _, b, :, no, [SEP], but, %, um, it, was, over, already, around, then, [SEP], but, they, were, still, friends, [SEP], and, this, guy, was, a, here, or, something, drawer, [SEP], i, think, [MASK], family, is, r, ##oman, ##ian, or, something, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 54 with text: \n",
            " \t\t[[CLS], yeah, [SEP], 82, ##0, ., 26, _, 82, ##2, ., 65, _, b, :, no, [SEP], but, %, um, it, was,\n",
            "\t\tover, already, around, then, [SEP], but, they, were, still, friends, [SEP], and, this, guy, was, a,\n",
            "\t\there, or, something, drawer, [SEP], i, think, [MASK], family, is, r, ##oman, ##ian, or, something,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 45. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] anyway [SEP] yeah [SEP] it was very fast [SEP] and they got married [SEP] how old is [MASK] ? [SEP] [[CLS], anyway, [SEP], yeah, [SEP], it, was, very, fast, [SEP], and, they, got, married, [SEP], how, old, is, [MASK], ?, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], anyway, [SEP], yeah, [SEP], it, was, very, fast, [SEP], and, they, got, married, [SEP], how,\n",
            "\t\told, is, [MASK], ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's eighteen and a half [SEP] [[CLS], [MASK], ', s, eighteen, and, a, half, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], ', s, eighteen, and, a, half, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 'll be nineteen in december [SEP] [[CLS], [MASK], ', ll, be, nineteen, in, de, ##ce, ##mber, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], [MASK], ', ll, be, nineteen, in, de, ##ce, ##mber, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] got engaged like before pesach [SEP] [[CLS], but, [MASK], got, engaged, like, before, p, ##es, ##ach, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], but, [MASK], got, engaged, like, before, p, ##es, ##ach, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] first of all we rent tables and chairs [SEP] yeah [SEP] so it 's like [SEP] and we did it on the grass outside the [SEP] and it was beautiful [SEP] we read [SEP] that [SEP] so [MASK] 's like the first child who got married [SEP] [[CLS], first, of, all, we, rent, tables, and, chairs, [SEP], yeah, [SEP], so, it, ', s, like, [SEP], and, we, did, it, on, the, grass, outside, the, [SEP], and, it, was, beautiful, [SEP], we, read, [SEP], that, [SEP], so, [MASK], ', s, like, the, first, child, who, got, married, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], first, of, all, we, rent, tables, and, chairs, [SEP], yeah, [SEP], so, it, ', s, like,\n",
            "\t\t[SEP], and, we, did, it, on, the, grass, outside, the, [SEP], and, it, was, beautiful, [SEP], we,\n",
            "\t\tread, [SEP], that, [SEP], so, [MASK], ', s, like, the, first, child, who, got, married, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] probably like the hard plastic [SEP] and it was amazing because they even had like a bar set up [SEP] and they had oh my go- fruit [SEP] they had hay [SEP] and then they had fruit [SEP] it was gorgeous [SEP] it was absolutely gorgeous [SEP] does [MASK] now have to do tzavah or no [SEP] [[CLS], probably, like, the, hard, plastic, [SEP], and, it, was, amazing, because, they, even, had, like, a, bar, set, up, [SEP], and, they, had, oh, my, go, -, fruit, [SEP], they, had, hay, [SEP], and, then, they, had, fruit, [SEP], it, was, gorgeous, [SEP], it, was, absolutely, gorgeous, [SEP], does, [MASK], now, have, to, do, t, ##za, ##va, ##h, or, no, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], probably, like, the, hard, plastic, [SEP], and, it, was, amazing, because, they, even, had,\n",
            "\t\tlike, a, bar, set, up, [SEP], and, they, had, oh, my, go, -, fruit, [SEP], they, had, hay, [SEP],\n",
            "\t\tand, then, they, had, fruit, [SEP], it, was, gorgeous, [SEP], it, was, absolutely, gorgeous, [SEP],\n",
            "\t\tdoes, [MASK], now, have, to, do, t, ##za, ##va, ##h, or, no, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no no [SEP] but %ah marriage is [SEP] you do n't have to do the tzavah [SEP] oh my go- [SEP] so [MASK] 's working now in the vegetarian kitchen and the a little bit [SEP] [[CLS], no, no, [SEP], but, %, ah, marriage, is, [SEP], you, do, n, ', t, have, to, do, the, t, ##za, ##va, ##h, [SEP], oh, my, go, -, [SEP], so, [MASK], ', s, working, now, in, the, ve, ##get, ##arian, kitchen, and, the, a, little, bit, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], no, no, [SEP], but, %, ah, marriage, is, [SEP], you, do, n, ', t, have, to, do, the, t,\n",
            "\t\t##za, ##va, ##h, [SEP], oh, my, go, -, [SEP], so, [MASK], ', s, working, now, in, the, ve, ##get,\n",
            "\t\t##arian, kitchen, and, the, a, little, bit, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 30. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 899it [00:07, 180.94it/s][CLS] but [MASK] 's going to study in september [SEP] [[CLS], but, [MASK], ', s, going, to, study, in, se, ##pt, ##em, ##ber, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], but, [MASK], ', s, going, to, study, in, se, ##pt, ##em, ##ber, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no [SEP] oh i keep [SEP] that other guy was real [SEP] this one is [SEP] i can not bel- [SEP] oh this other one 's like from a family that 's not at all [SEP] but [MASK] [SEP] [[CLS], no, [SEP], oh, i, keep, [SEP], that, other, guy, was, real, [SEP], this, one, is, [SEP], i, can, not, be, ##l, -, [SEP], oh, this, other, one, ', s, like, from, a, family, that, ', s, not, at, all, [SEP], but, [MASK], [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], no, [SEP], oh, i, keep, [SEP], that, other, guy, was, real, [SEP], this, one, is, [SEP], i,\n",
            "\t\tcan, not, be, ##l, -, [SEP], oh, this, other, one, ', s, like, from, a, family, that, ', s, not, at,\n",
            "\t\tall, [SEP], but, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 42. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] they went to america for a month after the wedding [SEP] [MASK] grandparents sent them %uh oh my even though they were mad [SEP] [[CLS], yeah, [SEP], they, went, to, am, ##eric, ##a, for, a, month, after, the, wedding, [SEP], [MASK], grandparents, sent, them, %, uh, oh, my, even, though, they, were, mad, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 30 with text: \n",
            " \t\t[[CLS], yeah, [SEP], they, went, to, am, ##eric, ##a, for, a, month, after, the, wedding, [SEP],\n",
            "\t\t[MASK], grandparents, sent, them, %, uh, oh, my, even, though, they, were, mad, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 16. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] wait [SEP] yeah [SEP] natan 's sister was here also [SEP] so they were all upset that the grandparents did n't come back for the wedding [SEP] but i do n't know [SEP] you have to look at you know both [SEP] oh my [SEP] i ca n't believe [MASK] got married [SEP] [[CLS], wait, [SEP], yeah, [SEP], na, ##tan, ', s, sister, was, here, also, [SEP], so, they, were, all, upset, that, the, grandparents, did, n, ', t, come, back, for, the, wedding, [SEP], but, i, do, n, ', t, know, [SEP], you, have, to, look, at, you, know, both, [SEP], oh, my, [SEP], i, ca, n, ', t, believe, [MASK], got, married, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], wait, [SEP], yeah, [SEP], na, ##tan, ', s, sister, was, here, also, [SEP], so, they, were,\n",
            "\t\tall, upset, that, the, grandparents, did, n, ', t, come, back, for, the, wedding, [SEP], but, i, do,\n",
            "\t\tn, ', t, know, [SEP], you, have, to, look, at, you, know, both, [SEP], oh, my, [SEP], i, ca, n, ',\n",
            "\t\tt, believe, [MASK], got, married, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 58. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i [SEP] [MASK] got married [SEP] [[CLS], i, [SEP], [MASK], got, married, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], i, [SEP], [MASK], got, married, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and then the week after ta'al paer got married [SEP] you know ta'al is the oldest %uh girl on the [SEP] i [SEP] yeah [SEP] where is [MASK] living [SEP] [[CLS], and, then, the, week, after, ta, ', al, p, ##ae, ##r, got, married, [SEP], you, know, ta, ', al, is, the, oldest, %, uh, girl, on, the, [SEP], i, [SEP], yeah, [SEP], where, is, [MASK], living, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], and, then, the, week, after, ta, ', al, p, ##ae, ##r, got, married, [SEP], you, know, ta, ',\n",
            "\t\tal, is, the, oldest, %, uh, girl, on, the, [SEP], i, [SEP], yeah, [SEP], where, is, [MASK], living,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 35. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so [MASK] looks really young [SEP] [[CLS], so, [MASK], looks, really, young, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], so, [MASK], looks, really, young, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %uh [MASK] just finished now the army [SEP] [[CLS], %, uh, [MASK], just, finished, now, the, army, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], %, uh, [MASK], just, finished, now, the, army, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's from a nonreligious kibbutz [SEP] [[CLS], [MASK], ', s, from, a, non, ##rel, ##igi, ##ous, k, ##ib, ##but, ##z, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], [MASK], ', s, from, a, non, ##rel, ##igi, ##ous, k, ##ib, ##but, ##z, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh my gosh [SEP] yeah [SEP] so now they 're living on [MASK] kibbutz %um because they allow like b'nai to live there [SEP] [[CLS], oh, my, go, ##sh, [SEP], yeah, [SEP], so, now, they, ', re, living, on, [MASK], k, ##ib, ##but, ##z, %, um, because, they, allow, like, b, ', na, ##i, to, live, there, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], oh, my, go, ##sh, [SEP], yeah, [SEP], so, now, they, ', re, living, on, [MASK], k, ##ib,\n",
            "\t\t##but, ##z, %, um, because, they, allow, like, b, ', na, ##i, to, live, there, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] gosh you would think at least in E2829 you would get what you want [SEP] no [SEP] but not only that [SEP] what [SEP] this kid just left the kibbutz at eighteen [SEP] and now at nineteen [MASK] 's coming back with &uh you know like [SEP] [[CLS], go, ##sh, you, would, think, at, least, in, E, ##28, ##29, you, would, get, what, you, want, [SEP], no, [SEP], but, not, only, that, [SEP], what, [SEP], this, kid, just, left, the, k, ##ib, ##but, ##z, at, eighteen, [SEP], and, now, at, nineteen, [MASK], ', s, coming, back, with, &, uh, you, know, like, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 56 with text: \n",
            " \t\t[[CLS], go, ##sh, you, would, think, at, least, in, E, ##28, ##29, you, would, get, what, you, want,\n",
            "\t\t[SEP], no, [SEP], but, not, only, that, [SEP], what, [SEP], this, kid, just, left, the, k, ##ib,\n",
            "\t\t##but, ##z, at, eighteen, [SEP], and, now, at, nineteen, [MASK], ', s, coming, back, with, &, uh,\n",
            "\t\tyou, know, like, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 44. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i do n't know why [SEP] that was ridiculous because everyone was just as happy [SEP] but [MASK] band they got stuck somewhere [SEP] [[CLS], i, do, n, ', t, know, why, [SEP], that, was, ridiculous, because, everyone, was, just, as, happy, [SEP], but, [MASK], band, they, got, stuck, somewhere, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 27 with text: \n",
            " \t\t[[CLS], i, do, n, ', t, know, why, [SEP], that, was, ridiculous, because, everyone, was, just, as,\n",
            "\t\thappy, [SEP], but, [MASK], band, they, got, stuck, somewhere, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 20. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and their %um [SEP] something with the electric equipment [SEP] it did n't come [SEP] so we had to get from us the amplifiers [SEP] and ours are terrible [SEP] %mm oy [SEP] they could n't hear anything [SEP] so so some people got really you know [SEP] just they did n't dance [SEP] and [MASK] was really upset [SEP] [[CLS], and, their, %, um, [SEP], something, with, the, electric, equipment, [SEP], it, did, n, ', t, come, [SEP], so, we, had, to, get, from, us, the, amplifier, ##s, [SEP], and, ours, are, terrible, [SEP], %, mm, o, ##y, [SEP], they, could, n, ', t, hear, anything, [SEP], so, so, some, people, got, really, you, know, [SEP], just, they, did, n, ', t, dance, [SEP], and, [MASK], was, really, upset, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 71 with text: \n",
            " \t\t[[CLS], and, their, %, um, [SEP], something, with, the, electric, equipment, [SEP], it, did, n, ',\n",
            "\t\tt, come, [SEP], so, we, had, to, get, from, us, the, amplifier, ##s, [SEP], and, ours, are,\n",
            "\t\tterrible, [SEP], %, mm, o, ##y, [SEP], they, could, n, ', t, hear, anything, [SEP], so, so, some,\n",
            "\t\tpeople, got, really, you, know, [SEP], just, they, did, n, ', t, dance, [SEP], and, [MASK], was,\n",
            "\t\treally, upset, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 66. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well that whole class that whole first [SEP] no [SEP] but there was only those two girls ta'al from the first class [SEP] E7332 from the second [SEP] now these other kids just finished high school [SEP] like & 's oldest daughter is going to the army next week or two weeks [SEP] wow [SEP] but does n't [MASK] have a son who 's older [SEP] [[CLS], well, that, whole, class, that, whole, first, [SEP], no, [SEP], but, there, was, only, those, two, girls, ta, ', al, from, the, first, class, [SEP], E, ##7, ##33, ##2, from, the, second, [SEP], now, these, other, kids, just, finished, high, school, [SEP], like, &, ', s, oldest, daughter, is, going, to, the, army, next, week, or, two, weeks, [SEP], w, ##ow, [SEP], but, does, n, ', t, [MASK], have, a, son, who, ', s, older, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 77 with text: \n",
            " \t\t[[CLS], well, that, whole, class, that, whole, first, [SEP], no, [SEP], but, there, was, only,\n",
            "\t\tthose, two, girls, ta, ', al, from, the, first, class, [SEP], E, ##7, ##33, ##2, from, the, second,\n",
            "\t\t[SEP], now, these, other, kids, just, finished, high, school, [SEP], like, &, ', s, oldest,\n",
            "\t\tdaughter, is, going, to, the, army, next, week, or, two, weeks, [SEP], w, ##ow, [SEP], but, does, n,\n",
            "\t\t', t, [MASK], have, a, son, who, ', s, older, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 68. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] [MASK] son is still in the army [SEP] [[CLS], yeah, [SEP], [MASK], son, is, still, in, the, army, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], yeah, [SEP], [MASK], son, is, still, in, the, army, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh [SEP] no [SEP] but [MASK] 's not engaged yet [SEP] [[CLS], uh, -, huh, [SEP], no, [SEP], but, [MASK], ', s, not, engaged, yet, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], uh, -, huh, [SEP], no, [SEP], but, [MASK], ', s, not, engaged, yet, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] there 's not necessarily any benefit to it either . [SEP] so [SEP] %hm . [SEP] sorry i was talking about the kids ' school . [SEP] right [SEP] the kids . yeah [SEP] r j told me how much -- yeah . -- it costs . [SEP] but meanwhile -- it 's a lot of money . -- [MASK] does n't know how much you pay . [SEP] [[CLS], there, ', s, not, necessarily, any, benefit, to, it, either, ., [SEP], so, [SEP], %, h, ##m, ., [SEP], sorry, i, was, talking, about, the, kids, ', school, ., [SEP], right, [SEP], the, kids, ., yeah, [SEP], r, j, told, me, how, much, -, -, yeah, ., -, -, it, costs, ., [SEP], but, meanwhile, -, -, it, ', s, a, lot, of, money, ., -, -, [MASK], does, n, ', t, know, how, much, you, pay, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 80 with text: \n",
            " \t\t[[CLS], there, ', s, not, necessarily, any, benefit, to, it, either, ., [SEP], so, [SEP], %, h, ##m,\n",
            "\t\t., [SEP], sorry, i, was, talking, about, the, kids, ', school, ., [SEP], right, [SEP], the, kids, .,\n",
            "\t\tyeah, [SEP], r, j, told, me, how, much, -, -, yeah, ., -, -, it, costs, ., [SEP], but, meanwhile, -,\n",
            "\t\t-, it, ', s, a, lot, of, money, ., -, -, [MASK], does, n, ', t, know, how, much, you, pay, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 68. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's a very nice place . [SEP] and i spoke to someone the other day %uh %uh about you . [SEP] oh yeah ? [SEP] plenty of jobs . [SEP] are you serious ? [SEP] because you know what the problem is [SEP] there 's a guy who 's working with me now who 's an israeli [SEP] and [MASK] 's active . [SEP] [[CLS], it, ', s, a, very, nice, place, ., [SEP], and, i, spoke, to, someone, the, other, day, %, uh, %, uh, about, you, ., [SEP], oh, yeah, ?, [SEP], plenty, of, jobs, ., [SEP], are, you, serious, ?, [SEP], because, you, know, what, the, problem, is, [SEP], there, ', s, a, guy, who, ', s, working, with, me, now, who, ', s, an, is, ##rae, ##li, [SEP], and, [MASK], ', s, active, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 75 with text: \n",
            " \t\t[[CLS], it, ', s, a, very, nice, place, ., [SEP], and, i, spoke, to, someone, the, other, day, %,\n",
            "\t\tuh, %, uh, about, you, ., [SEP], oh, yeah, ?, [SEP], plenty, of, jobs, ., [SEP], are, you, serious,\n",
            "\t\t?, [SEP], because, you, know, what, the, problem, is, [SEP], there, ', s, a, guy, who, ', s,\n",
            "\t\tworking, with, me, now, who, ', s, an, is, ##rae, ##li, [SEP], and, [MASK], ', s, active, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 69. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] do n't believe them . [SEP] what are they all liars ? [SEP] i mean what 's the story here ? [SEP] you 're i- [SEP] yes . [SEP] first of all israeli who is living in america -- yeah . -- has to justify himself . [SEP] so that 's what [MASK] 's going to do . [SEP] [[CLS], do, n, ', t, believe, them, ., [SEP], what, are, they, all, liar, ##s, ?, [SEP], i, mean, what, ', s, the, story, here, ?, [SEP], you, ', re, i, -, [SEP], yes, ., [SEP], first, of, all, is, ##rae, ##li, who, is, living, in, am, ##eric, ##a, -, -, yeah, ., -, -, has, to, justify, himself, ., [SEP], so, that, ', s, what, [MASK], ', s, going, to, do, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 74 with text: \n",
            " \t\t[[CLS], do, n, ', t, believe, them, ., [SEP], what, are, they, all, liar, ##s, ?, [SEP], i, mean,\n",
            "\t\twhat, ', s, the, story, here, ?, [SEP], you, ', re, i, -, [SEP], yes, ., [SEP], first, of, all, is,\n",
            "\t\t##rae, ##li, who, is, living, in, am, ##eric, ##a, -, -, yeah, ., -, -, has, to, justify, himself,\n",
            "\t\t., [SEP], so, that, ', s, what, [MASK], ', s, going, to, do, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 66. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 918it [00:07, 182.52it/s][CLS] i suppose so [SEP] because man [MASK] 's just like one thing after the next . [SEP] [[CLS], i, suppose, so, [SEP], because, man, [MASK], ', s, just, like, one, thing, after, the, next, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], i, suppose, so, [SEP], because, man, [MASK], ', s, just, like, one, thing, after, the, next,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you 're into software right ? [SEP] yeah . [SEP] you know that guy E4750 %uh go- [SEP] %uh %uh i do n't want to mention [MASK] name . [SEP] [[CLS], you, ', re, into, software, right, ?, [SEP], yeah, ., [SEP], you, know, that, guy, E, ##47, ##50, %, uh, go, -, [SEP], %, uh, %, uh, i, do, n, ', t, want, to, mention, [MASK], name, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 40 with text: \n",
            " \t\t[[CLS], you, ', re, into, software, right, ?, [SEP], yeah, ., [SEP], you, know, that, guy, E, ##47,\n",
            "\t\t##50, %, uh, go, -, [SEP], %, uh, %, uh, i, do, n, ', t, want, to, mention, [MASK], name, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E4153 E4750 ? [SEP] no . [SEP] oh [SEP] oh i know [SEP] the other guy who au- [SEP] right [SEP] yeah [SEP] the [SEP] ri- [SEP] mhm [SEP] that [MASK] said there 's absolutely no problem . [SEP] [[CLS], E, ##41, ##53, E, ##47, ##50, ?, [SEP], no, ., [SEP], oh, [SEP], oh, i, know, [SEP], the, other, guy, who, au, -, [SEP], right, [SEP], yeah, [SEP], the, [SEP], r, ##i, -, [SEP], m, ##hm, [SEP], that, [MASK], said, there, ', s, absolutely, no, problem, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 49 with text: \n",
            " \t\t[[CLS], E, ##41, ##53, E, ##47, ##50, ?, [SEP], no, ., [SEP], oh, [SEP], oh, i, know, [SEP], the,\n",
            "\t\tother, guy, who, au, -, [SEP], right, [SEP], yeah, [SEP], the, [SEP], r, ##i, -, [SEP], m, ##hm,\n",
            "\t\t[SEP], that, [MASK], said, there, ', s, absolutely, no, problem, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no problem . [SEP] %hm %hm . [SEP] [MASK] said any kind of job at a very very high salary . %uh with %uh the experience that %uh you have is absolutely no problem . [SEP] [[CLS], no, problem, ., [SEP], %, h, ##m, %, h, ##m, ., [SEP], [MASK], said, any, kind, of, job, at, a, very, very, high, salary, ., %, uh, with, %, uh, the, experience, that, %, uh, you, have, is, absolutely, no, problem, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], no, problem, ., [SEP], %, h, ##m, %, h, ##m, ., [SEP], [MASK], said, any, kind, of, job, at,\n",
            "\t\ta, very, very, high, salary, ., %, uh, with, %, uh, the, experience, that, %, uh, you, have, is,\n",
            "\t\tabsolutely, no, problem, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] so what am i waiting for ? [SEP] do you be b- [SEP] %huh ? [SEP] so what am i waiting for ? [SEP] where does [MASK] live [SEP] [[CLS], yeah, [SEP], so, what, am, i, waiting, for, ?, [SEP], do, you, be, b, -, [SEP], %, huh, ?, [SEP], so, what, am, i, waiting, for, ?, [SEP], where, does, [MASK], live, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], yeah, [SEP], so, what, am, i, waiting, for, ?, [SEP], do, you, be, b, -, [SEP], %, huh, ?,\n",
            "\t\t[SEP], so, what, am, i, waiting, for, ?, [SEP], where, does, [MASK], live, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 31. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] right [SEP] tel aviv hertzolia [SEP] even now they opened up a lot of new roads now they were building since you were here . [SEP] uh-huh [SEP] but %ah they were building while you were here . [SEP] right . [SEP] %um and you can also [SEP] E6099 said [MASK] got to tel aviv the other day in like forty - five minutes [SEP] [[CLS], right, [SEP], te, ##l, a, ##vi, ##v, her, ##tz, ##olia, [SEP], even, now, they, opened, up, a, lot, of, new, roads, now, they, were, building, since, you, were, here, ., [SEP], uh, -, huh, [SEP], but, %, ah, they, were, building, while, you, were, here, ., [SEP], right, ., [SEP], %, um, and, you, can, also, [SEP], E, ##60, ##9, ##9, said, [MASK], got, to, te, ##l, a, ##vi, ##v, the, other, day, in, like, forty, -, five, minutes, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 81 with text: \n",
            " \t\t[[CLS], right, [SEP], te, ##l, a, ##vi, ##v, her, ##tz, ##olia, [SEP], even, now, they, opened, up,\n",
            "\t\ta, lot, of, new, roads, now, they, were, building, since, you, were, here, ., [SEP], uh, -, huh,\n",
            "\t\t[SEP], but, %, ah, they, were, building, while, you, were, here, ., [SEP], right, ., [SEP], %, um,\n",
            "\t\tand, you, can, also, [SEP], E, ##60, ##9, ##9, said, [MASK], got, to, te, ##l, a, ##vi, ##v, the,\n",
            "\t\tother, day, in, like, forty, -, five, minutes, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 63. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] wow . with the traffic and everything ? [SEP] with yeah the traffic [SEP] there was no traffic until [MASK] got right there to %uh yeah tel aviv [SEP] [[CLS], w, ##ow, ., with, the, traffic, and, everything, ?, [SEP], with, yeah, the, traffic, [SEP], there, was, no, traffic, until, [MASK], got, right, there, to, %, uh, yeah, te, ##l, a, ##vi, ##v, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 35 with text: \n",
            " \t\t[[CLS], w, ##ow, ., with, the, traffic, and, everything, ?, [SEP], with, yeah, the, traffic, [SEP],\n",
            "\t\tthere, was, no, traffic, until, [MASK], got, right, there, to, %, uh, yeah, te, ##l, a, ##vi, ##v,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 21. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %mm %mm %mm [SEP] so yeah [SEP] it 's a [SEP] but you 're talking about jobs in good places good solid companies developing companies who %um [SEP] the thing just for the fun of it then %uh [SEP] you know how i feel that [SEP] yeah [SEP] little E2992 has [MASK] own reasons for you know being in america . [SEP] [[CLS], %, mm, %, mm, %, mm, [SEP], so, yeah, [SEP], it, ', s, a, [SEP], but, you, ', re, talking, about, jobs, in, good, places, good, solid, companies, developing, companies, who, %, um, [SEP], the, thing, just, for, the, fun, of, it, then, %, uh, [SEP], you, know, how, i, feel, that, [SEP], yeah, [SEP], little, E, ##29, ##9, ##2, has, [MASK], own, reasons, for, you, know, being, in, am, ##eric, ##a, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 75 with text: \n",
            " \t\t[[CLS], %, mm, %, mm, %, mm, [SEP], so, yeah, [SEP], it, ', s, a, [SEP], but, you, ', re, talking,\n",
            "\t\tabout, jobs, in, good, places, good, solid, companies, developing, companies, who, %, um, [SEP],\n",
            "\t\tthe, thing, just, for, the, fun, of, it, then, %, uh, [SEP], you, know, how, i, feel, that, [SEP],\n",
            "\t\tyeah, [SEP], little, E, ##29, ##9, ##2, has, [MASK], own, reasons, for, you, know, being, in, am,\n",
            "\t\t##eric, ##a, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 62. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but i mean i could say that for the next five or even eight or maybe -- yeah . -- ten years alright [SEP] even if i could project that far which i really ca n't [SEP] yeah [SEP] but i mean you got a kid you ca n't do fucking ten years w- -- [SEP] i know . [SEP] -- it 's like cool i think i 'll go off to the orient and start like bouncing from country country . [SEP] i know [SEP] well and i mean okay E4574 wants to get [MASK] doctorate [SEP] [[CLS], but, i, mean, i, could, say, that, for, the, next, five, or, even, eight, or, maybe, -, -, yeah, ., -, -, ten, years, alright, [SEP], even, if, i, could, project, that, far, which, i, really, ca, n, ', t, [SEP], yeah, [SEP], but, i, mean, you, got, a, kid, you, ca, n, ', t, do, fucking, ten, years, w, -, -, -, [SEP], i, know, ., [SEP], -, -, it, ', s, like, cool, i, think, i, ', ll, go, off, to, the, or, ##ient, and, start, like, bouncing, from, country, country, ., [SEP], i, know, [SEP], well, and, i, mean, okay, E, ##45, ##7, ##4, wants, to, get, [MASK], doctorate, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 114 with text: \n",
            " \t\t[[CLS], but, i, mean, i, could, say, that, for, the, next, five, or, even, eight, or, maybe, -, -,\n",
            "\t\tyeah, ., -, -, ten, years, alright, [SEP], even, if, i, could, project, that, far, which, i, really,\n",
            "\t\tca, n, ', t, [SEP], yeah, [SEP], but, i, mean, you, got, a, kid, you, ca, n, ', t, do, fucking, ten,\n",
            "\t\tyears, w, -, -, -, [SEP], i, know, ., [SEP], -, -, it, ', s, like, cool, i, think, i, ', ll, go,\n",
            "\t\toff, to, the, or, ##ient, and, start, like, bouncing, from, country, country, ., [SEP], i, know,\n",
            "\t\t[SEP], well, and, i, mean, okay, E, ##45, ##7, ##4, wants, to, get, [MASK], doctorate, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 111. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so we 'll know next spring if [MASK] can get at least accepted into the program which hopefully -- %mm . -- you know [SEP] [[CLS], so, we, ', ll, know, next, spring, if, [MASK], can, get, at, least, accepted, into, the, program, which, hopefully, -, -, %, mm, ., -, -, you, know, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 30 with text: \n",
            " \t\t[[CLS], so, we, ', ll, know, next, spring, if, [MASK], can, get, at, least, accepted, into, the,\n",
            "\t\tprogram, which, hopefully, -, -, %, mm, ., -, -, you, know, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and then there 's four or five years doing the damn work [SEP] and %mm . [MASK] 's crazy [SEP] [[CLS], and, then, there, ', s, four, or, five, years, doing, the, damn, work, [SEP], and, %, mm, ., [MASK], ', s, crazy, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], and, then, there, ', s, four, or, five, years, doing, the, damn, work, [SEP], and, %, mm, .,\n",
            "\t\t[MASK], ', s, crazy, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] wants it [SEP] [[CLS], [MASK], wants, it, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 5 with text: \n",
            " \t\t[[CLS], [MASK], wants, it, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] that 's fine [SEP] [MASK] can do it [SEP] [[CLS], that, ', s, fine, [SEP], [MASK], can, do, it, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], that, ', s, fine, [SEP], [MASK], can, do, it, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no problem [SEP] but %um that sort of ties us up here although we like the place [SEP] we 're in no hurry to leave it [SEP] yeah . [SEP] and that would be good [SEP] but then you know what if [MASK] could get a position overseas again which is something both of us have said would be pretty darn good because -- %mm . -- %um you know after coming back from spain from those two and half years it was n't so much an adjustment to the way things were [SEP] [[CLS], no, problem, [SEP], but, %, um, that, sort, of, ties, us, up, here, although, we, like, the, place, [SEP], we, ', re, in, no, hurry, to, leave, it, [SEP], yeah, ., [SEP], and, that, would, be, good, [SEP], but, then, you, know, what, if, [MASK], could, get, a, position, overseas, again, which, is, something, both, of, us, have, said, would, be, pretty, da, ##rn, good, because, -, -, %, mm, ., -, -, %, um, you, know, after, coming, back, from, spa, ##in, from, those, two, and, half, years, it, was, n, ', t, so, much, an, adjustment, to, the, way, things, were, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], no, problem, [SEP], but, %, um, that, sort, of, ties, us, up, here, although, we, like, the,\n",
            "\t\tplace, [SEP], we, ', re, in, no, hurry, to, leave, it, [SEP], yeah, ., [SEP], and, that, would, be,\n",
            "\t\tgood, [SEP], but, then, you, know, what, if, [MASK], could, get, a, position, overseas, again,\n",
            "\t\twhich, is, something, both, of, us, have, said, would, be, pretty, da, ##rn, good, because, -, -, %,\n",
            "\t\tmm, ., -, -, %, um, you, know, after, coming, back, from, spa, ##in, from, those, two, and, half,\n",
            "\t\tyears, it, was, n, ', t, so, much, an, adjustment, to, the, way, things, were, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 45. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's just asking the question gee do we really like %um the way things are over here with the lifestyle the nine to five kind of thing [SEP] i mean i 'm really in the nine to five thing more so than [MASK] is [SEP] [[CLS], it, ', s, just, asking, the, question, g, ##ee, do, we, really, like, %, um, the, way, things, are, over, here, with, the, lifestyle, the, nine, to, five, kind, of, thing, [SEP], i, mean, i, ', m, really, in, the, nine, to, five, thing, more, so, than, [MASK], is, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 51 with text: \n",
            " \t\t[[CLS], it, ', s, just, asking, the, question, g, ##ee, do, we, really, like, %, um, the, way,\n",
            "\t\tthings, are, over, here, with, the, lifestyle, the, nine, to, five, kind, of, thing, [SEP], i, mean,\n",
            "\t\ti, ', m, really, in, the, nine, to, five, thing, more, so, than, [MASK], is, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 48. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know [MASK] should be there by oh oh well let 's say eight thirty to nine [SEP] [[CLS], you, know, [MASK], should, be, there, by, oh, oh, well, let, ', s, say, eight, thirty, to, nine, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], you, know, [MASK], should, be, there, by, oh, oh, well, let, ', s, say, eight, thirty, to,\n",
            "\t\tnine, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but yeah [SEP] [MASK] does n't care when [SEP] [[CLS], but, yeah, [SEP], [MASK], does, n, ', t, care, when, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], but, yeah, [SEP], [MASK], does, n, ', t, care, when, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well okay [SEP] %um and [MASK] professor is obviously %uh %uh a great person to work for [SEP] [[CLS], well, okay, [SEP], %, um, and, [MASK], professor, is, obviously, %, uh, %, uh, a, great, person, to, work, for, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], well, okay, [SEP], %, um, and, [MASK], professor, is, obviously, %, uh, %, uh, a, great,\n",
            "\t\tperson, to, work, for, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] what you have to do is concentrate on the advantages in what ever place you 're in -- mhm -- and think about the disadvantages of the place that you 're not in . [SEP] yeah [SEP] otherwise you go nuts right ? [SEP] that 's true [SEP] yeah . [SEP] yeah . [SEP] i know E3496 tried to [SEP] %uh because we have email and so [MASK] sent me a letter back saying you know remember how crowded the transportation is and %uh what else -- %mm . how expensive everything is [SEP] [[CLS], what, you, have, to, do, is, concentrate, on, the, advantages, in, what, ever, place, you, ', re, in, -, -, m, ##hm, -, -, and, think, about, the, disadvantage, ##s, of, the, place, that, you, ', re, not, in, ., [SEP], yeah, [SEP], otherwise, you, go, nuts, right, ?, [SEP], that, ', s, true, [SEP], yeah, ., [SEP], yeah, ., [SEP], i, know, E, ##34, ##9, ##6, tried, to, [SEP], %, uh, because, we, have, email, and, so, [MASK], sent, me, a, letter, back, saying, you, know, remember, how, crowded, the, transportation, is, and, %, uh, what, else, -, -, %, mm, ., how, expensive, everything, is, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 109 with text: \n",
            " \t\t[[CLS], what, you, have, to, do, is, concentrate, on, the, advantages, in, what, ever, place, you,\n",
            "\t\t', re, in, -, -, m, ##hm, -, -, and, think, about, the, disadvantage, ##s, of, the, place, that,\n",
            "\t\tyou, ', re, not, in, ., [SEP], yeah, [SEP], otherwise, you, go, nuts, right, ?, [SEP], that, ', s,\n",
            "\t\ttrue, [SEP], yeah, ., [SEP], yeah, ., [SEP], i, know, E, ##34, ##9, ##6, tried, to, [SEP], %, uh,\n",
            "\t\tbecause, we, have, email, and, so, [MASK], sent, me, a, letter, back, saying, you, know, remember,\n",
            "\t\thow, crowded, the, transportation, is, and, %, uh, what, else, -, -, %, mm, ., how, expensive,\n",
            "\t\teverything, is, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 79. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 937it [00:07, 181.56it/s][CLS] well you know and that 's why it helps to have kind of a glimpse into the future right ? [SEP] and that 's how we got to -- %mm . -- spain in the first place is we said we wanted to live abroad and we would try to do it you know as teachers [SEP] and it turned out that way because -- %mm . -- we had this idea of [SEP] and i really feel that that 's important [SEP] and so what i 'm having an idea of now is [SEP] okay you know E4574 works on [MASK] doctorate [SEP] [[CLS], well, you, know, and, that, ', s, why, it, helps, to, have, kind, of, a, glimpse, into, the, future, right, ?, [SEP], and, that, ', s, how, we, got, to, -, -, %, mm, ., -, -, spa, ##in, in, the, first, place, is, we, said, we, wanted, to, live, abroad, and, we, would, try, to, do, it, you, know, as, teachers, [SEP], and, it, turned, out, that, way, because, -, -, %, mm, ., -, -, we, had, this, idea, of, [SEP], and, i, really, feel, that, that, ', s, important, [SEP], and, so, what, i, ', m, having, an, idea, of, now, is, [SEP], okay, you, know, E, ##45, ##7, ##4, works, on, [MASK], doctorate, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 119 with text: \n",
            " \t\t[[CLS], well, you, know, and, that, ', s, why, it, helps, to, have, kind, of, a, glimpse, into, the,\n",
            "\t\tfuture, right, ?, [SEP], and, that, ', s, how, we, got, to, -, -, %, mm, ., -, -, spa, ##in, in,\n",
            "\t\tthe, first, place, is, we, said, we, wanted, to, live, abroad, and, we, would, try, to, do, it, you,\n",
            "\t\tknow, as, teachers, [SEP], and, it, turned, out, that, way, because, -, -, %, mm, ., -, -, we, had,\n",
            "\t\tthis, idea, of, [SEP], and, i, really, feel, that, that, ', s, important, [SEP], and, so, what, i,\n",
            "\t\t', m, having, an, idea, of, now, is, [SEP], okay, you, know, E, ##45, ##7, ##4, works, on, [MASK],\n",
            "\t\tdoctorate, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 116. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it takes about five or six years [SEP] we have the kid probably [SEP] [MASK] gets a research position somewhere in E6216 or in spain %um [SEP] [[CLS], it, takes, about, five, or, six, years, [SEP], we, have, the, kid, probably, [SEP], [MASK], gets, a, research, position, somewhere, in, E, ##6, ##21, ##6, or, in, spa, ##in, %, um, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], it, takes, about, five, or, six, years, [SEP], we, have, the, kid, probably, [SEP], [MASK],\n",
            "\t\tgets, a, research, position, somewhere, in, E, ##6, ##21, ##6, or, in, spa, ##in, %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] actually it 's on the other side of leotana %huh . like down over by the bornillo [SEP] okay . [SEP] and i 've only eaten there once alright [SEP] but i 've been in a million times because every time i pass with somebody i have to show them the place . [SEP] uh-huh . [SEP] actually you know i know where i 'm going to go [SEP] i have a friend my friend from england [SEP] i do n't remember if i told you about [MASK] [SEP] [[CLS], actually, it, ', s, on, the, other, side, of, le, ##ota, ##na, %, huh, ., like, down, over, by, the, born, ##illo, [SEP], okay, ., [SEP], and, i, ', ve, only, eaten, there, once, alright, [SEP], but, i, ', ve, been, in, a, million, times, because, every, time, i, pass, with, somebody, i, have, to, show, them, the, place, ., [SEP], uh, -, huh, ., [SEP], actually, you, know, i, know, where, i, ', m, going, to, go, [SEP], i, have, a, friend, my, friend, from, en, ##g, ##land, [SEP], i, do, n, ', t, remember, if, i, told, you, about, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 104 with text: \n",
            " \t\t[[CLS], actually, it, ', s, on, the, other, side, of, le, ##ota, ##na, %, huh, ., like, down, over,\n",
            "\t\tby, the, born, ##illo, [SEP], okay, ., [SEP], and, i, ', ve, only, eaten, there, once, alright,\n",
            "\t\t[SEP], but, i, ', ve, been, in, a, million, times, because, every, time, i, pass, with, somebody, i,\n",
            "\t\thave, to, show, them, the, place, ., [SEP], uh, -, huh, ., [SEP], actually, you, know, i, know,\n",
            "\t\twhere, i, ', m, going, to, go, [SEP], i, have, a, friend, my, friend, from, en, ##g, ##land, [SEP],\n",
            "\t\ti, do, n, ', t, remember, if, i, told, you, about, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 102. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's my old %um roommate mhm . from oviato [SEP] [[CLS], [MASK], ', s, my, old, %, um, roommate, m, ##hm, ., from, o, ##via, ##to, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], [MASK], ', s, my, old, %, um, roommate, m, ##hm, ., from, o, ##via, ##to, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's coming [SEP] [[CLS], and, [MASK], ', s, coming, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, coming, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] got a job down here [SEP] [[CLS], [MASK], got, a, job, down, here, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], got, a, job, down, here, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's coming on friday to live [SEP] [[CLS], [MASK], ', s, coming, on, f, ##rida, ##y, to, live, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], ', s, coming, on, f, ##rida, ##y, to, live, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i did n't yet [SEP] i had a summer job [SEP] i worked [SEP] yeah [SEP] well i just finished work yesterday [SEP] it was a nine week course [SEP] it started in july and ended yesterday [SEP] [MASK] said [SEP] [[CLS], i, did, n, ', t, yet, [SEP], i, had, a, summer, job, [SEP], i, worked, [SEP], yeah, [SEP], well, i, just, finished, work, yesterday, [SEP], it, was, a, nine, week, course, [SEP], it, started, in, j, ##ul, ##y, and, ended, yesterday, [SEP], [MASK], said, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 46 with text: \n",
            " \t\t[[CLS], i, did, n, ', t, yet, [SEP], i, had, a, summer, job, [SEP], i, worked, [SEP], yeah, [SEP],\n",
            "\t\twell, i, just, finished, work, yesterday, [SEP], it, was, a, nine, week, course, [SEP], it, started,\n",
            "\t\tin, j, ##ul, ##y, and, ended, yesterday, [SEP], [MASK], said, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] that 's good [SEP] %eh it 's not too late that i called [SEP] no no no no not at all %eh [SEP] oh good [SEP] good [SEP] there 's something else i was in the middle [SEP] and i meant to do it weeks a- [SEP] i 'm sending a big information and letter to E1926 perez yeah because i heard [MASK] say on the radio weeks ago yeah that i mea- [SEP] [[CLS], that, ', s, good, [SEP], %, eh, it, ', s, not, too, late, that, i, called, [SEP], no, no, no, no, not, at, all, %, eh, [SEP], oh, good, [SEP], good, [SEP], there, ', s, something, else, i, was, in, the, middle, [SEP], and, i, meant, to, do, it, weeks, a, -, [SEP], i, ', m, sending, a, big, information, and, letter, to, E, ##19, ##26, per, ##ez, yeah, because, i, heard, [MASK], say, on, the, radio, weeks, ago, yeah, that, i, me, ##a, -, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 87 with text: \n",
            " \t\t[[CLS], that, ', s, good, [SEP], %, eh, it, ', s, not, too, late, that, i, called, [SEP], no, no,\n",
            "\t\tno, no, not, at, all, %, eh, [SEP], oh, good, [SEP], good, [SEP], there, ', s, something, else, i,\n",
            "\t\twas, in, the, middle, [SEP], and, i, meant, to, do, it, weeks, a, -, [SEP], i, ', m, sending, a,\n",
            "\t\tbig, information, and, letter, to, E, ##19, ##26, per, ##ez, yeah, because, i, heard, [MASK], say,\n",
            "\t\ton, the, radio, weeks, ago, yeah, that, i, me, ##a, -, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 73. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i do n't know if i should really say this [SEP] you know it 's really nothing [SEP] but [SEP] oh well were being [SEP] no no [SEP] well maybe it 's politics [SEP] maybe we should n't talk politics [SEP] %uh it 's environmental politics [SEP] it 's not %uh politics though [SEP] %um [MASK] said that is- E2829 is such a highly i knew that i do n't know i %uh advanced technical country that if there 's something good they 're going to use it [SEP] [[CLS], i, do, n, ', t, know, if, i, should, really, say, this, [SEP], you, know, it, ', s, really, nothing, [SEP], but, [SEP], oh, well, were, being, [SEP], no, no, [SEP], well, maybe, it, ', s, politics, [SEP], maybe, we, should, n, ', t, talk, politics, [SEP], %, uh, it, ', s, environmental, politics, [SEP], it, ', s, not, %, uh, politics, though, [SEP], %, um, [MASK], said, that, is, -, E, ##28, ##29, is, such, a, highly, i, knew, that, i, do, n, ', t, know, i, %, uh, advanced, technical, country, that, if, there, ', s, something, good, they, ', re, going, to, use, it, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 109 with text: \n",
            " \t\t[[CLS], i, do, n, ', t, know, if, i, should, really, say, this, [SEP], you, know, it, ', s, really,\n",
            "\t\tnothing, [SEP], but, [SEP], oh, well, were, being, [SEP], no, no, [SEP], well, maybe, it, ', s,\n",
            "\t\tpolitics, [SEP], maybe, we, should, n, ', t, talk, politics, [SEP], %, uh, it, ', s, environmental,\n",
            "\t\tpolitics, [SEP], it, ', s, not, %, uh, politics, though, [SEP], %, um, [MASK], said, that, is, -, E,\n",
            "\t\t##28, ##29, is, such, a, highly, i, knew, that, i, do, n, ', t, know, i, %, uh, advanced, technical,\n",
            "\t\tcountry, that, if, there, ', s, something, good, they, ', re, going, to, use, it, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 67. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] mentioned the hormones for the milk yeah oh yeah that was a big thing he- no there was a big thing here about the milk that they put silicon in the milk here or something there was a big thing where they were [SEP] [[CLS], and, [MASK], mentioned, the, hormones, for, the, milk, yeah, oh, yeah, that, was, a, big, thing, he, -, no, there, was, a, big, thing, here, about, the, milk, that, they, put, silicon, in, the, milk, here, or, something, there, was, a, big, thing, where, they, were, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 48 with text: \n",
            " \t\t[[CLS], and, [MASK], mentioned, the, hormones, for, the, milk, yeah, oh, yeah, that, was, a, big,\n",
            "\t\tthing, he, -, no, there, was, a, big, thing, here, about, the, milk, that, they, put, silicon, in,\n",
            "\t\tthe, milk, here, or, something, there, was, a, big, thing, where, they, were, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's hormones to make the cows give more [SEP] oh oh oh no [SEP] that 's something different [SEP] yes [SEP] and it 's terrible [SEP] it 's a terrible horrible thing [SEP] yeah [SEP] yeah [SEP] when [MASK] says we 'll use it you know my heart went down to the floor because we 're fighting here oh i did n't hear that very much against it because it 's such a bad thing [SEP] [[CLS], it, ', s, hormones, to, make, the, cows, give, more, [SEP], oh, oh, oh, no, [SEP], that, ', s, something, different, [SEP], yes, [SEP], and, it, ', s, terrible, [SEP], it, ', s, a, terrible, horrible, thing, [SEP], yeah, [SEP], yeah, [SEP], when, [MASK], says, we, ', ll, use, it, you, know, my, heart, went, down, to, the, floor, because, we, ', re, fighting, here, oh, i, did, n, ', t, hear, that, very, much, against, it, because, it, ', s, such, a, bad, thing, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 87 with text: \n",
            " \t\t[[CLS], it, ', s, hormones, to, make, the, cows, give, more, [SEP], oh, oh, oh, no, [SEP], that, ',\n",
            "\t\ts, something, different, [SEP], yes, [SEP], and, it, ', s, terrible, [SEP], it, ', s, a, terrible,\n",
            "\t\thorrible, thing, [SEP], yeah, [SEP], yeah, [SEP], when, [MASK], says, we, ', ll, use, it, you, know,\n",
            "\t\tmy, heart, went, down, to, the, floor, because, we, ', re, fighting, here, oh, i, did, n, ', t,\n",
            "\t\thear, that, very, much, against, it, because, it, ', s, such, a, bad, thing, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 44. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's more we eat fruit vegetables an- yeah yeah stuff [SEP] nobody really eats steaks anymore [SEP] like i grew up on steaks all the time from E4651 's supermarket you know all the [SEP] yeah [SEP] from E4651 's supermark- [SEP] uh-huh [SEP] yeah [SEP] jeez %uh the last time [MASK] had a steak was i do n't know months ago [SEP] [[CLS], it, ', s, more, we, eat, fruit, vegetables, an, -, yeah, yeah, stuff, [SEP], nobody, really, eats, steak, ##s, anymore, [SEP], like, i, grew, up, on, steak, ##s, all, the, time, from, E, ##46, ##51, ', s, supermarket, you, know, all, the, [SEP], yeah, [SEP], from, E, ##46, ##51, ', s, super, ##mark, -, [SEP], uh, -, huh, [SEP], yeah, [SEP], j, ##ee, ##z, %, uh, the, last, time, [MASK], had, a, steak, was, i, do, n, ', t, know, months, ago, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 84 with text: \n",
            " \t\t[[CLS], it, ', s, more, we, eat, fruit, vegetables, an, -, yeah, yeah, stuff, [SEP], nobody, really,\n",
            "\t\teats, steak, ##s, anymore, [SEP], like, i, grew, up, on, steak, ##s, all, the, time, from, E, ##46,\n",
            "\t\t##51, ', s, supermarket, you, know, all, the, [SEP], yeah, [SEP], from, E, ##46, ##51, ', s, super,\n",
            "\t\t##mark, -, [SEP], uh, -, huh, [SEP], yeah, [SEP], j, ##ee, ##z, %, uh, the, last, time, [MASK], had,\n",
            "\t\ta, steak, was, i, do, n, ', t, know, months, ago, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 70. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] oh it 's pitiful [SEP] that makes me very very sad [SEP] you know oh it 's %eh [SEP] very sad [SEP] and even the russians they come over [SEP] and i know i can see the attitude %ah well that of course they know for ins- [SEP] [MASK] 's working in a neighborhood association like on funds [SEP] [[CLS], yeah, [SEP], oh, it, ', s, pit, ##iful, [SEP], that, makes, me, very, very, sad, [SEP], you, know, oh, it, ', s, %, eh, [SEP], very, sad, [SEP], and, even, the, r, ##uss, ##ians, they, come, over, [SEP], and, i, know, i, can, see, the, attitude, %, ah, well, that, of, course, they, know, for, ins, -, [SEP], [MASK], ', s, working, in, a, neighborhood, association, like, on, funds, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 71 with text: \n",
            " \t\t[[CLS], yeah, [SEP], oh, it, ', s, pit, ##iful, [SEP], that, makes, me, very, very, sad, [SEP], you,\n",
            "\t\tknow, oh, it, ', s, %, eh, [SEP], very, sad, [SEP], and, even, the, r, ##uss, ##ians, they, come,\n",
            "\t\tover, [SEP], and, i, know, i, can, see, the, attitude, %, ah, well, that, of, course, they, know,\n",
            "\t\tfor, ins, -, [SEP], [MASK], ', s, working, in, a, neighborhood, association, like, on, funds, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 59. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] yeah [SEP] on grants [SEP] [MASK] does a wonderful job [SEP] [[CLS], yeah, [SEP], yeah, [SEP], on, grants, [SEP], [MASK], does, a, wonderful, job, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], yeah, [SEP], yeah, [SEP], on, grants, [SEP], [MASK], does, a, wonderful, job, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and they help [MASK] [SEP] [[CLS], and, they, help, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], and, they, help, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh i think [MASK] was n't working when E4633 was here [SEP] [[CLS], oh, i, think, [MASK], was, n, ', t, working, when, E, ##46, ##33, was, here, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], oh, i, think, [MASK], was, n, ', t, working, when, E, ##46, ##33, was, here, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i think [MASK] was n't working [SEP] [[CLS], i, think, [MASK], was, n, ', t, working, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], i, think, [MASK], was, n, ', t, working, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but this is parttime [SEP] but it 's very involving [SEP] it 's very exciting [SEP] oh that 's nice [SEP] and i thought that the people line up to see [MASK] [SEP] [[CLS], but, this, is, part, ##time, [SEP], but, it, ', s, very, involving, [SEP], it, ', s, very, exciting, [SEP], oh, that, ', s, nice, [SEP], and, i, thought, that, the, people, line, up, to, see, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], but, this, is, part, ##time, [SEP], but, it, ', s, very, involving, [SEP], it, ', s, very,\n",
            "\t\texciting, [SEP], oh, that, ', s, nice, [SEP], and, i, thought, that, the, people, line, up, to, see,\n",
            "\t\t[MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 956it [00:07, 144.07it/s][CLS] and it 's about getting help with housing and food stamps and everything [SEP] yeah [SEP] and a lady said you know my husband died and old lady [SEP] i said oh i 'm sorry [SEP] [MASK] says i really thought united states that you do n't die you know [SEP] [[CLS], and, it, ', s, about, getting, help, with, housing, and, food, stamps, and, everything, [SEP], yeah, [SEP], and, a, lady, said, you, know, my, husband, died, and, old, lady, [SEP], i, said, oh, i, ', m, sorry, [SEP], [MASK], says, i, really, thought, united, states, that, you, do, n, ', t, die, you, know, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 56 with text: \n",
            " \t\t[[CLS], and, it, ', s, about, getting, help, with, housing, and, food, stamps, and, everything,\n",
            "\t\t[SEP], yeah, [SEP], and, a, lady, said, you, know, my, husband, died, and, old, lady, [SEP], i,\n",
            "\t\tsaid, oh, i, ', m, sorry, [SEP], [MASK], says, i, really, thought, united, states, that, you, do, n,\n",
            "\t\t', t, die, you, know, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i know [SEP] i know %eh [SEP] %eh %uh it 's %uh pitiful [SEP] but [SEP] oh well [SEP] yeah [SEP] i 'm su- [SEP] E4633 is %uh busy with all [MASK] no that 's good now that 's nice cook things [SEP] [[CLS], i, know, [SEP], i, know, %, eh, [SEP], %, eh, %, uh, it, ', s, %, uh, pit, ##iful, [SEP], but, [SEP], oh, well, [SEP], yeah, [SEP], i, ', m, su, -, [SEP], E, ##46, ##33, is, %, uh, busy, with, all, [MASK], no, that, ', s, good, now, that, ', s, nice, cook, things, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 57 with text: \n",
            " \t\t[[CLS], i, know, [SEP], i, know, %, eh, [SEP], %, eh, %, uh, it, ', s, %, uh, pit, ##iful, [SEP],\n",
            "\t\tbut, [SEP], oh, well, [SEP], yeah, [SEP], i, ', m, su, -, [SEP], E, ##46, ##33, is, %, uh, busy,\n",
            "\t\twith, all, [MASK], no, that, ', s, good, now, that, ', s, nice, cook, things, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's going to teach [SEP] [[CLS], and, [MASK], ', s, going, to, teach, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, going, to, teach, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] [MASK] had a good time sure [SEP] [[CLS], yeah, [SEP], [MASK], had, a, good, time, sure, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], yeah, [SEP], [MASK], had, a, good, time, sure, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 'll come back and visit and all [SEP] [[CLS], [MASK], ', ll, come, back, and, visit, and, all, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], [MASK], ', ll, come, back, and, visit, and, all, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] had a good [SEP] [[CLS], [MASK], had, a, good, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], [MASK], had, a, good, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh [MASK] 's lost too [SEP] [[CLS], oh, [MASK], ', s, lost, too, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], oh, [MASK], ', s, lost, too, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh [MASK] misses right now %eh %eh for rosh hashanah [SEP] [[CLS], oh, [MASK], misses, right, now, %, eh, %, eh, for, r, ##osh, has, ##hana, ##h, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], oh, [MASK], misses, right, now, %, eh, %, eh, for, r, ##osh, has, ##hana, ##h, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes [SEP] [MASK] says oh on the kibbutz everybody would go out and have the meal under the stars together the first night [SEP] [[CLS], yes, [SEP], [MASK], says, oh, on, the, k, ##ib, ##but, ##z, everybody, would, go, out, and, have, the, meal, under, the, stars, together, the, first, night, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], yes, [SEP], [MASK], says, oh, on, the, k, ##ib, ##but, ##z, everybody, would, go, out, and,\n",
            "\t\thave, the, meal, under, the, stars, together, the, first, night, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] they have th- [SEP] yeah [SEP] that 's right [SEP] that 's right [SEP] and you know it 's just [SEP] so [MASK] misses [SEP] [[CLS], yeah, [SEP], they, have, th, -, [SEP], yeah, [SEP], that, ', s, right, [SEP], that, ', s, right, [SEP], and, you, know, it, ', s, just, [SEP], so, [MASK], misses, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 32 with text: \n",
            " \t\t[[CLS], yeah, [SEP], they, have, th, -, [SEP], yeah, [SEP], that, ', s, right, [SEP], that, ', s,\n",
            "\t\tright, [SEP], and, you, know, it, ', s, just, [SEP], so, [MASK], misses, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 29. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well you know %uh you know it 's like new year 's %uh [SEP] everything is closed and all in america [SEP] you do n't feel the holidays because everybody 's busy doing [SEP] and here [SEP] yes [SEP] it 's not E2829 although in [SEP] well you live in a very jewish sec- [SEP] i 'm wi- [SEP] yeah [SEP] here you do [SEP] you see [MASK] [SEP] [[CLS], well, you, know, %, uh, you, know, it, ', s, like, new, year, ', s, %, uh, [SEP], everything, is, closed, and, all, in, am, ##eric, ##a, [SEP], you, do, n, ', t, feel, the, holidays, because, everybody, ', s, busy, doing, [SEP], and, here, [SEP], yes, [SEP], it, ', s, not, E, ##28, ##29, although, in, [SEP], well, you, live, in, a, very, j, ##ew, ##ish, se, ##c, -, [SEP], i, ', m, w, ##i, -, [SEP], yeah, [SEP], here, you, do, [SEP], you, see, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 89 with text: \n",
            " \t\t[[CLS], well, you, know, %, uh, you, know, it, ', s, like, new, year, ', s, %, uh, [SEP],\n",
            "\t\teverything, is, closed, and, all, in, am, ##eric, ##a, [SEP], you, do, n, ', t, feel, the, holidays,\n",
            "\t\tbecause, everybody, ', s, busy, doing, [SEP], and, here, [SEP], yes, [SEP], it, ', s, not, E, ##28,\n",
            "\t\t##29, although, in, [SEP], well, you, live, in, a, very, j, ##ew, ##ish, se, ##c, -, [SEP], i, ', m,\n",
            "\t\tw, ##i, -, [SEP], yeah, [SEP], here, you, do, [SEP], you, see, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 87. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well keep talking until we hang up [SEP] it 's nice talking to you [SEP] yeah [SEP] yeah [SEP] %um [SEP] so what about the allens ? [SEP] then they moved away to middletown [SEP] [MASK] was in the girls ' [SEP] [[CLS], well, keep, talking, until, we, hang, up, [SEP], it, ', s, nice, talking, to, you, [SEP], yeah, [SEP], yeah, [SEP], %, um, [SEP], so, what, about, the, all, ##ens, ?, [SEP], then, they, moved, away, to, middle, ##town, [SEP], [MASK], was, in, the, girls, ', [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], well, keep, talking, until, we, hang, up, [SEP], it, ', s, nice, talking, to, you, [SEP],\n",
            "\t\tyeah, [SEP], yeah, [SEP], %, um, [SEP], so, what, about, the, all, ##ens, ?, [SEP], then, they,\n",
            "\t\tmoved, away, to, middle, ##town, [SEP], [MASK], was, in, the, girls, ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 40. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] do you remember the girls ' reformatory in E814 [SEP] yeah [SEP] yeah [SEP] %um [SEP] it was right across the street from my aunt seal 's house [SEP] oh [MASK] lived there ? [SEP] [[CLS], do, you, remember, the, girls, ', reform, ##atory, in, E, ##8, ##14, [SEP], yeah, [SEP], yeah, [SEP], %, um, [SEP], it, was, right, across, the, street, from, my, aunt, seal, ', s, house, [SEP], oh, [MASK], lived, there, ?, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 41 with text: \n",
            " \t\t[[CLS], do, you, remember, the, girls, ', reform, ##atory, in, E, ##8, ##14, [SEP], yeah, [SEP],\n",
            "\t\tyeah, [SEP], %, um, [SEP], it, was, right, across, the, street, from, my, aunt, seal, ', s, house,\n",
            "\t\t[SEP], oh, [MASK], lived, there, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] on E1641 avenue i th- [SEP] [MASK] did on wor- [SEP] [[CLS], on, E, ##16, ##41, avenue, i, th, -, [SEP], [MASK], did, on, w, ##or, -, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], on, E, ##16, ##41, avenue, i, th, -, [SEP], [MASK], did, on, w, ##or, -, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah like E2852 lived on E1641 avenue [SEP] yeah [SEP] %um [SEP] yeah [SEP] %um well it was %um my uncle E2068 's [SEP] it was my father 's %eh you know half brother [SEP] right [SEP] right [SEP] i remember [SEP] yeah [SEP] mitnics the mitnics [SEP] yes [SEP] yes [SEP] well what about the allens [SEP] what was their connection to that [SEP] well so originally [MASK] had come back from the army [SEP] [[CLS], yeah, like, E, ##28, ##5, ##2, lived, on, E, ##16, ##41, avenue, [SEP], yeah, [SEP], %, um, [SEP], yeah, [SEP], %, um, well, it, was, %, um, my, uncle, E, ##20, ##6, ##8, ', s, [SEP], it, was, my, father, ', s, %, eh, you, know, half, brother, [SEP], right, [SEP], right, [SEP], i, remember, [SEP], yeah, [SEP], mit, ##nics, the, mit, ##nics, [SEP], yes, [SEP], yes, [SEP], well, what, about, the, all, ##ens, [SEP], what, was, their, connection, to, that, [SEP], well, so, originally, [MASK], had, come, back, from, the, army, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 94 with text: \n",
            " \t\t[[CLS], yeah, like, E, ##28, ##5, ##2, lived, on, E, ##16, ##41, avenue, [SEP], yeah, [SEP], %, um,\n",
            "\t\t[SEP], yeah, [SEP], %, um, well, it, was, %, um, my, uncle, E, ##20, ##6, ##8, ', s, [SEP], it, was,\n",
            "\t\tmy, father, ', s, %, eh, you, know, half, brother, [SEP], right, [SEP], right, [SEP], i, remember,\n",
            "\t\t[SEP], yeah, [SEP], mit, ##nics, the, mit, ##nics, [SEP], yes, [SEP], yes, [SEP], well, what, about,\n",
            "\t\tthe, all, ##ens, [SEP], what, was, their, connection, to, that, [SEP], well, so, originally, [MASK],\n",
            "\t\thad, come, back, from, the, army, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 86. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they got married [SEP] [MASK] graduated cornell agricultural [SEP] [[CLS], they, got, married, [SEP], [MASK], graduated, corn, ##ell, agricultural, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], they, got, married, [SEP], [MASK], graduated, corn, ##ell, agricultural, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes [SEP] yeah [SEP] could b- [SEP] or just about the time you were born they left E814 [SEP] yeah [SEP] not the- [SEP] so then they went to middletow- [SEP] %um then [MASK] came %um for the girls ' school [SEP] [[CLS], yes, [SEP], yeah, [SEP], could, b, -, [SEP], or, just, about, the, time, you, were, born, they, left, E, ##8, ##14, [SEP], yeah, [SEP], not, the, -, [SEP], so, then, they, went, to, middle, ##to, ##w, -, [SEP], %, um, then, [MASK], came, %, um, for, the, girls, ', school, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 52 with text: \n",
            " \t\t[[CLS], yes, [SEP], yeah, [SEP], could, b, -, [SEP], or, just, about, the, time, you, were, born,\n",
            "\t\tthey, left, E, ##8, ##14, [SEP], yeah, [SEP], not, the, -, [SEP], so, then, they, went, to, middle,\n",
            "\t\t##to, ##w, -, [SEP], %, um, then, [MASK], came, %, um, for, the, girls, ', school, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 42. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] became [SEP] [[CLS], [MASK], became, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 4 with text: \n",
            " \t\t[[CLS], [MASK], became, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] since [MASK] did n't at farming [SEP] [[CLS], since, [MASK], did, n, ', t, at, farming, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], since, [MASK], did, n, ', t, at, farming, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] could n't be managed [SEP] yeah [SEP] [MASK] became [SEP] [[CLS], could, n, ', t, be, managed, [SEP], yeah, [SEP], [MASK], became, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], could, n, ', t, be, managed, [SEP], yeah, [SEP], [MASK], became, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i mean [MASK] got a job [SEP] [[CLS], i, mean, [MASK], got, a, job, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], i, mean, [MASK], got, a, job, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 977it [00:07, 159.25it/s][CLS] the girls ' training school there needed someone [SEP] and [MASK] got into that [SEP] [[CLS], the, girls, ', training, school, there, needed, someone, [SEP], and, [MASK], got, into, that, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], the, girls, ', training, school, there, needed, someone, [SEP], and, [MASK], got, into,\n",
            "\t\tthat, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh [SEP] so after that they moved to middletown [SEP] and then they went to south portright upstate near onianta [SEP] yeah yeah and how shall i say [MASK] rebuilt a boys ' training school [SEP] [[CLS], oh, [SEP], so, after, that, they, moved, to, middle, ##town, [SEP], and, then, they, went, to, south, port, ##right, ups, ##tate, near, on, ##iant, ##a, [SEP], yeah, yeah, and, how, shall, i, say, [MASK], rebuilt, a, boys, ', training, school, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 42 with text: \n",
            " \t\t[[CLS], oh, [SEP], so, after, that, they, moved, to, middle, ##town, [SEP], and, then, they, went,\n",
            "\t\tto, south, port, ##right, ups, ##tate, near, on, ##iant, ##a, [SEP], yeah, yeah, and, how, shall, i,\n",
            "\t\tsay, [MASK], rebuilt, a, boys, ', training, school, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and it was so successful they named it after [MASK] now [SEP] [[CLS], and, it, was, so, successful, they, named, it, after, [MASK], now, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], and, it, was, so, successful, they, named, it, after, [MASK], now, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh i did n't know any of this [SEP] yeah [SEP] the E43 all- [SEP] yeah [SEP] [MASK] 's very very %um prominent in that field in the correc- [SEP] [[CLS], oh, i, did, n, ', t, know, any, of, this, [SEP], yeah, [SEP], the, E, ##43, all, -, [SEP], yeah, [SEP], [MASK], ', s, very, very, %, um, prominent, in, that, field, in, the, co, ##rre, ##c, -, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 40 with text: \n",
            " \t\t[[CLS], oh, i, did, n, ', t, know, any, of, this, [SEP], yeah, [SEP], the, E, ##43, all, -, [SEP],\n",
            "\t\tyeah, [SEP], [MASK], ', s, very, very, %, um, prominent, in, that, field, in, the, co, ##rre, ##c,\n",
            "\t\t-, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 22. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no no [SEP] [MASK] did a wonderful job [SEP] [[CLS], no, no, [SEP], [MASK], did, a, wonderful, job, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], no, no, [SEP], [MASK], did, a, wonderful, job, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in fact [MASK] had the best average of rehabilitation in the country for that age oh is that nice which is not too great [SEP] [[CLS], in, fact, [MASK], had, the, best, average, of, rehabilitation, in, the, country, for, that, age, oh, is, that, nice, which, is, not, too, great, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], in, fact, [MASK], had, the, best, average, of, rehabilitation, in, the, country, for, that,\n",
            "\t\tage, oh, is, that, nice, which, is, not, too, great, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] had the best average in the country [SEP] [[CLS], but, [MASK], had, the, best, average, in, the, country, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], but, [MASK], had, the, best, average, in, the, country, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] what 's [MASK] doing now [SEP] [[CLS], yeah, [SEP], what, ', s, [MASK], doing, now, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], yeah, [SEP], what, ', s, [MASK], doing, now, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] they 're a wonderful couple [SEP] E7396 died [SEP] that 's E2852 's [SEP] yeah [SEP] %uh do you remember %uh E2466 kocherstein and [SEP] of course [SEP] E2466 i know [SEP] and E1453 and E2852 kocherstein i remember [SEP] yeah [SEP] E2466 you said [MASK] was the husband [SEP] [[CLS], yeah, [SEP], they, ', re, a, wonderful, couple, [SEP], E, ##7, ##39, ##6, died, [SEP], that, ', s, E, ##28, ##5, ##2, ', s, [SEP], yeah, [SEP], %, uh, do, you, remember, %, uh, E, ##24, ##6, ##6, k, ##och, ##ers, ##tein, and, [SEP], of, course, [SEP], E, ##24, ##6, ##6, i, know, [SEP], and, E, ##14, ##53, and, E, ##28, ##5, ##2, k, ##och, ##ers, ##tein, i, remember, [SEP], yeah, [SEP], E, ##24, ##6, ##6, you, said, [MASK], was, the, husband, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 84 with text: \n",
            " \t\t[[CLS], yeah, [SEP], they, ', re, a, wonderful, couple, [SEP], E, ##7, ##39, ##6, died, [SEP], that,\n",
            "\t\t', s, E, ##28, ##5, ##2, ', s, [SEP], yeah, [SEP], %, uh, do, you, remember, %, uh, E, ##24, ##6,\n",
            "\t\t##6, k, ##och, ##ers, ##tein, and, [SEP], of, course, [SEP], E, ##24, ##6, ##6, i, know, [SEP], and,\n",
            "\t\tE, ##14, ##53, and, E, ##28, ##5, ##2, k, ##och, ##ers, ##tein, i, remember, [SEP], yeah, [SEP], E,\n",
            "\t\t##24, ##6, ##6, you, said, [MASK], was, the, husband, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 79. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's a little older [SEP] [[CLS], [MASK], ', s, a, little, older, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], ', s, a, little, older, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] like i do n't know how many years older than [MASK] [SEP] [[CLS], like, i, do, n, ', t, know, how, many, years, older, than, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], like, i, do, n, ', t, know, how, many, years, older, than, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] like one or two years older than [MASK] [SEP] [[CLS], like, one, or, two, years, older, than, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], like, one, or, two, years, older, than, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh no [SEP] i just remember E2466 and E1453 [SEP] no i do n't remember meeting [MASK] [SEP] [[CLS], oh, no, [SEP], i, just, remember, E, ##24, ##6, ##6, and, E, ##14, ##53, [SEP], no, i, do, n, ', t, remember, meeting, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], oh, no, [SEP], i, just, remember, E, ##24, ##6, ##6, and, E, ##14, ##53, [SEP], no, i, do,\n",
            "\t\tn, ', t, remember, meeting, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] because E2466 was the youngest i think [SEP] no [SEP] yeah [SEP] well E2466 lived right in E814 and [MASK] kids and all [SEP] [[CLS], yeah, [SEP], because, E, ##24, ##6, ##6, was, the, youngest, i, think, [SEP], no, [SEP], yeah, [SEP], well, E, ##24, ##6, ##6, lived, right, in, E, ##8, ##14, and, [MASK], kids, and, all, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 35 with text: \n",
            " \t\t[[CLS], yeah, [SEP], because, E, ##24, ##6, ##6, was, the, youngest, i, think, [SEP], no, [SEP],\n",
            "\t\tyeah, [SEP], well, E, ##24, ##6, ##6, lived, right, in, E, ##8, ##14, and, [MASK], kids, and, all,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 30. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] we grew up with [MASK] kids [SEP] [[CLS], we, grew, up, with, [MASK], kids, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], we, grew, up, with, [MASK], kids, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] right [SEP] right [SEP] so [MASK] you know i knew [SEP] [[CLS], yeah, [SEP], right, [SEP], right, [SEP], so, [MASK], you, know, i, knew, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], yeah, [SEP], right, [SEP], right, [SEP], so, [MASK], you, know, i, knew, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] [MASK] used to work in our store sometimes even as a butcher [SEP] [[CLS], yeah, [SEP], [MASK], used, to, work, in, our, store, sometimes, even, as, a, butcher, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], yeah, [SEP], [MASK], used, to, work, in, our, store, sometimes, even, as, a, butcher, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they took over %uh %uh the drug store [SEP] they took over the lunch counters [SEP] i remember that they gave me a summer job once [SEP] right [SEP] yeah [SEP] they were always th- [SEP] now your mother must tell you the son E7153 is now in that business [SEP] [MASK] 's got a cookie subs [SEP] [[CLS], they, took, over, %, uh, %, uh, the, drug, store, [SEP], they, took, over, the, lunch, counters, [SEP], i, remember, that, they, gave, me, a, summer, job, once, [SEP], right, [SEP], yeah, [SEP], they, were, always, th, -, [SEP], now, your, mother, must, tell, you, the, son, E, ##7, ##15, ##3, is, now, in, that, business, [SEP], [MASK], ', s, got, a, cookie, sub, ##s, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 67 with text: \n",
            " \t\t[[CLS], they, took, over, %, uh, %, uh, the, drug, store, [SEP], they, took, over, the, lunch,\n",
            "\t\tcounters, [SEP], i, remember, that, they, gave, me, a, summer, job, once, [SEP], right, [SEP], yeah,\n",
            "\t\t[SEP], they, were, always, th, -, [SEP], now, your, mother, must, tell, you, the, son, E, ##7, ##15,\n",
            "\t\t##3, is, now, in, that, business, [SEP], [MASK], ', s, got, a, cookie, sub, ##s, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 58. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] yeah [SEP] yeah [SEP] i remember when they sai- [SEP] i think [MASK] 's president of the synagogue too or something [SEP] [[CLS], yeah, [SEP], yeah, [SEP], yeah, [SEP], i, remember, when, they, sa, ##i, -, [SEP], i, think, [MASK], ', s, president, of, the, synagogue, too, or, something, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], yeah, [SEP], yeah, [SEP], yeah, [SEP], i, remember, when, they, sa, ##i, -, [SEP], i, think,\n",
            "\t\t[MASK], ', s, president, of, the, synagogue, too, or, something, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 17. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's a big shot in the synagogue or something too [SEP] [[CLS], [MASK], ', s, a, big, shot, in, the, synagogue, or, something, too, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], [MASK], ', s, a, big, shot, in, the, synagogue, or, something, too, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] big shot also with catering [SEP] maybe [SEP] i hope that place keeps up and everything because both my parents ' plaques are there [SEP] oh yeah [SEP] tha- [SEP] yeah [SEP] yeah [SEP] that 's nice then [SEP] i hope it continues [SEP] yeah [SEP] i [SEP] yeah [SEP] of course of course [SEP] so [MASK] became proud with that [SEP] [[CLS], big, shot, also, with, catering, [SEP], maybe, [SEP], i, hope, that, place, keeps, up, and, everything, because, both, my, parents, ', plaque, ##s, are, there, [SEP], oh, yeah, [SEP], th, ##a, -, [SEP], yeah, [SEP], yeah, [SEP], that, ', s, nice, then, [SEP], i, hope, it, continues, [SEP], yeah, [SEP], i, [SEP], yeah, [SEP], of, course, of, course, [SEP], so, [MASK], became, proud, with, that, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 67 with text: \n",
            " \t\t[[CLS], big, shot, also, with, catering, [SEP], maybe, [SEP], i, hope, that, place, keeps, up, and,\n",
            "\t\teverything, because, both, my, parents, ', plaque, ##s, are, there, [SEP], oh, yeah, [SEP], th, ##a,\n",
            "\t\t-, [SEP], yeah, [SEP], yeah, [SEP], that, ', s, nice, then, [SEP], i, hope, it, continues, [SEP],\n",
            "\t\tyeah, [SEP], i, [SEP], yeah, [SEP], of, course, of, course, [SEP], so, [MASK], became, proud, with,\n",
            "\t\tthat, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 61. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 998it [00:07, 171.82it/s][CLS] and [MASK] has two children who did n't live in E814 after they were babies on the farm [SEP] [[CLS], and, [MASK], has, two, children, who, did, n, ', t, live, in, E, ##8, ##14, after, they, were, babies, on, the, farm, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], and, [MASK], has, two, children, who, did, n, ', t, live, in, E, ##8, ##14, after, they,\n",
            "\t\twere, babies, on, the, farm, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] they moved away so they do n't even know anybody except E6125 [SEP] remember %uh E6125 %uh kocherstein [SEP] yeah [SEP] E6125 [SEP] %eh yeah [SEP] yeah [SEP] right now i think [MASK] older one is going to be bar mitzvahed soon [SEP] [[CLS], yeah, [SEP], they, moved, away, so, they, do, n, ', t, even, know, anybody, except, E, ##6, ##12, ##5, [SEP], remember, %, uh, E, ##6, ##12, ##5, %, uh, k, ##och, ##ers, ##tein, [SEP], yeah, [SEP], E, ##6, ##12, ##5, [SEP], %, eh, yeah, [SEP], yeah, [SEP], right, now, i, think, [MASK], older, one, is, going, to, be, bar, mit, ##z, ##va, ##hed, soon, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 66 with text: \n",
            " \t\t[[CLS], yeah, [SEP], they, moved, away, so, they, do, n, ', t, even, know, anybody, except, E, ##6,\n",
            "\t\t##12, ##5, [SEP], remember, %, uh, E, ##6, ##12, ##5, %, uh, k, ##och, ##ers, ##tein, [SEP], yeah,\n",
            "\t\t[SEP], E, ##6, ##12, ##5, [SEP], %, eh, yeah, [SEP], yeah, [SEP], right, now, i, think, [MASK],\n",
            "\t\tolder, one, is, going, to, be, bar, mit, ##z, ##va, ##hed, soon, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh wow oh [SEP] i do n't know if any [SEP] little **pammy** just little **pammy** little smiley little fat little kid [SEP] little little skinny [SEP] yeah [SEP] [MASK] was yeah skinny now n- [SEP] [[CLS], oh, w, ##ow, oh, [SEP], i, do, n, ', t, know, if, any, [SEP], little, *, *, p, ##am, ##my, *, *, just, little, *, *, p, ##am, ##my, *, *, little, smile, ##y, little, fat, little, kid, [SEP], little, little, skinny, [SEP], yeah, [SEP], [MASK], was, yeah, skinny, now, n, -, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 54 with text: \n",
            " \t\t[[CLS], oh, w, ##ow, oh, [SEP], i, do, n, ', t, know, if, any, [SEP], little, *, *, p, ##am, ##my,\n",
            "\t\t*, *, just, little, *, *, p, ##am, ##my, *, *, little, smile, ##y, little, fat, little, kid, [SEP],\n",
            "\t\tlittle, little, skinny, [SEP], yeah, [SEP], [MASK], was, yeah, skinny, now, n, -, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 46. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no [SEP] i remember [SEP] no that was E2326 was the fat one [SEP] E2326 kocherstein was the fat one [SEP] yeah [SEP] now [MASK] is n't [SEP] [[CLS], no, [SEP], i, remember, [SEP], no, that, was, E, ##23, ##26, was, the, fat, one, [SEP], E, ##23, ##26, k, ##och, ##ers, ##tein, was, the, fat, one, [SEP], yeah, [SEP], now, [MASK], is, n, ', t, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], no, [SEP], i, remember, [SEP], no, that, was, E, ##23, ##26, was, the, fat, one, [SEP], E,\n",
            "\t\t##23, ##26, k, ##och, ##ers, ##tein, was, the, fat, one, [SEP], yeah, [SEP], now, [MASK], is, n, ',\n",
            "\t\tt, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 32. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's unbelievable [SEP] oh i do n't know [SEP] jeez i do n't know them anymore [SEP] oh jeez [SEP] but [MASK] 's thin [SEP] [[CLS], it, ', s, un, ##believable, [SEP], oh, i, do, n, ', t, know, [SEP], j, ##ee, ##z, i, do, n, ', t, know, them, anymore, [SEP], oh, j, ##ee, ##z, [SEP], but, [MASK], ', s, thin, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], it, ', s, un, ##believable, [SEP], oh, i, do, n, ', t, know, [SEP], j, ##ee, ##z, i, do, n,\n",
            "\t\t', t, know, them, anymore, [SEP], oh, j, ##ee, ##z, [SEP], but, [MASK], ', s, thin, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 33. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] [MASK] 's got three lovely girls [SEP] [[CLS], yeah, [SEP], [MASK], ', s, got, three, lovely, girls, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], yeah, [SEP], [MASK], ', s, got, three, lovely, girls, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] lived in saint E5855 missouri [SEP] [[CLS], and, [MASK], lived, in, saint, E, ##5, ##8, ##55, miss, ##our, ##i, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], and, [MASK], lived, in, saint, E, ##5, ##8, ##55, miss, ##our, ##i, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] yeah [SEP] i heard that [SEP] yeah [SEP] that 's right [SEP] yeah [SEP] %uh E6125 is living near divensinville wherever that is [SEP] yeah [SEP] that 's right near E814 [SEP] yeah [SEP] that is n't too far away [SEP] yeah [SEP] and [MASK] has two lovely boys [SEP] [[CLS], yeah, [SEP], yeah, [SEP], i, heard, that, [SEP], yeah, [SEP], that, ', s, right, [SEP], yeah, [SEP], %, uh, E, ##6, ##12, ##5, is, living, near, dive, ##ns, ##in, ##ville, wherever, that, is, [SEP], yeah, [SEP], that, ', s, right, near, E, ##8, ##14, [SEP], yeah, [SEP], that, is, n, ', t, too, far, away, [SEP], yeah, [SEP], and, [MASK], has, two, lovely, boys, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 66 with text: \n",
            " \t\t[[CLS], yeah, [SEP], yeah, [SEP], i, heard, that, [SEP], yeah, [SEP], that, ', s, right, [SEP],\n",
            "\t\tyeah, [SEP], %, uh, E, ##6, ##12, ##5, is, living, near, dive, ##ns, ##in, ##ville, wherever, that,\n",
            "\t\tis, [SEP], yeah, [SEP], that, ', s, right, near, E, ##8, ##14, [SEP], yeah, [SEP], that, is, n, ',\n",
            "\t\tt, too, far, away, [SEP], yeah, [SEP], and, [MASK], has, two, lovely, boys, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 60. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so all the kids are okay [SEP] oh that 's nice [SEP] yeah [SEP] i tried [SEP] you know my mother [SEP] i keep telling [MASK] what 's going on in E814 [SEP] [[CLS], so, all, the, kids, are, okay, [SEP], oh, that, ', s, nice, [SEP], yeah, [SEP], i, tried, [SEP], you, know, my, mother, [SEP], i, keep, telling, [MASK], what, ', s, going, on, in, E, ##8, ##14, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], so, all, the, kids, are, okay, [SEP], oh, that, ', s, nice, [SEP], yeah, [SEP], i, tried,\n",
            "\t\t[SEP], you, know, my, mother, [SEP], i, keep, telling, [MASK], what, ', s, going, on, in, E, ##8,\n",
            "\t\t##14, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 27. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but i do n't hear all the news [SEP] and i do n't know exactly what [SEP] no [SEP] now do you remember E2994 potswool [SEP] no [SEP] i do n't know [MASK] either [SEP] [[CLS], but, i, do, n, ', t, hear, all, the, news, [SEP], and, i, do, n, ', t, know, exactly, what, [SEP], no, [SEP], now, do, you, remember, E, ##29, ##9, ##4, pots, ##wo, ##ol, [SEP], no, [SEP], i, do, n, ', t, know, [MASK], either, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], but, i, do, n, ', t, hear, all, the, news, [SEP], and, i, do, n, ', t, know, exactly, what,\n",
            "\t\t[SEP], no, [SEP], now, do, you, remember, E, ##29, ##9, ##4, pots, ##wo, ##ol, [SEP], no, [SEP], i,\n",
            "\t\tdo, n, ', t, know, [MASK], either, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 44. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no [SEP] E819 kocherstein 's daughter [SEP] no no [SEP] i do n't know [MASK] either [SEP] [[CLS], no, [SEP], E, ##8, ##19, k, ##och, ##ers, ##tein, ', s, daughter, [SEP], no, no, [SEP], i, do, n, ', t, know, [MASK], either, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], no, [SEP], E, ##8, ##19, k, ##och, ##ers, ##tein, ', s, daughter, [SEP], no, no, [SEP], i,\n",
            "\t\tdo, n, ', t, know, [MASK], either, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] three other relatives of [MASK] were also injured in the accident including two children . [SEP] [[CLS], three, other, relatives, of, [MASK], were, also, injured, in, the, accident, including, two, children, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], three, other, relatives, of, [MASK], were, also, injured, in, the, accident, including, two,\n",
            "\t\tchildren, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] please publish the film on the satellite channels so that everyone will know how cowardly these bastards are . [SEP] the film is a copy of the film which was taken after [MASK] execution on the morning of the blessed eid . [SEP] [[CLS], please, publish, the, film, on, the, satellite, channels, so, that, everyone, will, know, how, coward, ##ly, these, bastards, are, ., [SEP], the, film, is, a, copy, of, the, film, which, was, taken, after, [MASK], execution, on, the, morning, of, the, blessed, e, ##id, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 46 with text: \n",
            " \t\t[[CLS], please, publish, the, film, on, the, satellite, channels, so, that, everyone, will, know,\n",
            "\t\thow, coward, ##ly, these, bastards, are, ., [SEP], the, film, is, a, copy, of, the, film, which,\n",
            "\t\twas, taken, after, [MASK], execution, on, the, morning, of, the, blessed, e, ##id, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] from the moslem picture archives [MASK] heart was ripped out by the rafida . [SEP] [[CLS], from, the, m, ##os, ##lem, picture, archives, [MASK], heart, was, ripped, out, by, the, r, ##af, ##ida, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], from, the, m, ##os, ##lem, picture, archives, [MASK], heart, was, ripped, out, by, the, r,\n",
            "\t\t##af, ##ida, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] --- [SEP] http://z12.zupload.com/download.php?file=getfile&filepath=6894 [SEP] --- [SEP] http://www.furk.net/newsadam.avi.html [SEP] for correspondence : -- [SEP] alisau@gmail.com [SEP] abu dilama [SEP] google is broadcasting a new video recording of E6832 's corpse after [MASK] execution . [SEP] [[CLS], -, -, -, [SEP], http, :, /, /, z, ##12, ., zu, ##p, ##load, ., com, /, download, ., p, ##hp, ?, file, =, get, ##fi, ##le, &, file, ##path, =, 68, ##9, ##4, [SEP], -, -, -, [SEP], http, :, /, /, www, ., fur, ##k, ., net, /, news, ##ada, ##m, ., a, ##vi, ., html, [SEP], for, correspondence, :, -, -, [SEP], al, ##isa, ##u, @, g, ##mail, ., com, [SEP], a, ##bu, di, ##lam, ##a, [SEP], go, ##og, ##le, is, broadcasting, a, new, video, recording, of, E, ##6, ##8, ##32, ', s, corpse, after, [MASK], execution, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], -, -, -, [SEP], http, :, /, /, z, ##12, ., zu, ##p, ##load, ., com, /, download, ., p, ##hp,\n",
            "\t\t?, file, =, get, ##fi, ##le, &, file, ##path, =, 68, ##9, ##4, [SEP], -, -, -, [SEP], http, :, /, /,\n",
            "\t\twww, ., fur, ##k, ., net, /, news, ##ada, ##m, ., a, ##vi, ., html, [SEP], for, correspondence, :,\n",
            "\t\t-, -, [SEP], al, ##isa, ##u, @, g, ##mail, ., com, [SEP], a, ##bu, di, ##lam, ##a, [SEP], go, ##og,\n",
            "\t\t##le, is, broadcasting, a, new, video, recording, of, E, ##6, ##8, ##32, ', s, corpse, after,\n",
            "\t\t[MASK], execution, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 99. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] a new video recording depicting the corpse of the late iraqi president E6832 E6293 after [MASK] execution on the morning of the first day of the eidul adha was broadcasted monday evening on the google internet website . [SEP] [[CLS], a, new, video, recording, depicting, the, corpse, of, the, late, i, ##ra, ##qi, president, E, ##6, ##8, ##32, E, ##6, ##29, ##3, after, [MASK], execution, on, the, morning, of, the, first, day, of, the, e, ##id, ##ul, ad, ##ha, was, broadcast, ##ed, mon, ##day, evening, on, the, go, ##og, ##le, internet, website, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 55 with text: \n",
            " \t\t[[CLS], a, new, video, recording, depicting, the, corpse, of, the, late, i, ##ra, ##qi, president,\n",
            "\t\tE, ##6, ##8, ##32, E, ##6, ##29, ##3, after, [MASK], execution, on, the, morning, of, the, first,\n",
            "\t\tday, of, the, e, ##id, ##ul, ad, ##ha, was, broadcast, ##ed, mon, ##day, evening, on, the, go, ##og,\n",
            "\t\t##le, internet, website, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the video shows E6832 E6293 lying covered on a stretcher whilst the camera closes on [MASK] . [SEP] [[CLS], the, video, shows, E, ##6, ##8, ##32, E, ##6, ##29, ##3, lying, covered, on, a, stretch, ##er, whilst, the, camera, closes, on, [MASK], ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], the, video, shows, E, ##6, ##8, ##32, E, ##6, ##29, ##3, lying, covered, on, a, stretch,\n",
            "\t\t##er, whilst, the, camera, closes, on, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the clip does not last longer than half a minute and is the second one being broadcast unofficially on the internet following the film taken on a mobile phone last week which recorded the execution of E6832 and depicted insults and repeated shiite slogans by those who attended [MASK] execution . [SEP] [[CLS], the, clip, does, not, last, longer, than, half, a, minute, and, is, the, second, one, being, broadcast, unofficial, ##ly, on, the, internet, following, the, film, taken, on, a, mobile, phone, last, week, which, recorded, the, execution, of, E, ##6, ##8, ##32, and, depicted, insults, and, repeated, s, ##hi, ##ite, slogan, ##s, by, those, who, attended, [MASK], execution, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 60 with text: \n",
            " \t\t[[CLS], the, clip, does, not, last, longer, than, half, a, minute, and, is, the, second, one, being,\n",
            "\t\tbroadcast, unofficial, ##ly, on, the, internet, following, the, film, taken, on, a, mobile, phone,\n",
            "\t\tlast, week, which, recorded, the, execution, of, E, ##6, ##8, ##32, and, depicted, insults, and,\n",
            "\t\trepeated, s, ##hi, ##ite, slogan, ##s, by, those, who, attended, [MASK], execution, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 56. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the film link -- http://www.liveleak.com/view?i=c5daa5b733 [SEP] for correspondence : -- [SEP] alisau@gmail.com [SEP] ibn al - faqeer [SEP] why did n't this appear in the media -- [SEP] tonight i 'll list the two films of the rafid dervish moqtada for everyone to get to know [MASK] up close . [SEP] [[CLS], the, film, link, -, -, http, :, /, /, www, ., live, ##lea, ##k, ., com, /, view, ?, i, =, c, ##5, ##da, ##a, ##5, ##b, ##7, ##33, [SEP], for, correspondence, :, -, -, [SEP], al, ##isa, ##u, @, g, ##mail, ., com, [SEP], ibn, al, -, f, ##aq, ##eer, [SEP], why, did, n, ', t, this, appear, in, the, media, -, -, [SEP], tonight, i, ', ll, list, the, two, films, of, the, r, ##af, ##id, der, ##vis, ##h, m, ##o, ##q, ##tad, ##a, for, everyone, to, get, to, know, [MASK], up, close, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 98 with text: \n",
            " \t\t[[CLS], the, film, link, -, -, http, :, /, /, www, ., live, ##lea, ##k, ., com, /, view, ?, i, =, c,\n",
            "\t\t##5, ##da, ##a, ##5, ##b, ##7, ##33, [SEP], for, correspondence, :, -, -, [SEP], al, ##isa, ##u, @,\n",
            "\t\tg, ##mail, ., com, [SEP], ibn, al, -, f, ##aq, ##eer, [SEP], why, did, n, ', t, this, appear, in,\n",
            "\t\tthe, media, -, -, [SEP], tonight, i, ', ll, list, the, two, films, of, the, r, ##af, ##id, der,\n",
            "\t\t##vis, ##h, m, ##o, ##q, ##tad, ##a, for, everyone, to, get, to, know, [MASK], up, close, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 93. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1017it [00:07, 174.62it/s][CLS] thank you for your visit . [SEP] here we are saying good - bye to another year ... what a pity we have been negligent in it . [SEP] ibn al - faqeer [SEP] thanks be to god alone yasmine that [MASK] made easy the circulation of such evidence which shows the hate of the safawiya ... and who will follow the fate of their first grandfather god permitting . [SEP] [[CLS], thank, you, for, your, visit, ., [SEP], here, we, are, saying, good, -, bye, to, another, year, ., ., ., what, a, pity, we, have, been, ne, ##gli, ##gent, in, it, ., [SEP], ibn, al, -, f, ##aq, ##eer, [SEP], thanks, be, to, god, alone, ya, ##sm, ##ine, that, [MASK], made, easy, the, circulation, of, such, evidence, which, shows, the, hate, of, the, sa, ##fa, ##wi, ##ya, ., ., ., and, who, will, follow, the, fate, of, their, first, grandfather, god, permitting, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 85 with text: \n",
            " \t\t[[CLS], thank, you, for, your, visit, ., [SEP], here, we, are, saying, good, -, bye, to, another,\n",
            "\t\tyear, ., ., ., what, a, pity, we, have, been, ne, ##gli, ##gent, in, it, ., [SEP], ibn, al, -, f,\n",
            "\t\t##aq, ##eer, [SEP], thanks, be, to, god, alone, ya, ##sm, ##ine, that, [MASK], made, easy, the,\n",
            "\t\tcirculation, of, such, evidence, which, shows, the, hate, of, the, sa, ##fa, ##wi, ##ya, ., ., .,\n",
            "\t\tand, who, will, follow, the, fate, of, their, first, grandfather, god, permitting, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] confirmed it by repeating verses from the noble koran and the two testimonies . [SEP] [[CLS], [MASK], confirmed, it, by, repeating, verses, from, the, noble, k, ##oran, and, the, two, test, ##imo, ##nies, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], [MASK], confirmed, it, by, repeating, verses, from, the, noble, k, ##oran, and, the, two,\n",
            "\t\ttest, ##imo, ##nies, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] will be envied by all arab leaders both living and dead for this honorable martyrdom and this enormous love in the midst of hundreds of millions of arabs and muslims all over the different parts of the world . [SEP] [[CLS], [MASK], will, be, en, ##vied, by, all, a, ##rab, leaders, both, living, and, dead, for, this, honorable, martyr, ##dom, and, this, enormous, love, in, the, midst, of, hundreds, of, millions, of, a, ##rab, ##s, and, m, ##us, ##lim, ##s, all, over, the, different, parts, of, the, world, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], [MASK], will, be, en, ##vied, by, all, a, ##rab, leaders, both, living, and, dead, for,\n",
            "\t\tthis, honorable, martyr, ##dom, and, this, enormous, love, in, the, midst, of, hundreds, of,\n",
            "\t\tmillions, of, a, ##rab, ##s, and, m, ##us, ##lim, ##s, all, over, the, different, parts, of, the,\n",
            "\t\tworld, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E6832 E6293 was the only leader in the modern history of this nation who went to the guillotine because [MASK] was a patriot who refused occupation and surrender to the invasion and chose resistance . [SEP] [[CLS], E, ##6, ##8, ##32, E, ##6, ##29, ##3, was, the, only, leader, in, the, modern, history, of, this, nation, who, went, to, the, g, ##uil, ##lot, ##ine, because, [MASK], was, a, pat, ##riot, who, refused, occupation, and, surrender, to, the, invasion, and, chose, resistance, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], E, ##6, ##8, ##32, E, ##6, ##29, ##3, was, the, only, leader, in, the, modern, history, of,\n",
            "\t\tthis, nation, who, went, to, the, g, ##uil, ##lot, ##ine, because, [MASK], was, a, pat, ##riot, who,\n",
            "\t\trefused, occupation, and, surrender, to, the, invasion, and, chose, resistance, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 29. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the only ones who preceded [MASK] in this honor were men like umar al - mukhtar and yusuf al - azma by whose struggle the nation and its history were honored and kept it for them in the records of pride and dignity . [SEP] [[CLS], the, only, ones, who, preceded, [MASK], in, this, honor, were, men, like, um, ##ar, al, -, m, ##uk, ##hta, ##r, and, y, ##us, ##uf, al, -, a, ##z, ##ma, by, whose, struggle, the, nation, and, its, history, were, honored, and, kept, it, for, them, in, the, records, of, pride, and, dignity, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 54 with text: \n",
            " \t\t[[CLS], the, only, ones, who, preceded, [MASK], in, this, honor, were, men, like, um, ##ar, al, -,\n",
            "\t\tm, ##uk, ##hta, ##r, and, y, ##us, ##uf, al, -, a, ##z, ##ma, by, whose, struggle, the, nation, and,\n",
            "\t\tits, history, were, honored, and, kept, it, for, them, in, the, records, of, pride, and, dignity, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] embarrassed them and dwarfed them by standing as a spear in the prisoner 's dock . [SEP] [[CLS], [MASK], embarrassed, them, and, dwarf, ##ed, them, by, standing, as, a, spear, in, the, prisoner, ', s, dock, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], [MASK], embarrassed, them, and, dwarf, ##ed, them, by, standing, as, a, spear, in, the,\n",
            "\t\tprisoner, ', s, dock, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they were terrified by the penetrating looks of [MASK] eyes . [SEP] [[CLS], they, were, terrified, by, the, penetrating, looks, of, [MASK], eyes, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], they, were, terrified, by, the, penetrating, looks, of, [MASK], eyes, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so they decided to get rid of [MASK] with a confused and embarrassing hastiness . [SEP] [[CLS], so, they, decided, to, get, rid, of, [MASK], with, a, confused, and, embarrassing, has, ##tine, ##ss, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], so, they, decided, to, get, rid, of, [MASK], with, a, confused, and, embarrassing, has,\n",
            "\t\t##tine, ##ss, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but they do not know anything except grudges and they do not want anything other than inflicting more degradation on the arabs and muslims by [MASK] execution . [SEP] [[CLS], but, they, do, not, know, anything, except, g, ##rudge, ##s, and, they, do, not, want, anything, other, than, in, ##f, ##lic, ##ting, more, degradation, on, the, a, ##rab, ##s, and, m, ##us, ##lim, ##s, by, [MASK], execution, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 40 with text: \n",
            " \t\t[[CLS], but, they, do, not, know, anything, except, g, ##rudge, ##s, and, they, do, not, want,\n",
            "\t\tanything, other, than, in, ##f, ##lic, ##ting, more, degradation, on, the, a, ##rab, ##s, and, m,\n",
            "\t\t##us, ##lim, ##s, by, [MASK], execution, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] chose resistance and rejected safe and luxurious exile . [SEP] [[CLS], [MASK], chose, resistance, and, rejected, safe, and, luxurious, exile, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], chose, resistance, and, rejected, safe, and, luxurious, exile, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the curse of E6832 will continue to chase them chase the americans and chase [MASK] spiteful sectarian executioners . [SEP] [[CLS], the, curse, of, E, ##6, ##8, ##32, will, continue, to, chase, them, chase, the, am, ##eric, ##ans, and, chase, [MASK], spite, ##ful, sect, ##arian, execution, ##ers, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], the, curse, of, E, ##6, ##8, ##32, will, continue, to, chase, them, chase, the, am, ##eric,\n",
            "\t\t##ans, and, chase, [MASK], spite, ##ful, sect, ##arian, execution, ##ers, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 20. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] pendant of soldier in golan since 1967 war found [SEP] the cold of the north [SEP] gaza - dunya al - watan . [SEP] students at schools of the qisrin settlement found the pendant of a syrian soldier south of the occupied syrian golan . [SEP] on the pendant was written the name of the soldier E2046 bin E5229 and [MASK] military serial number m 16241 . [SEP] [[CLS], pendant, of, soldier, in, go, ##lan, since, 1967, war, found, [SEP], the, cold, of, the, north, [SEP], g, ##az, ##a, -, du, ##nya, al, -, wa, ##tan, ., [SEP], students, at, schools, of, the, q, ##is, ##rin, settlement, found, the, pendant, of, a, s, ##yrian, soldier, south, of, the, occupied, s, ##yrian, go, ##lan, ., [SEP], on, the, pendant, was, written, the, name, of, the, soldier, E, ##20, ##46, bin, E, ##5, ##22, ##9, and, [MASK], military, serial, number, m, 162, ##41, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 85 with text: \n",
            " \t\t[[CLS], pendant, of, soldier, in, go, ##lan, since, 1967, war, found, [SEP], the, cold, of, the,\n",
            "\t\tnorth, [SEP], g, ##az, ##a, -, du, ##nya, al, -, wa, ##tan, ., [SEP], students, at, schools, of,\n",
            "\t\tthe, q, ##is, ##rin, settlement, found, the, pendant, of, a, s, ##yrian, soldier, south, of, the,\n",
            "\t\toccupied, s, ##yrian, go, ##lan, ., [SEP], on, the, pendant, was, written, the, name, of, the,\n",
            "\t\tsoldier, E, ##20, ##46, bin, E, ##5, ##22, ##9, and, [MASK], military, serial, number, m, 162, ##41,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 76. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in fact because not only is the consumer unlucky not choosing the right beauty salon but to a great extent this is determined by whether they choose the right occasion for beautification and understand the results . [SEP] to learn more click www.120zy.com . [SEP] qingqing 20070107 . [SEP] bill shen [SEP] cloudy temperature approx. 2 degrees . [SEP] while qingqing was rinsing [MASK] mouth for the second time last night mom said : leaks air and leaks rain . [SEP] [[CLS], in, fact, because, not, only, is, the, consumer, un, ##luck, ##y, not, choosing, the, right, beauty, salon, but, to, a, great, extent, this, is, determined, by, whether, they, choose, the, right, occasion, for, be, ##aut, ##ification, and, understand, the, results, ., [SEP], to, learn, more, click, www, ., 120, ##zy, ., com, ., [SEP], q, ##ing, ##qing, 2007, ##01, ##0, ##7, ., [SEP], bill, she, ##n, [SEP], cloud, ##y, temperature, approx, ., 2, degrees, ., [SEP], while, q, ##ing, ##qing, was, r, ##ins, ##ing, [MASK], mouth, for, the, second, time, last, night, mom, said, :, leak, ##s, air, and, leak, ##s, rain, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], in, fact, because, not, only, is, the, consumer, un, ##luck, ##y, not, choosing, the, right,\n",
            "\t\tbeauty, salon, but, to, a, great, extent, this, is, determined, by, whether, they, choose, the,\n",
            "\t\tright, occasion, for, be, ##aut, ##ification, and, understand, the, results, ., [SEP], to, learn,\n",
            "\t\tmore, click, www, ., 120, ##zy, ., com, ., [SEP], q, ##ing, ##qing, 2007, ##01, ##0, ##7, ., [SEP],\n",
            "\t\tbill, she, ##n, [SEP], cloud, ##y, temperature, approx, ., 2, degrees, ., [SEP], while, q, ##ing,\n",
            "\t\t##qing, was, r, ##ins, ##ing, [MASK], mouth, for, the, second, time, last, night, mom, said, :,\n",
            "\t\tleak, ##s, air, and, leak, ##s, rain, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 85. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dad also scolded mom : you also leaked air and leaked rain while your permanent teeth grew in your childhood . [SEP] qingqing looked at mom with water in the mouth . [SEP] at last [MASK] said to mom after spitting it into the basin : i meant to spew it on your head . [SEP] [[CLS], dad, also, s, ##co, ##lded, mom, :, you, also, leaked, air, and, leaked, rain, while, your, permanent, teeth, grew, in, your, childhood, ., [SEP], q, ##ing, ##qing, looked, at, mom, with, water, in, the, mouth, ., [SEP], at, last, [MASK], said, to, mom, after, spit, ##ting, it, into, the, basin, :, i, meant, to, s, ##pe, ##w, it, on, your, head, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 64 with text: \n",
            " \t\t[[CLS], dad, also, s, ##co, ##lded, mom, :, you, also, leaked, air, and, leaked, rain, while, your,\n",
            "\t\tpermanent, teeth, grew, in, your, childhood, ., [SEP], q, ##ing, ##qing, looked, at, mom, with,\n",
            "\t\twater, in, the, mouth, ., [SEP], at, last, [MASK], said, to, mom, after, spit, ##ting, it, into,\n",
            "\t\tthe, basin, :, i, meant, to, s, ##pe, ##w, it, on, your, head, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 40. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dad praised qingqing for being thoughtful : knowing what [MASK] should do . [SEP] [[CLS], dad, praised, q, ##ing, ##qing, for, being, thoughtful, :, knowing, what, [MASK], should, do, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], dad, praised, q, ##ing, ##qing, for, being, thoughtful, :, knowing, what, [MASK], should,\n",
            "\t\tdo, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] qingqing played in bed after waking up in the morning . [SEP] dad yelled but [MASK] would n't get up and wanted to play a little longer . [SEP] [[CLS], q, ##ing, ##qing, played, in, bed, after, waking, up, in, the, morning, ., [SEP], dad, yelled, but, [MASK], would, n, ', t, get, up, and, wanted, to, play, a, little, longer, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], q, ##ing, ##qing, played, in, bed, after, waking, up, in, the, morning, ., [SEP], dad,\n",
            "\t\tyelled, but, [MASK], would, n, ', t, get, up, and, wanted, to, play, a, little, longer, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dad looked at qingqing and teased qingqing : how come this kid opens [MASK] eyes while sleeping ? [SEP] [[CLS], dad, looked, at, q, ##ing, ##qing, and, teased, q, ##ing, ##qing, :, how, come, this, kid, opens, [MASK], eyes, while, sleeping, ?, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], dad, looked, at, q, ##ing, ##qing, and, teased, q, ##ing, ##qing, :, how, come, this, kid,\n",
            "\t\topens, [MASK], eyes, while, sleeping, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] instead [MASK] asked for soaked biscuits and did drawings in the room after finishing . [SEP] [[CLS], instead, [MASK], asked, for, soaked, bi, ##s, ##cu, ##its, and, did, drawings, in, the, room, after, finishing, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], instead, [MASK], asked, for, soaked, bi, ##s, ##cu, ##its, and, did, drawings, in, the,\n",
            "\t\troom, after, finishing, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dad read to qingqing and then we arrived at jinrunfa by shuttle bus but could n't find the stickers that qingqing wanted . [SEP] we then went to jinsheng where we finally found them . [SEP] dad bought four volumes : . [SEP] one for each of [MASK] two sisters . [SEP] [[CLS], dad, read, to, q, ##ing, ##qing, and, then, we, arrived, at, ji, ##n, ##run, ##fa, by, shuttle, bus, but, could, n, ', t, find, the, stick, ##ers, that, q, ##ing, ##qing, wanted, ., [SEP], we, then, went, to, ji, ##ns, ##hen, ##g, where, we, finally, found, them, ., [SEP], dad, bought, four, volumes, :, ., [SEP], one, for, each, of, [MASK], two, sisters, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 66 with text: \n",
            " \t\t[[CLS], dad, read, to, q, ##ing, ##qing, and, then, we, arrived, at, ji, ##n, ##run, ##fa, by,\n",
            "\t\tshuttle, bus, but, could, n, ', t, find, the, stick, ##ers, that, q, ##ing, ##qing, wanted, .,\n",
            "\t\t[SEP], we, then, went, to, ji, ##ns, ##hen, ##g, where, we, finally, found, them, ., [SEP], dad,\n",
            "\t\tbought, four, volumes, :, ., [SEP], one, for, each, of, [MASK], two, sisters, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 61. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1036it [00:07, 177.06it/s][CLS] one to be placed at qingqing 's home and one at [MASK] kindergarten . [SEP] [[CLS], one, to, be, placed, at, q, ##ing, ##qing, ', s, home, and, one, at, [MASK], kindergarten, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], one, to, be, placed, at, q, ##ing, ##qing, ', s, home, and, one, at, [MASK], kindergarten,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] qingqing wanted to pee on the way dad had to hold qingqing to pee by the tree . [SEP] it was very cold and dad tidied qingqing 's pants quickly . [SEP] qingqing wanted to play with the sticker after we arrived at the hotel . [SEP] dad discussed conditions with qingqing : dad will let you play if you eat well . [SEP] qingqing agreed and began to play again . [SEP] mom said [MASK] wanted to swap to another place but dad wanted to let qingqing stay longer in the sun . [SEP] [[CLS], q, ##ing, ##qing, wanted, to, p, ##ee, on, the, way, dad, had, to, hold, q, ##ing, ##qing, to, p, ##ee, by, the, tree, ., [SEP], it, was, very, cold, and, dad, t, ##id, ##ied, q, ##ing, ##qing, ', s, pants, quickly, ., [SEP], q, ##ing, ##qing, wanted, to, play, with, the, stick, ##er, after, we, arrived, at, the, hotel, ., [SEP], dad, discussed, conditions, with, q, ##ing, ##qing, :, dad, will, let, you, play, if, you, eat, well, ., [SEP], q, ##ing, ##qing, agreed, and, began, to, play, again, ., [SEP], mom, said, [MASK], wanted, to, swap, to, another, place, but, dad, wanted, to, let, q, ##ing, ##qing, stay, longer, in, the, sun, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 116 with text: \n",
            " \t\t[[CLS], q, ##ing, ##qing, wanted, to, p, ##ee, on, the, way, dad, had, to, hold, q, ##ing, ##qing,\n",
            "\t\tto, p, ##ee, by, the, tree, ., [SEP], it, was, very, cold, and, dad, t, ##id, ##ied, q, ##ing,\n",
            "\t\t##qing, ', s, pants, quickly, ., [SEP], q, ##ing, ##qing, wanted, to, play, with, the, stick, ##er,\n",
            "\t\tafter, we, arrived, at, the, hotel, ., [SEP], dad, discussed, conditions, with, q, ##ing, ##qing, :,\n",
            "\t\tdad, will, let, you, play, if, you, eat, well, ., [SEP], q, ##ing, ##qing, agreed, and, began, to,\n",
            "\t\tplay, again, ., [SEP], mom, said, [MASK], wanted, to, swap, to, another, place, but, dad, wanted,\n",
            "\t\tto, let, q, ##ing, ##qing, stay, longer, in, the, sun, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 94. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] qingqing was so obsessed with playing that [MASK] would n't have dinner . [SEP] [[CLS], q, ##ing, ##qing, was, so, obsessed, with, playing, that, [MASK], would, n, ', t, have, dinner, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], q, ##ing, ##qing, was, so, obsessed, with, playing, that, [MASK], would, n, ', t, have,\n",
            "\t\tdinner, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dad had to put away qingqing 's sticker book and asked qingqing to eat well . [SEP] qingqing said it was spicy and [MASK] would n't eat . [SEP] [[CLS], dad, had, to, put, away, q, ##ing, ##qing, ', s, stick, ##er, book, and, asked, q, ##ing, ##qing, to, eat, well, ., [SEP], q, ##ing, ##qing, said, it, was, s, ##pic, ##y, and, [MASK], would, n, ', t, eat, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 42 with text: \n",
            " \t\t[[CLS], dad, had, to, put, away, q, ##ing, ##qing, ', s, stick, ##er, book, and, asked, q, ##ing,\n",
            "\t\t##qing, to, eat, well, ., [SEP], q, ##ing, ##qing, said, it, was, s, ##pic, ##y, and, [MASK], would,\n",
            "\t\tn, ', t, eat, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mom promised to buy qingqing white fungus broth later . [SEP] qingqing became happy immediately and played on . [SEP] we walked home after eating and qingqing played in [MASK] own room after dinner . [SEP] [[CLS], mom, promised, to, buy, q, ##ing, ##qing, white, fungus, br, ##oth, later, ., [SEP], q, ##ing, ##qing, became, happy, immediately, and, played, on, ., [SEP], we, walked, home, after, eating, and, q, ##ing, ##qing, played, in, [MASK], own, room, after, dinner, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], mom, promised, to, buy, q, ##ing, ##qing, white, fungus, br, ##oth, later, ., [SEP], q,\n",
            "\t\t##ing, ##qing, became, happy, immediately, and, played, on, ., [SEP], we, walked, home, after,\n",
            "\t\teating, and, q, ##ing, ##qing, played, in, [MASK], own, room, after, dinner, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 37. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] teachers and students at colleges in the capital in building a harmonious society reminisce about the great man 's charisma . [SEP] ------ the sun that never sets a large - scale evening gala in grand commemoration of the 113th birthday of comrade E5746 zedong [SEP] comrade E5746 zedong was a great marxist a great proletarian revolutionary strategist and theoretician . [SEP] [MASK] is china 's greatest patriot and national hero of recent times . [SEP] [[CLS], teachers, and, students, at, colleges, in, the, capital, in, building, a, harm, ##oni, ##ous, society, re, ##mini, ##s, ##ce, about, the, great, man, ', s, ch, ##aris, ##ma, ., [SEP], -, -, -, -, -, -, the, sun, that, never, sets, a, large, -, scale, evening, gal, ##a, in, grand, com, ##me, ##moration, of, the, 113, ##th, birthday, of, com, ##rade, E, ##5, ##7, ##46, z, ##ed, ##ong, [SEP], com, ##rade, E, ##5, ##7, ##46, z, ##ed, ##ong, was, a, great, ma, ##r, ##xi, ##st, a, great, pro, ##let, ##arian, revolutionary, s, ##trate, ##gis, ##t, and, the, ##ore, ##tic, ##ian, ., [SEP], [MASK], is, chin, ##a, ', s, greatest, pat, ##riot, and, national, hero, of, recent, times, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 120 with text: \n",
            " \t\t[[CLS], teachers, and, students, at, colleges, in, the, capital, in, building, a, harm, ##oni,\n",
            "\t\t##ous, society, re, ##mini, ##s, ##ce, about, the, great, man, ', s, ch, ##aris, ##ma, ., [SEP], -,\n",
            "\t\t-, -, -, -, -, the, sun, that, never, sets, a, large, -, scale, evening, gal, ##a, in, grand, com,\n",
            "\t\t##me, ##moration, of, the, 113, ##th, birthday, of, com, ##rade, E, ##5, ##7, ##46, z, ##ed, ##ong,\n",
            "\t\t[SEP], com, ##rade, E, ##5, ##7, ##46, z, ##ed, ##ong, was, a, great, ma, ##r, ##xi, ##st, a, great,\n",
            "\t\tpro, ##let, ##arian, revolutionary, s, ##trate, ##gis, ##t, and, the, ##ore, ##tic, ##ian, ., [SEP],\n",
            "\t\t[MASK], is, chin, ##a, ', s, greatest, pat, ##riot, and, national, hero, of, recent, times, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 103. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] was a great man of the times who led the chinese people to completely change their own destiny and national image . [SEP] [[CLS], [MASK], was, a, great, man, of, the, times, who, led, the, chin, ##ese, people, to, completely, change, their, own, destiny, and, national, image, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], [MASK], was, a, great, man, of, the, times, who, led, the, chin, ##ese, people, to,\n",
            "\t\tcompletely, change, their, own, destiny, and, national, image, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E5746 zedong 's name E5746 zedong 's spirit and E5746 zedong 's thought will have increasing influence on china and the world . [SEP] just as general secretary hu jintao said : in all times and under all circumstances we must always hold high the great banner of E5746 zedong thought . [SEP] the east is red the sun has risen . [SEP] china has brought forth E5746 zedong . [SEP] [MASK] works for the people 's happiness . [SEP] [[CLS], E, ##5, ##7, ##46, z, ##ed, ##ong, ', s, name, E, ##5, ##7, ##46, z, ##ed, ##ong, ', s, spirit, and, E, ##5, ##7, ##46, z, ##ed, ##ong, ', s, thought, will, have, increasing, influence, on, chin, ##a, and, the, world, ., [SEP], just, as, general, secretary, h, ##u, ji, ##nta, ##o, said, :, in, all, times, and, under, all, circumstances, we, must, always, hold, high, the, great, banner, of, E, ##5, ##7, ##46, z, ##ed, ##ong, thought, ., [SEP], the, east, is, red, the, sun, has, risen, ., [SEP], chin, ##a, has, brought, forth, E, ##5, ##7, ##46, z, ##ed, ##ong, ., [SEP], [MASK], works, for, the, people, ', s, happiness, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 115 with text: \n",
            " \t\t[[CLS], E, ##5, ##7, ##46, z, ##ed, ##ong, ', s, name, E, ##5, ##7, ##46, z, ##ed, ##ong, ', s,\n",
            "\t\tspirit, and, E, ##5, ##7, ##46, z, ##ed, ##ong, ', s, thought, will, have, increasing, influence,\n",
            "\t\ton, chin, ##a, and, the, world, ., [SEP], just, as, general, secretary, h, ##u, ji, ##nta, ##o,\n",
            "\t\tsaid, :, in, all, times, and, under, all, circumstances, we, must, always, hold, high, the, great,\n",
            "\t\tbanner, of, E, ##5, ##7, ##46, z, ##ed, ##ong, thought, ., [SEP], the, east, is, red, the, sun, has,\n",
            "\t\trisen, ., [SEP], chin, ##a, has, brought, forth, E, ##5, ##7, ##46, z, ##ed, ##ong, ., [SEP],\n",
            "\t\t[MASK], works, for, the, people, ', s, happiness, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 105. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] is the people 's great savior . [SEP] [[CLS], [MASK], is, the, people, ', s, great, sa, ##vior, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], is, the, people, ', s, great, sa, ##vior, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] from a keen sense of responsibility toward successors of the future without expecting any reward [MASK] supports E5841 education to recalibrate the outlook on life and values of contemporary college students in treasuring the hard - won nature of beautiful life and to encourage college students of the capital to exert themselves for the great revival of the chinese nation . [SEP] [[CLS], from, a, keen, sense, of, responsibility, toward, successors, of, the, future, without, expecting, any, reward, [MASK], supports, E, ##5, ##8, ##41, education, to, re, ##cal, ##ib, ##rate, the, outlook, on, life, and, values, of, contemporary, college, students, in, t, ##rea, ##su, ##ring, the, hard, -, won, nature, of, beautiful, life, and, to, encourage, college, students, of, the, capital, to, ex, ##ert, themselves, for, the, great, revival, of, the, chin, ##ese, nation, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 74 with text: \n",
            " \t\t[[CLS], from, a, keen, sense, of, responsibility, toward, successors, of, the, future, without,\n",
            "\t\texpecting, any, reward, [MASK], supports, E, ##5, ##8, ##41, education, to, re, ##cal, ##ib, ##rate,\n",
            "\t\tthe, outlook, on, life, and, values, of, contemporary, college, students, in, t, ##rea, ##su,\n",
            "\t\t##ring, the, hard, -, won, nature, of, beautiful, life, and, to, encourage, college, students, of,\n",
            "\t\tthe, capital, to, ex, ##ert, themselves, for, the, great, revival, of, the, chin, ##ese, nation, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 16. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] conjee [SEP] poster : conjee -lrb- conjee -rrb- forum : automobile [SEP] title : zt E7102 E7102 : car knowledge and manifestation of life -lrb- regarding the car accident involving that beautiful/talented taiwanese girl -rrb- [SEP] poster 's site : mitbbs.com bbs -lrb- wed E7006 31 11:16:04 2007 -rrb- [SEP] a few days ago another taiwanese star passed away . [SEP] i had never heard of [MASK] . [SEP] [[CLS], con, ##jee, [SEP], poster, :, con, ##jee, -, l, ##rb, -, con, ##jee, -, r, ##rb, -, forum, :, automobile, [SEP], title, :, z, ##t, E, ##7, ##10, ##2, E, ##7, ##10, ##2, :, car, knowledge, and, manifest, ##ation, of, life, -, l, ##rb, -, regarding, the, car, accident, involving, that, beautiful, /, talented, ta, ##i, ##wan, ##ese, girl, -, r, ##rb, -, [SEP], poster, ', s, site, :, mit, ##bbs, ., com, b, ##bs, -, l, ##rb, -, wed, E, ##70, ##0, ##6, 31, 11, :, 16, :, 04, 2007, -, r, ##rb, -, [SEP], a, few, days, ago, another, ta, ##i, ##wan, ##ese, star, passed, away, ., [SEP], i, had, never, heard, of, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 119 with text: \n",
            " \t\t[[CLS], con, ##jee, [SEP], poster, :, con, ##jee, -, l, ##rb, -, con, ##jee, -, r, ##rb, -, forum,\n",
            "\t\t:, automobile, [SEP], title, :, z, ##t, E, ##7, ##10, ##2, E, ##7, ##10, ##2, :, car, knowledge,\n",
            "\t\tand, manifest, ##ation, of, life, -, l, ##rb, -, regarding, the, car, accident, involving, that,\n",
            "\t\tbeautiful, /, talented, ta, ##i, ##wan, ##ese, girl, -, r, ##rb, -, [SEP], poster, ', s, site, :,\n",
            "\t\tmit, ##bbs, ., com, b, ##bs, -, l, ##rb, -, wed, E, ##70, ##0, ##6, 31, 11, :, 16, :, 04, 2007, -,\n",
            "\t\tr, ##rb, -, [SEP], a, few, days, ago, another, ta, ##i, ##wan, ##ese, star, passed, away, ., [SEP],\n",
            "\t\ti, had, never, heard, of, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 116. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i hear [MASK] was a talented beauty . [SEP] [[CLS], i, hear, [MASK], was, a, talented, beauty, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], i, hear, [MASK], was, a, talented, beauty, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] what a great pity ! [SEP] the accident was a car crash . [SEP] much of the entertainment news denounced that +her car 's front airbag failed to release which caused the death . [SEP] in fact the mini cooper [MASK] was riding in is not what the reports said . [SEP] [[CLS], what, a, great, pity, !, [SEP], the, accident, was, a, car, crash, ., [SEP], much, of, the, entertainment, news, denounced, that, +, her, car, ', s, front, air, ##bag, failed, to, release, which, caused, the, death, ., [SEP], in, fact, the, mini, co, ##oper, [MASK], was, riding, in, is, not, what, the, reports, said, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 57 with text: \n",
            " \t\t[[CLS], what, a, great, pity, !, [SEP], the, accident, was, a, car, crash, ., [SEP], much, of, the,\n",
            "\t\tentertainment, news, denounced, that, +, her, car, ', s, front, air, ##bag, failed, to, release,\n",
            "\t\twhich, caused, the, death, ., [SEP], in, fact, the, mini, co, ##oper, [MASK], was, riding, in, is,\n",
            "\t\tnot, what, the, reports, said, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 45. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] all of the car 's passive safety protections had already fully carried out their functions . [SEP] so it is extremely ridiculous and unscientific to blame and suspect the car and the car manufacturer for such an accident . [SEP] in particular speculation about why the front airbags did not open was very amateurish . [SEP] only by not opening would it have been correct . [SEP] if they had opened [MASK] assistant might still be lying in the hospital . [SEP] [[CLS], all, of, the, car, ', s, passive, safety, protection, ##s, had, already, fully, carried, out, their, functions, ., [SEP], so, it, is, extremely, ridiculous, and, un, ##s, ##cie, ##nti, ##fic, to, blame, and, suspect, the, car, and, the, car, manufacturer, for, such, an, accident, ., [SEP], in, particular, speculation, about, why, the, front, air, ##bag, ##s, did, not, open, was, very, amateur, ##ish, ., [SEP], only, by, not, opening, would, it, have, been, correct, ., [SEP], if, they, had, opened, [MASK], assistant, might, still, be, lying, in, the, hospital, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 92 with text: \n",
            " \t\t[[CLS], all, of, the, car, ', s, passive, safety, protection, ##s, had, already, fully, carried,\n",
            "\t\tout, their, functions, ., [SEP], so, it, is, extremely, ridiculous, and, un, ##s, ##cie, ##nti,\n",
            "\t\t##fic, to, blame, and, suspect, the, car, and, the, car, manufacturer, for, such, an, accident, .,\n",
            "\t\t[SEP], in, particular, speculation, about, why, the, front, air, ##bag, ##s, did, not, open, was,\n",
            "\t\tvery, amateur, ##ish, ., [SEP], only, by, not, opening, would, it, have, been, correct, ., [SEP],\n",
            "\t\tif, they, had, opened, [MASK], assistant, might, still, be, lying, in, the, hospital, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 81. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] below i am going to refer to these 200 people collectively as [MASK] uncle . [SEP] [[CLS], below, i, am, going, to, refer, to, these, 200, people, collectively, as, [MASK], uncle, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], below, i, am, going, to, refer, to, these, 200, people, collectively, as, [MASK], uncle, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] therefore i believe it is extremely important to recognize that one 's own knowledge has limitations . [SEP] we need to maintain a sufficiently open attitude toward things that do n't correspond to our common sense . [SEP] so just because every time your uncle goes to the supermarket [MASK] buys hog head meat does n't mean that we should immediately assume that any theory that does not predict people going to the supermarket will buy hog head meat is wrong . [SEP] [[CLS], therefore, i, believe, it, is, extremely, important, to, recognize, that, one, ', s, own, knowledge, has, limitations, ., [SEP], we, need, to, maintain, a, sufficiently, open, attitude, toward, things, that, do, n, ', t, correspond, to, our, common, sense, ., [SEP], so, just, because, every, time, your, uncle, goes, to, the, supermarket, [MASK], buys, ho, ##g, head, meat, does, n, ', t, mean, that, we, should, immediately, assume, that, any, theory, that, does, not, predict, people, going, to, the, supermarket, will, buy, ho, ##g, head, meat, is, wrong, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 91 with text: \n",
            " \t\t[[CLS], therefore, i, believe, it, is, extremely, important, to, recognize, that, one, ', s, own,\n",
            "\t\tknowledge, has, limitations, ., [SEP], we, need, to, maintain, a, sufficiently, open, attitude,\n",
            "\t\ttoward, things, that, do, n, ', t, correspond, to, our, common, sense, ., [SEP], so, just, because,\n",
            "\t\tevery, time, your, uncle, goes, to, the, supermarket, [MASK], buys, ho, ##g, head, meat, does, n, ',\n",
            "\t\tt, mean, that, we, should, immediately, assume, that, any, theory, that, does, not, predict, people,\n",
            "\t\tgoing, to, the, supermarket, will, buy, ho, ##g, head, meat, is, wrong, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 53. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] when you feel other people are talking baloney and what other people say does not correspond to your common sense you should at the same time recognize that your uncle is only your uncle [MASK] is not the whole world . [SEP] [[CLS], when, you, feel, other, people, are, talking, b, ##alo, ##ney, and, what, other, people, say, does, not, correspond, to, your, common, sense, you, should, at, the, same, time, recognize, that, your, uncle, is, only, your, uncle, [MASK], is, not, the, whole, world, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], when, you, feel, other, people, are, talking, b, ##alo, ##ney, and, what, other, people,\n",
            "\t\tsay, does, not, correspond, to, your, common, sense, you, should, at, the, same, time, recognize,\n",
            "\t\tthat, your, uncle, is, only, your, uncle, [MASK], is, not, the, whole, world, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 37. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the millions who died in the iran - iraq war had nothing to do with E2829 . [SEP] the mass murder happening right now in sudan where the arab moslem regime is massacring its black E5093 citizens has nothing to do with E2829 . [SEP] the frequent reports from algeria about the murders of hundreds of civilians in one village or another by other algerians have nothing to do with E2829 . [SEP] E6832 E6293 did not invade kuwait endanger saudi arabia and butcher [MASK] own people because of E2829 . [SEP] [[CLS], the, millions, who, died, in, the, i, ##ran, -, i, ##ra, ##q, war, had, nothing, to, do, with, E, ##28, ##29, ., [SEP], the, mass, murder, happening, right, now, in, su, ##dan, where, the, a, ##rab, m, ##os, ##lem, regime, is, mass, ##ac, ##ring, its, black, E, ##50, ##9, ##3, citizens, has, nothing, to, do, with, E, ##28, ##29, ., [SEP], the, frequent, reports, from, al, ##ger, ##ia, about, the, murders, of, hundreds, of, civilians, in, one, village, or, another, by, other, al, ##ger, ##ians, have, nothing, to, do, with, E, ##28, ##29, ., [SEP], E, ##6, ##8, ##32, E, ##6, ##29, ##3, did, not, invade, k, ##u, ##wai, ##t, end, ##anger, sa, ##udi, a, ##rab, ##ia, and, butcher, [MASK], own, people, because, of, E, ##28, ##29, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 130 with text: \n",
            " \t\t[[CLS], the, millions, who, died, in, the, i, ##ran, -, i, ##ra, ##q, war, had, nothing, to, do,\n",
            "\t\twith, E, ##28, ##29, ., [SEP], the, mass, murder, happening, right, now, in, su, ##dan, where, the,\n",
            "\t\ta, ##rab, m, ##os, ##lem, regime, is, mass, ##ac, ##ring, its, black, E, ##50, ##9, ##3, citizens,\n",
            "\t\thas, nothing, to, do, with, E, ##28, ##29, ., [SEP], the, frequent, reports, from, al, ##ger, ##ia,\n",
            "\t\tabout, the, murders, of, hundreds, of, civilians, in, one, village, or, another, by, other, al,\n",
            "\t\t##ger, ##ians, have, nothing, to, do, with, E, ##28, ##29, ., [SEP], E, ##6, ##8, ##32, E, ##6,\n",
            "\t\t##29, ##3, did, not, invade, k, ##u, ##wai, ##t, end, ##anger, sa, ##udi, a, ##rab, ##ia, and,\n",
            "\t\tbutcher, [MASK], own, people, because, of, E, ##28, ##29, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 120. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] egypt did not use poison gas against yemen in the 60's because of E2829 . [SEP] assad the father did not kill tens of thousands of [MASK] own citizens in one week in el hamma in syria because of E2829 . [SEP] [[CLS], e, ##gy, ##pt, did, not, use, poison, gas, against, ye, ##men, in, the, 60, ', s, because, of, E, ##28, ##29, ., [SEP], ass, ##ad, the, father, did, not, kill, tens, of, thousands, of, [MASK], own, citizens, in, one, week, in, el, ha, ##mma, in, s, ##yr, ##ia, because, of, E, ##28, ##29, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 56 with text: \n",
            " \t\t[[CLS], e, ##gy, ##pt, did, not, use, poison, gas, against, ye, ##men, in, the, 60, ', s, because,\n",
            "\t\tof, E, ##28, ##29, ., [SEP], ass, ##ad, the, father, did, not, kill, tens, of, thousands, of,\n",
            "\t\t[MASK], own, citizens, in, one, week, in, el, ha, ##mma, in, s, ##yr, ##ia, because, of, E, ##28,\n",
            "\t\t##29, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 35. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1055it [00:08, 176.30it/s][CLS] an incredible number of people in the arab world believe that september 11 never happened or was an american provocation or even better a jewish plot . [SEP] you all remember the iraqi minister of information mr. mouhamad said al-sahaf and [MASK] press conferences when the us forces were already inside baghdad . [SEP] [[CLS], an, incredible, number, of, people, in, the, a, ##rab, world, believe, that, se, ##pt, ##em, ##ber, 11, never, happened, or, was, an, am, ##eric, ##an, pro, ##vocation, or, even, better, a, j, ##ew, ##ish, plot, ., [SEP], you, all, remember, the, i, ##ra, ##qi, minister, of, information, m, ##r, ., m, ##ou, ##ham, ##ad, said, al, -, sa, ##ha, ##f, and, [MASK], press, conferences, when, the, us, forces, were, already, inside, bag, ##h, ##dad, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 77 with text: \n",
            " \t\t[[CLS], an, incredible, number, of, people, in, the, a, ##rab, world, believe, that, se, ##pt, ##em,\n",
            "\t\t##ber, 11, never, happened, or, was, an, am, ##eric, ##an, pro, ##vocation, or, even, better, a, j,\n",
            "\t\t##ew, ##ish, plot, ., [SEP], you, all, remember, the, i, ##ra, ##qi, minister, of, information, m,\n",
            "\t\t##r, ., m, ##ou, ##ham, ##ad, said, al, -, sa, ##ha, ##f, and, [MASK], press, conferences, when,\n",
            "\t\tthe, us, forces, were, already, inside, bag, ##h, ##dad, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 62. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] disinformation at time of war is an accepted tactic . [SEP] but to stand day after day and to make such preposterous statements known to everybody to be lies without even being ridiculed in your own milieu can only happen in this region . [SEP] mr. sahaf eventually became a popular icon as a court jester [SEP] but this did not stop some allegedly respectable newspapers from giving [MASK] equal time . [SEP] [[CLS], di, ##sin, ##formation, at, time, of, war, is, an, accepted, tactic, ., [SEP], but, to, stand, day, after, day, and, to, make, such, pre, ##post, ##ero, ##us, statements, known, to, everybody, to, be, lies, without, even, being, rid, ##ic, ##ule, ##d, in, your, own, mi, ##lie, ##u, can, only, happen, in, this, region, ., [SEP], m, ##r, ., sa, ##ha, ##f, eventually, became, a, popular, icon, as, a, court, j, ##ester, [SEP], but, this, did, not, stop, some, allegedly, respectable, newspapers, from, giving, [MASK], equal, time, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 89 with text: \n",
            " \t\t[[CLS], di, ##sin, ##formation, at, time, of, war, is, an, accepted, tactic, ., [SEP], but, to,\n",
            "\t\tstand, day, after, day, and, to, make, such, pre, ##post, ##ero, ##us, statements, known, to,\n",
            "\t\teverybody, to, be, lies, without, even, being, rid, ##ic, ##ule, ##d, in, your, own, mi, ##lie, ##u,\n",
            "\t\tcan, only, happen, in, this, region, ., [SEP], m, ##r, ., sa, ##ha, ##f, eventually, became, a,\n",
            "\t\tpopular, icon, as, a, court, j, ##ester, [SEP], but, this, did, not, stop, some, allegedly,\n",
            "\t\trespectable, newspapers, from, giving, [MASK], equal, time, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 84. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] when these same leaders make other statements the western media report them as if they could be true . [SEP] it is a daily occurrence that the same people who finance arm and dispatch suicide murderers condemn the act in english in front of western tv cameras talking to a world audience which even partly believes them . [SEP] it is a daily routine to hear the same leader making opposite statements in arabic to [MASK] people and in english to the rest of the world . [SEP] [[CLS], when, these, same, leaders, make, other, statements, the, western, media, report, them, as, if, they, could, be, true, ., [SEP], it, is, a, daily, occurrence, that, the, same, people, who, finance, arm, and, dispatch, suicide, murderer, ##s, con, ##de, ##m, ##n, the, act, in, en, ##gli, ##sh, in, front, of, western, t, ##v, cameras, talking, to, a, world, audience, which, even, partly, believes, them, ., [SEP], it, is, a, daily, routine, to, hear, the, same, leader, making, opposite, statements, in, a, ##rab, ##ic, to, [MASK], people, and, in, en, ##gli, ##sh, to, the, rest, of, the, world, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], when, these, same, leaders, make, other, statements, the, western, media, report, them, as,\n",
            "\t\tif, they, could, be, true, ., [SEP], it, is, a, daily, occurrence, that, the, same, people, who,\n",
            "\t\tfinance, arm, and, dispatch, suicide, murderer, ##s, con, ##de, ##m, ##n, the, act, in, en, ##gli,\n",
            "\t\t##sh, in, front, of, western, t, ##v, cameras, talking, to, a, world, audience, which, even, partly,\n",
            "\t\tbelieves, them, ., [SEP], it, is, a, daily, routine, to, hear, the, same, leader, making, opposite,\n",
            "\t\tstatements, in, a, ##rab, ##ic, to, [MASK], people, and, in, en, ##gli, ##sh, to, the, rest, of,\n",
            "\t\tthe, world, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 85. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you may support or oppose the iraq war but to refer to fans of E6832 arafat or bin laden as peace activists is a bit too much . [SEP] a woman walks into an israeli restaurant in mid-day eats observes families with old people and children eating their lunch in the adjacent tables and pays the bill . [SEP] [MASK] then blows herself up killing 20 people including many children with heads and arms rolling around in the restaurant . [SEP] [[CLS], you, may, support, or, oppose, the, i, ##ra, ##q, war, but, to, refer, to, fans, of, E, ##6, ##8, ##32, a, ##ra, ##fa, ##t, or, bin, laden, as, peace, activists, is, a, bit, too, much, ., [SEP], a, woman, walks, into, an, is, ##rae, ##li, restaurant, in, mid, -, day, eats, observes, families, with, old, people, and, children, eating, their, lunch, in, the, adjacent, tables, and, pays, the, bill, ., [SEP], [MASK], then, blows, herself, up, killing, 20, people, including, many, children, with, heads, and, arms, rolling, around, in, the, restaurant, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 94 with text: \n",
            " \t\t[[CLS], you, may, support, or, oppose, the, i, ##ra, ##q, war, but, to, refer, to, fans, of, E, ##6,\n",
            "\t\t##8, ##32, a, ##ra, ##fa, ##t, or, bin, laden, as, peace, activists, is, a, bit, too, much, .,\n",
            "\t\t[SEP], a, woman, walks, into, an, is, ##rae, ##li, restaurant, in, mid, -, day, eats, observes,\n",
            "\t\tfamilies, with, old, people, and, children, eating, their, lunch, in, the, adjacent, tables, and,\n",
            "\t\tpays, the, bill, ., [SEP], [MASK], then, blows, herself, up, killing, 20, people, including, many,\n",
            "\t\tchildren, with, heads, and, arms, rolling, around, in, the, restaurant, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 72. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] is called `` martyr '' by several arab leaders and `` activist '' by the european press . [SEP] [[CLS], [MASK], is, called, `, `, martyr, ', ', by, several, a, ##rab, leaders, and, `, `, activist, ', ', by, the, euro, ##pe, ##an, press, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], [MASK], is, called, `, `, martyr, ', ', by, several, a, ##rab, leaders, and, `, `, activist,\n",
            "\t\t', ', by, the, euro, ##pe, ##an, press, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] dignitaries condemn the act but visit [MASK] bereaved family and the money flows . [SEP] [[CLS], dig, ##ni, ##tar, ##ies, con, ##de, ##m, ##n, the, act, but, visit, [MASK], be, ##rea, ##ved, family, and, the, money, flows, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], dig, ##ni, ##tar, ##ies, con, ##de, ##m, ##n, the, act, but, visit, [MASK], be, ##rea,\n",
            "\t\t##ved, family, and, the, money, flows, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the jihad `` soldiers '' join packaged death tours to iraq and other hotspots while some of their leaders ski in switzerland . [SEP] mrs. arafat who lives in paris with [MASK] daughter receives tens of thousands of dollars per month from the allegedly bankrupt palestinian authority while a typical local ringleader of the al-aksa brigade reporting to arafat receives only a cash payment of a couple of hundred dollars for performing murders at the retail level . [SEP] [[CLS], the, ji, ##had, `, `, soldiers, ', ', join, packaged, death, tours, to, i, ##ra, ##q, and, other, hot, ##sp, ##ots, while, some, of, their, leaders, ski, in, s, ##witz, ##erland, ., [SEP], m, ##rs, ., a, ##ra, ##fa, ##t, who, lives, in, par, ##is, with, [MASK], daughter, receives, tens, of, thousands, of, dollars, per, month, from, the, allegedly, bankrupt, pale, ##st, ##inian, authority, while, a, typical, local, ring, ##leader, of, the, al, -, a, ##ks, ##a, brigade, reporting, to, a, ##ra, ##fa, ##t, receives, only, a, cash, payment, of, a, couple, of, hundred, dollars, for, performing, murders, at, the, retail, level, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], the, ji, ##had, `, `, soldiers, ', ', join, packaged, death, tours, to, i, ##ra, ##q, and,\n",
            "\t\tother, hot, ##sp, ##ots, while, some, of, their, leaders, ski, in, s, ##witz, ##erland, ., [SEP], m,\n",
            "\t\t##rs, ., a, ##ra, ##fa, ##t, who, lives, in, par, ##is, with, [MASK], daughter, receives, tens, of,\n",
            "\t\tthousands, of, dollars, per, month, from, the, allegedly, bankrupt, pale, ##st, ##inian, authority,\n",
            "\t\twhile, a, typical, local, ring, ##leader, of, the, al, -, a, ##ks, ##a, brigade, reporting, to, a,\n",
            "\t\t##ra, ##fa, ##t, receives, only, a, cash, payment, of, a, couple, of, hundred, dollars, for,\n",
            "\t\tperforming, murders, at, the, retail, level, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 47. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] never in history not even in the nazi period was there such total disregard of all of the above as we observe now . [SEP] every student of political science debates how you prevent an anti-democratic force from winning a democratic election and abolishing democracy . [SEP] other aspects of a civilized society must also have limitations . [SEP] can a policeman open fire on someone trying to kill [MASK] ? [SEP] [[CLS], never, in, history, not, even, in, the, na, ##zi, period, was, there, such, total, di, ##s, ##regard, of, all, of, the, above, as, we, observe, now, ., [SEP], every, student, of, political, science, debates, how, you, prevent, an, anti, -, democratic, force, from, winning, a, democratic, election, and, a, ##bol, ##ishing, democracy, ., [SEP], other, aspects, of, a, civil, ##ized, society, must, also, have, limitations, ., [SEP], can, a, policeman, open, fire, on, someone, trying, to, kill, [MASK], ?, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 81 with text: \n",
            " \t\t[[CLS], never, in, history, not, even, in, the, na, ##zi, period, was, there, such, total, di, ##s,\n",
            "\t\t##regard, of, all, of, the, above, as, we, observe, now, ., [SEP], every, student, of, political,\n",
            "\t\tscience, debates, how, you, prevent, an, anti, -, democratic, force, from, winning, a, democratic,\n",
            "\t\telection, and, a, ##bol, ##ishing, democracy, ., [SEP], other, aspects, of, a, civil, ##ized,\n",
            "\t\tsociety, must, also, have, limitations, ., [SEP], can, a, policeman, open, fire, on, someone,\n",
            "\t\ttrying, to, kill, [MASK], ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 78. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but now we have an entire new set . [SEP] do you raid a mosque which serves as a terrorist ammunition storage ? [SEP] do you return fire if you are attacked from a hospital ? [SEP] do you storm a church taken over by terrorists who took the priests hostages ? [SEP] do you search every ambulance after a few suicide murderers use ambulances to reach their targets ? [SEP] do you strip every woman because one pretended to be pregnant and carried a suicide bomb on [MASK] belly ? [SEP] [[CLS], but, now, we, have, an, entire, new, set, ., [SEP], do, you, raid, a, mosque, which, serves, as, a, terrorist, ammunition, storage, ?, [SEP], do, you, return, fire, if, you, are, attacked, from, a, hospital, ?, [SEP], do, you, storm, a, church, taken, over, by, terrorists, who, took, the, priests, hostages, ?, [SEP], do, you, search, every, ambulance, after, a, few, suicide, murderer, ##s, use, ambulance, ##s, to, reach, their, targets, ?, [SEP], do, you, strip, every, woman, because, one, pretended, to, be, pregnant, and, carried, a, suicide, bomb, on, [MASK], belly, ?, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 95 with text: \n",
            " \t\t[[CLS], but, now, we, have, an, entire, new, set, ., [SEP], do, you, raid, a, mosque, which, serves,\n",
            "\t\tas, a, terrorist, ammunition, storage, ?, [SEP], do, you, return, fire, if, you, are, attacked,\n",
            "\t\tfrom, a, hospital, ?, [SEP], do, you, storm, a, church, taken, over, by, terrorists, who, took, the,\n",
            "\t\tpriests, hostages, ?, [SEP], do, you, search, every, ambulance, after, a, few, suicide, murderer,\n",
            "\t\t##s, use, ambulance, ##s, to, reach, their, targets, ?, [SEP], do, you, strip, every, woman,\n",
            "\t\tbecause, one, pretended, to, be, pregnant, and, carried, a, suicide, bomb, on, [MASK], belly, ?,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 91. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in the same way that no country has a law against cannibals eating its prime minister because such an act is unthinkable international law does not address killers shooting from hospitals mosques and ambulances while being protected by their government or society . [SEP] international law does not know how to handle someone who sends children to throw stones stands behind them and shoots with immunity and can not be arrested because [MASK] is sheltered by a government . [SEP] [[CLS], in, the, same, way, that, no, country, has, a, law, against, can, ##ni, ##bal, ##s, eating, its, prime, minister, because, such, an, act, is, un, ##thin, ##ka, ##ble, international, law, does, not, address, killers, shooting, from, hospitals, mosques, and, ambulance, ##s, while, being, protected, by, their, government, or, society, ., [SEP], international, law, does, not, know, how, to, handle, someone, who, sends, children, to, throw, stones, stands, behind, them, and, shoots, with, immunity, and, can, not, be, arrested, because, [MASK], is, sheltered, by, a, government, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 88 with text: \n",
            " \t\t[[CLS], in, the, same, way, that, no, country, has, a, law, against, can, ##ni, ##bal, ##s, eating,\n",
            "\t\tits, prime, minister, because, such, an, act, is, un, ##thin, ##ka, ##ble, international, law, does,\n",
            "\t\tnot, address, killers, shooting, from, hospitals, mosques, and, ambulance, ##s, while, being,\n",
            "\t\tprotected, by, their, government, or, society, ., [SEP], international, law, does, not, know, how,\n",
            "\t\tto, handle, someone, who, sends, children, to, throw, stones, stands, behind, them, and, shoots,\n",
            "\t\twith, immunity, and, can, not, be, arrested, because, [MASK], is, sheltered, by, a, government, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 80. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] what more will we hear of abu sifa now haditha has become the representative and inevitable example of honour 's exception ? [SEP] because along with haditha comes E4252 macbeth allegedly a former army ranger and iraq war veteran whose claims that massacre was method rather than madness rapidly went viral on the net . [SEP] [MASK] story was unsubstantiated and exteme yet plausible because it was extreme and provided a template to the pattern of force on exhibit in iraq . [SEP] [[CLS], what, more, will, we, hear, of, a, ##bu, si, ##fa, now, had, ##ith, ##a, has, become, the, representative, and, inevitable, example, of, honour, ', s, exception, ?, [SEP], because, along, with, had, ##ith, ##a, comes, E, ##42, ##5, ##2, mac, ##beth, allegedly, a, former, army, range, ##r, and, i, ##ra, ##q, war, veteran, whose, claims, that, massacre, was, method, rather, than, madness, rapidly, went, viral, on, the, net, ., [SEP], [MASK], story, was, un, ##su, ##bs, ##tant, ##iated, and, ex, ##tem, ##e, yet, plausible, because, it, was, extreme, and, provided, a, template, to, the, pattern, of, force, on, exhibit, in, i, ##ra, ##q, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 106 with text: \n",
            " \t\t[[CLS], what, more, will, we, hear, of, a, ##bu, si, ##fa, now, had, ##ith, ##a, has, become, the,\n",
            "\t\trepresentative, and, inevitable, example, of, honour, ', s, exception, ?, [SEP], because, along,\n",
            "\t\twith, had, ##ith, ##a, comes, E, ##42, ##5, ##2, mac, ##beth, allegedly, a, former, army, range,\n",
            "\t\t##r, and, i, ##ra, ##q, war, veteran, whose, claims, that, massacre, was, method, rather, than,\n",
            "\t\tmadness, rapidly, went, viral, on, the, net, ., [SEP], [MASK], story, was, un, ##su, ##bs, ##tant,\n",
            "\t\t##iated, and, ex, ##tem, ##e, yet, plausible, because, it, was, extreme, and, provided, a, template,\n",
            "\t\tto, the, pattern, of, force, on, exhibit, in, i, ##ra, ##q, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 71. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] a pattern rarely admitted by the E7282 's institutional media . [SEP] but macbeth it now appears is the pentagon 's timely strawman to buttress its case for haditha 's exceptionalism and to discredit influential anti-war voices such as iraq veterans against the war . [SEP] whether unaware or not of [MASK] status as a cointelpro asset it does n't matter [SEP] [[CLS], a, pattern, rarely, admitted, by, the, E, ##7, ##28, ##2, ', s, institutional, media, ., [SEP], but, mac, ##beth, it, now, appears, is, the, pen, ##tag, ##on, ', s, time, ##ly, straw, ##man, to, butt, ##ress, its, case, for, had, ##ith, ##a, ', s, exceptional, ##ism, and, to, disc, ##red, ##it, influential, anti, -, war, voices, such, as, i, ##ra, ##q, veterans, against, the, war, ., [SEP], whether, unaware, or, not, of, [MASK], status, as, a, coin, ##tel, ##p, ##ro, asset, it, does, n, ', t, matter, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 89 with text: \n",
            " \t\t[[CLS], a, pattern, rarely, admitted, by, the, E, ##7, ##28, ##2, ', s, institutional, media, .,\n",
            "\t\t[SEP], but, mac, ##beth, it, now, appears, is, the, pen, ##tag, ##on, ', s, time, ##ly, straw,\n",
            "\t\t##man, to, butt, ##ress, its, case, for, had, ##ith, ##a, ', s, exceptional, ##ism, and, to, disc,\n",
            "\t\t##red, ##it, influential, anti, -, war, voices, such, as, i, ##ra, ##q, veterans, against, the, war,\n",
            "\t\t., [SEP], whether, unaware, or, not, of, [MASK], status, as, a, coin, ##tel, ##p, ##ro, asset, it,\n",
            "\t\tdoes, n, ', t, matter, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 73. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] when the police state crushes [MASK] family we 'll all just laugh a say `` well fucker it just is ... '' [SEP] [[CLS], when, the, police, state, crush, ##es, [MASK], family, we, ', ll, all, just, laugh, a, say, `, `, well, fuck, ##er, it, just, is, ., ., ., ', ', [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 31 with text: \n",
            " \t\t[[CLS], when, the, police, state, crush, ##es, [MASK], family, we, ', ll, all, just, laugh, a, say,\n",
            "\t\t`, `, well, fuck, ##er, it, just, is, ., ., ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] now in darkness world stops turning [SEP] ashes where the bodies burning [SEP] no more war pigs have the power [SEP] hand of god has struck the hour [SEP] day of judgement god is calling [SEP] on their knees the war pigs crawling [SEP] begging mercy for their sins [SEP] satan laughing spreads [MASK] wings [SEP] [[CLS], now, in, darkness, world, stops, turning, [SEP], ashes, where, the, bodies, burning, [SEP], no, more, war, pigs, have, the, power, [SEP], hand, of, god, has, struck, the, hour, [SEP], day, of, judgement, god, is, calling, [SEP], on, their, knees, the, war, pigs, crawling, [SEP], begging, mercy, for, their, sins, [SEP], sat, ##an, laughing, spreads, [MASK], wings, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 58 with text: \n",
            " \t\t[[CLS], now, in, darkness, world, stops, turning, [SEP], ashes, where, the, bodies, burning, [SEP],\n",
            "\t\tno, more, war, pigs, have, the, power, [SEP], hand, of, god, has, struck, the, hour, [SEP], day, of,\n",
            "\t\tjudgement, god, is, calling, [SEP], on, their, knees, the, war, pigs, crawling, [SEP], begging,\n",
            "\t\tmercy, for, their, sins, [SEP], sat, ##an, laughing, spreads, [MASK], wings, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 55. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in the long run it may be that the middle eastern conflagrations are designed to harden the american people to the inevitability of genocide . [SEP] not that it really is inevitable : but it would make ruling a whole lot easier for a government controlled by the military front of the world 's largest private equity firm . [SEP] '' .. when the police state crushes [MASK] family .. '' [SEP] [[CLS], in, the, long, run, it, may, be, that, the, middle, eastern, con, ##f, ##lag, ##rations, are, designed, to, hard, ##en, the, am, ##eric, ##an, people, to, the, in, ##ev, ##ita, ##bility, of, genocide, ., [SEP], not, that, it, really, is, inevitable, :, but, it, would, make, ruling, a, whole, lot, easier, for, a, government, controlled, by, the, military, front, of, the, world, ', s, largest, private, equity, firm, ., [SEP], ', ', ., ., when, the, police, state, crush, ##es, [MASK], family, ., ., ', ', [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 88 with text: \n",
            " \t\t[[CLS], in, the, long, run, it, may, be, that, the, middle, eastern, con, ##f, ##lag, ##rations,\n",
            "\t\tare, designed, to, hard, ##en, the, am, ##eric, ##an, people, to, the, in, ##ev, ##ita, ##bility,\n",
            "\t\tof, genocide, ., [SEP], not, that, it, really, is, inevitable, :, but, it, would, make, ruling, a,\n",
            "\t\twhole, lot, easier, for, a, government, controlled, by, the, military, front, of, the, world, ', s,\n",
            "\t\tlargest, private, equity, firm, ., [SEP], ', ', ., ., when, the, police, state, crush, ##es, [MASK],\n",
            "\t\tfamily, ., ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 81. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] although i do not agree with h / hr ethic morality there are too many people on earth right now [SEP] that is if they all want to have a car and drive to the mall - and a lot of them are waking up to that idea which as it stands now is impossible . [SEP] [MASK] nonchelant benign god force mannerism is appalling and is apparent in the neo-con world of the project for a new american century which if i 'm not mistaken did address the issues of exactly what is being said here . [SEP] [[CLS], although, i, do, not, agree, with, h, /, h, ##r, et, ##hic, morality, there, are, too, many, people, on, earth, right, now, [SEP], that, is, if, they, all, want, to, have, a, car, and, drive, to, the, mall, -, and, a, lot, of, them, are, waking, up, to, that, idea, which, as, it, stands, now, is, impossible, ., [SEP], [MASK], non, ##chel, ##ant, ben, ##ign, god, force, manner, ##ism, is, app, ##all, ##ing, and, is, apparent, in, the, neo, -, con, world, of, the, project, for, a, new, am, ##eric, ##an, century, which, if, i, ', m, not, mistaken, did, address, the, issues, of, exactly, what, is, being, said, here, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 113 with text: \n",
            " \t\t[[CLS], although, i, do, not, agree, with, h, /, h, ##r, et, ##hic, morality, there, are, too, many,\n",
            "\t\tpeople, on, earth, right, now, [SEP], that, is, if, they, all, want, to, have, a, car, and, drive,\n",
            "\t\tto, the, mall, -, and, a, lot, of, them, are, waking, up, to, that, idea, which, as, it, stands,\n",
            "\t\tnow, is, impossible, ., [SEP], [MASK], non, ##chel, ##ant, ben, ##ign, god, force, manner, ##ism,\n",
            "\t\tis, app, ##all, ##ing, and, is, apparent, in, the, neo, -, con, world, of, the, project, for, a,\n",
            "\t\tnew, am, ##eric, ##an, century, which, if, i, ', m, not, mistaken, did, address, the, issues, of,\n",
            "\t\texactly, what, is, being, said, here, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 60. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i would go even further than this : [SEP] `` war is the mobilization of immorality . '' [SEP] would these apologists for such killings have been satisfied if the same deaths to the same innocents had been brought about by aerial bombs ? [SEP] i am reminded of a distinction one of my students tried making a few years ago in contrasting suicide bombers with militarily disciplined killing : [SEP] `` at least we have the decency to drop bombs from airplanes '' [MASK] declared . [SEP] [[CLS], i, would, go, even, further, than, this, :, [SEP], `, `, war, is, the, mob, ##ilization, of, im, ##mor, ##ality, ., ', ', [SEP], would, these, a, ##pol, ##ogist, ##s, for, such, killings, have, been, satisfied, if, the, same, deaths, to, the, same, innocent, ##s, had, been, brought, about, by, aerial, bombs, ?, [SEP], i, am, reminded, of, a, distinction, one, of, my, students, tried, making, a, few, years, ago, in, contrasting, suicide, bombers, with, mi, ##lit, ##arily, discipline, ##d, killing, :, [SEP], `, `, at, least, we, have, the, de, ##ce, ##ncy, to, drop, bombs, from, airplane, ##s, ', ', [MASK], declared, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 106 with text: \n",
            " \t\t[[CLS], i, would, go, even, further, than, this, :, [SEP], `, `, war, is, the, mob, ##ilization, of,\n",
            "\t\tim, ##mor, ##ality, ., ', ', [SEP], would, these, a, ##pol, ##ogist, ##s, for, such, killings, have,\n",
            "\t\tbeen, satisfied, if, the, same, deaths, to, the, same, innocent, ##s, had, been, brought, about, by,\n",
            "\t\taerial, bombs, ?, [SEP], i, am, reminded, of, a, distinction, one, of, my, students, tried, making,\n",
            "\t\ta, few, years, ago, in, contrasting, suicide, bombers, with, mi, ##lit, ##arily, discipline, ##d,\n",
            "\t\tkilling, :, [SEP], `, `, at, least, we, have, the, de, ##ce, ##ncy, to, drop, bombs, from, airplane,\n",
            "\t\t##s, ', ', [MASK], declared, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 102. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] out of a sense of moral decency this weekend might be an appropriate occasion to remove such symbols from their cars homes and - better yet - their minds ! [SEP] question : did u.s. troops in ww2 shoot children while their mothers tried to shield them screaming for mercy ? [SEP] remember maggot the telly savalas rapist - character from `` the dirty dozen ? '' [SEP] E3977 brown machine - gunned [MASK] for messing with a german woman in a simple act of field justice . [SEP] [[CLS], out, of, a, sense, of, moral, de, ##ce, ##ncy, this, weekend, might, be, an, appropriate, occasion, to, remove, such, symbols, from, their, cars, homes, and, -, better, yet, -, their, minds, !, [SEP], question, :, did, u, ., s, ., troops, in, w, ##w, ##2, shoot, children, while, their, mothers, tried, to, shield, them, screaming, for, mercy, ?, [SEP], remember, ma, ##gg, ##ot, the, tell, ##y, sa, ##val, ##as, rap, ##ist, -, character, from, `, `, the, dirty, dozen, ?, ', ', [SEP], E, ##39, ##7, ##7, brown, machine, -, gun, ##ned, [MASK], for, messing, with, a, g, ##erman, woman, in, a, simple, act, of, field, justice, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 110 with text: \n",
            " \t\t[[CLS], out, of, a, sense, of, moral, de, ##ce, ##ncy, this, weekend, might, be, an, appropriate,\n",
            "\t\toccasion, to, remove, such, symbols, from, their, cars, homes, and, -, better, yet, -, their, minds,\n",
            "\t\t!, [SEP], question, :, did, u, ., s, ., troops, in, w, ##w, ##2, shoot, children, while, their,\n",
            "\t\tmothers, tried, to, shield, them, screaming, for, mercy, ?, [SEP], remember, ma, ##gg, ##ot, the,\n",
            "\t\ttell, ##y, sa, ##val, ##as, rap, ##ist, -, character, from, `, `, the, dirty, dozen, ?, ', ', [SEP],\n",
            "\t\tE, ##39, ##7, ##7, brown, machine, -, gun, ##ned, [MASK], for, messing, with, a, g, ##erman, woman,\n",
            "\t\tin, a, simple, act, of, field, justice, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 93. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1073it [00:08, 138.33it/s][CLS] that sort of moral action seemed fairly unremarkable to people back then . [SEP] fast - forward to today 's gi : innured to violence by a lifetime of ultra-violent `` kill E6094 '' movies ultra-violent fps video games [MASK] has become the thing an earlier generation was taught was most detestable : a nazi storm trooper a self - appointed ss death 's - head executioner . [SEP] [[CLS], that, sort, of, moral, action, seemed, fairly, un, ##rem, ##ark, ##able, to, people, back, then, ., [SEP], fast, -, forward, to, today, ', s, g, ##i, :, inn, ##ured, to, violence, by, a, lifetime, of, ultra, -, violent, `, `, kill, E, ##60, ##9, ##4, ', ', movies, ultra, -, violent, f, ##ps, video, games, [MASK], has, become, the, thing, an, earlier, generation, was, taught, was, most, de, ##test, ##able, :, a, na, ##zi, storm, troop, ##er, a, self, -, appointed, s, ##s, death, ', s, -, head, execution, ##er, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 93 with text: \n",
            " \t\t[[CLS], that, sort, of, moral, action, seemed, fairly, un, ##rem, ##ark, ##able, to, people, back,\n",
            "\t\tthen, ., [SEP], fast, -, forward, to, today, ', s, g, ##i, :, inn, ##ured, to, violence, by, a,\n",
            "\t\tlifetime, of, ultra, -, violent, `, `, kill, E, ##60, ##9, ##4, ', ', movies, ultra, -, violent, f,\n",
            "\t\t##ps, video, games, [MASK], has, become, the, thing, an, earlier, generation, was, taught, was,\n",
            "\t\tmost, de, ##test, ##able, :, a, na, ##zi, storm, troop, ##er, a, self, -, appointed, s, ##s, death,\n",
            "\t\t', s, -, head, execution, ##er, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 56. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] bereft of any sense of the sacred [MASK] shoots little girls without compunction . [SEP] [[CLS], be, ##re, ##ft, of, any, sense, of, the, sacred, [MASK], shoots, little, girls, without, com, ##pu, ##nc, ##tion, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], be, ##re, ##ft, of, any, sense, of, the, sacred, [MASK], shoots, little, girls, without,\n",
            "\t\tcom, ##pu, ##nc, ##tion, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] is quite literally insane a `` natural born killer . '' [SEP] [[CLS], [MASK], is, quite, literally, insane, a, `, `, natural, born, killer, ., ', ', [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], [MASK], is, quite, literally, insane, a, `, `, natural, born, killer, ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] very clever . [SEP] so instead of thousands of us soldiers snapping and killing the odd iraqi civillian or five . we have claims that many or all us soldiers are murdering iraqis by the hundreds . [SEP] if that was the case there would be fuck all people left in iraq . [SEP] obviously crap spoken by a nutter anything [MASK] says or anything that comes from a similar place belongs in the same file `` batshit crazy rubbish designed to discredit us '' . [SEP] [[CLS], very, clever, ., [SEP], so, instead, of, thousands, of, us, soldiers, snapping, and, killing, the, odd, i, ##ra, ##qi, civil, ##lian, or, five, ., we, have, claims, that, many, or, all, us, soldiers, are, murdering, i, ##ra, ##qi, ##s, by, the, hundreds, ., [SEP], if, that, was, the, case, there, would, be, fuck, all, people, left, in, i, ##ra, ##q, ., [SEP], obviously, crap, spoken, by, a, nut, ##ter, anything, [MASK], says, or, anything, that, comes, from, a, similar, place, belongs, in, the, same, file, `, `, bats, ##hit, crazy, rub, ##bish, designed, to, disc, ##red, ##it, us, ', ', ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], very, clever, ., [SEP], so, instead, of, thousands, of, us, soldiers, snapping, and,\n",
            "\t\tkilling, the, odd, i, ##ra, ##qi, civil, ##lian, or, five, ., we, have, claims, that, many, or, all,\n",
            "\t\tus, soldiers, are, murdering, i, ##ra, ##qi, ##s, by, the, hundreds, ., [SEP], if, that, was, the,\n",
            "\t\tcase, there, would, be, fuck, all, people, left, in, i, ##ra, ##q, ., [SEP], obviously, crap,\n",
            "\t\tspoken, by, a, nut, ##ter, anything, [MASK], says, or, anything, that, comes, from, a, similar,\n",
            "\t\tplace, belongs, in, the, same, file, `, `, bats, ##hit, crazy, rub, ##bish, designed, to, disc,\n",
            "\t\t##red, ##it, us, ', ', ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 71. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] pity those armies only exist in niave children 's imaginations or fantasy novels set in mythical worlds . [SEP] all this babbling here will remedy nothing . [SEP] ask god 's forgiveness . [SEP] get out while you can . [SEP] revelation 18 : 18:1 and after these things i saw another angel come down from heaven having great power ; [SEP] and the earth was lightened with [MASK] glory . [SEP] [[CLS], pity, those, armies, only, exist, in, ni, ##ave, children, ', s, imagination, ##s, or, fantasy, novels, set, in, mythical, worlds, ., [SEP], all, this, b, ##ab, ##bling, here, will, remedy, nothing, ., [SEP], ask, god, ', s, forgiveness, ., [SEP], get, out, while, you, can, ., [SEP], revelation, 18, :, 18, :, 1, and, after, these, things, i, saw, another, angel, come, down, from, heaven, having, great, power, ;, [SEP], and, the, earth, was, light, ##ened, with, [MASK], glory, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 82 with text: \n",
            " \t\t[[CLS], pity, those, armies, only, exist, in, ni, ##ave, children, ', s, imagination, ##s, or,\n",
            "\t\tfantasy, novels, set, in, mythical, worlds, ., [SEP], all, this, b, ##ab, ##bling, here, will,\n",
            "\t\tremedy, nothing, ., [SEP], ask, god, ', s, forgiveness, ., [SEP], get, out, while, you, can, .,\n",
            "\t\t[SEP], revelation, 18, :, 18, :, 1, and, after, these, things, i, saw, another, angel, come, down,\n",
            "\t\tfrom, heaven, having, great, power, ;, [SEP], and, the, earth, was, light, ##ened, with, [MASK],\n",
            "\t\tglory, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 78. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] 18:2 and [MASK] cried mightily with a strong voice saying babylon the great is fallen is fallen and is become the habitation of devils and the hold of every foul spirit and a cage of every unclean and hateful bird . [SEP] [[CLS], 18, :, 2, and, [MASK], cried, might, ##ily, with, a, strong, voice, saying, baby, ##lon, the, great, is, fallen, is, fallen, and, is, become, the, habitat, ##ion, of, devil, ##s, and, the, hold, of, every, foul, spirit, and, a, cage, of, every, uncle, ##an, and, hate, ##ful, bird, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 51 with text: \n",
            " \t\t[[CLS], 18, :, 2, and, [MASK], cried, might, ##ily, with, a, strong, voice, saying, baby, ##lon,\n",
            "\t\tthe, great, is, fallen, is, fallen, and, is, become, the, habitat, ##ion, of, devil, ##s, and, the,\n",
            "\t\thold, of, every, foul, spirit, and, a, cage, of, every, uncle, ##an, and, hate, ##ful, bird, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] 18:3 for all nations have drunk of the wine of the wrath of [MASK] fornication [SEP] [[CLS], 18, :, 3, for, all, nations, have, drunk, of, the, wine, of, the, wrath, of, [MASK], for, ##nica, ##tion, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], 18, :, 3, for, all, nations, have, drunk, of, the, wine, of, the, wrath, of, [MASK], for,\n",
            "\t\t##nica, ##tion, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 16. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and the kings of the earth have committed fornication with [MASK] [SEP] [[CLS], and, the, kings, of, the, earth, have, committed, for, ##nica, ##tion, with, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], and, the, kings, of, the, earth, have, committed, for, ##nica, ##tion, with, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and the merchants of the earth are waxed rich through the abundance of [MASK] delicacies . [SEP] [[CLS], and, the, merchants, of, the, earth, are, wax, ##ed, rich, through, the, abundance, of, [MASK], del, ##ica, ##cies, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], and, the, merchants, of, the, earth, are, wax, ##ed, rich, through, the, abundance, of,\n",
            "\t\t[MASK], del, ##ica, ##cies, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] -lrb- actual presidential quote -rrb- . [SEP] please people there is no conspiracy here : just a bunch of morons realizing they have unleashed horror on innocents and trying to cover their asses . [SEP] why do you think only lowly soldiers and no generals have been convicted ? [SEP] because generals have the power that 's why ! [SEP] so one soldier stood up and talked about some razed shithole in iraq ? [SEP] maybe [MASK] is going overboard and creating a fancy tale of horror ? [SEP] [[CLS], -, l, ##rb, -, actual, presidential, quote, -, r, ##rb, -, ., [SEP], please, people, there, is, no, conspiracy, here, :, just, a, bunch, of, m, ##oro, ##ns, realizing, they, have, un, ##leashed, horror, on, innocent, ##s, and, trying, to, cover, their, ass, ##es, ., [SEP], why, do, you, think, only, low, ##ly, soldiers, and, no, generals, have, been, convicted, ?, [SEP], because, generals, have, the, power, that, ', s, why, !, [SEP], so, one, soldier, stood, up, and, talked, about, some, r, ##azed, shit, ##hole, in, i, ##ra, ##q, ?, [SEP], maybe, [MASK], is, going, over, ##board, and, creating, a, fancy, tale, of, horror, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 108 with text: \n",
            " \t\t[[CLS], -, l, ##rb, -, actual, presidential, quote, -, r, ##rb, -, ., [SEP], please, people, there,\n",
            "\t\tis, no, conspiracy, here, :, just, a, bunch, of, m, ##oro, ##ns, realizing, they, have, un,\n",
            "\t\t##leashed, horror, on, innocent, ##s, and, trying, to, cover, their, ass, ##es, ., [SEP], why, do,\n",
            "\t\tyou, think, only, low, ##ly, soldiers, and, no, generals, have, been, convicted, ?, [SEP], because,\n",
            "\t\tgenerals, have, the, power, that, ', s, why, !, [SEP], so, one, soldier, stood, up, and, talked,\n",
            "\t\tabout, some, r, ##azed, shit, ##hole, in, i, ##ra, ##q, ?, [SEP], maybe, [MASK], is, going, over,\n",
            "\t\t##board, and, creating, a, fancy, tale, of, horror, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 94. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in your little history lesson -lrb- why not go back to the romans who slaughtered everyone in carthage or for that matter the israelites in the ot commanded by yawheh to slaughter man women and child ? -rrb- you mention the bombing of dresden . [SEP] i 'll toss in hiroshima and nagasaki [SEP] but those pilot - murderers were at one remove from the slaughter they were uh enabling : [SEP] they did n't have to look at the mother trying to shield [MASK] kids before pulling the trigger . [SEP] [[CLS], in, your, little, history, lesson, -, l, ##rb, -, why, not, go, back, to, the, r, ##oman, ##s, who, slaughtered, everyone, in, cart, ##hage, or, for, that, matter, the, is, ##rae, ##lite, ##s, in, the, o, ##t, commanded, by, ya, ##w, ##he, ##h, to, slaughter, man, women, and, child, ?, -, r, ##rb, -, you, mention, the, bombing, of, d, ##res, ##den, ., [SEP], i, ', ll, toss, in, hi, ##ros, ##hima, and, na, ##gas, ##aki, [SEP], but, those, pilot, -, murderer, ##s, were, at, one, remove, from, the, slaughter, they, were, uh, enabling, :, [SEP], they, did, n, ', t, have, to, look, at, the, mother, trying, to, shield, [MASK], kids, before, pulling, the, trigger, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 119 with text: \n",
            " \t\t[[CLS], in, your, little, history, lesson, -, l, ##rb, -, why, not, go, back, to, the, r, ##oman,\n",
            "\t\t##s, who, slaughtered, everyone, in, cart, ##hage, or, for, that, matter, the, is, ##rae, ##lite,\n",
            "\t\t##s, in, the, o, ##t, commanded, by, ya, ##w, ##he, ##h, to, slaughter, man, women, and, child, ?,\n",
            "\t\t-, r, ##rb, -, you, mention, the, bombing, of, d, ##res, ##den, ., [SEP], i, ', ll, toss, in, hi,\n",
            "\t\t##ros, ##hima, and, na, ##gas, ##aki, [SEP], but, those, pilot, -, murderer, ##s, were, at, one,\n",
            "\t\tremove, from, the, slaughter, they, were, uh, enabling, :, [SEP], they, did, n, ', t, have, to,\n",
            "\t\tlook, at, the, mother, trying, to, shield, [MASK], kids, before, pulling, the, trigger, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 111. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no one `` turns off the tv '' before first realizing that tv is unhealthy -lrb- E819 mander 's book is brilliant in this regard -rrb- just as no one will `` avert their eyes from the imperium '' without realizing that empire - worship is nothing but moloch - worship . [SEP] moloch ? [SEP] indeed . [SEP] as mother E5835 put it `` if we accept that a mother can kill even [MASK] own child how can we tell people not to kill one another ? [SEP] [[CLS], no, one, `, `, turns, off, the, t, ##v, ', ', before, first, realizing, that, t, ##v, is, un, ##hea, ##lt, ##hy, -, l, ##rb, -, E, ##8, ##19, man, ##der, ', s, book, is, brilliant, in, this, regard, -, r, ##rb, -, just, as, no, one, will, `, `, a, ##vert, their, eyes, from, the, imp, ##eri, ##um, ', ', without, realizing, that, empire, -, worship, is, nothing, but, m, ##olo, ##ch, -, worship, ., [SEP], m, ##olo, ##ch, ?, [SEP], indeed, ., [SEP], as, mother, E, ##5, ##8, ##35, put, it, `, `, if, we, accept, that, a, mother, can, kill, even, [MASK], own, child, how, can, we, tell, people, not, to, kill, one, another, ?, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 120 with text: \n",
            " \t\t[[CLS], no, one, `, `, turns, off, the, t, ##v, ', ', before, first, realizing, that, t, ##v, is,\n",
            "\t\tun, ##hea, ##lt, ##hy, -, l, ##rb, -, E, ##8, ##19, man, ##der, ', s, book, is, brilliant, in, this,\n",
            "\t\tregard, -, r, ##rb, -, just, as, no, one, will, `, `, a, ##vert, their, eyes, from, the, imp, ##eri,\n",
            "\t\t##um, ', ', without, realizing, that, empire, -, worship, is, nothing, but, m, ##olo, ##ch, -,\n",
            "\t\tworship, ., [SEP], m, ##olo, ##ch, ?, [SEP], indeed, ., [SEP], as, mother, E, ##5, ##8, ##35, put,\n",
            "\t\tit, `, `, if, we, accept, that, a, mother, can, kill, even, [MASK], own, child, how, can, we, tell,\n",
            "\t\tpeople, not, to, kill, one, another, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 105. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but please do n't insult our intelligence by claiming all of this could be mitigated if we just worshipped some god more frequently and properly . [SEP] a couple of years prior one of the neighbor 's kids was trick or treat 'n dressed as bible man ..... [SEP] yeah that 's right [SEP] you heard it ... [SEP] bible man . [SEP] bible man bible man spin me a verse as fast as you can [SEP] [MASK] reward [SEP] [[CLS], but, please, do, n, ', t, insult, our, intelligence, by, claiming, all, of, this, could, be, mit, ##igate, ##d, if, we, just, worshipped, some, god, more, frequently, and, properly, ., [SEP], a, couple, of, years, prior, one, of, the, neighbor, ', s, kids, was, trick, or, treat, ', n, dressed, as, bi, ##ble, man, ., ., ., ., ., [SEP], yeah, that, ', s, right, [SEP], you, heard, it, ., ., ., [SEP], bi, ##ble, man, ., [SEP], bi, ##ble, man, bi, ##ble, man, spin, me, a, verse, as, fast, as, you, can, [SEP], [MASK], reward, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 98 with text: \n",
            " \t\t[[CLS], but, please, do, n, ', t, insult, our, intelligence, by, claiming, all, of, this, could, be,\n",
            "\t\tmit, ##igate, ##d, if, we, just, worshipped, some, god, more, frequently, and, properly, ., [SEP],\n",
            "\t\ta, couple, of, years, prior, one, of, the, neighbor, ', s, kids, was, trick, or, treat, ', n,\n",
            "\t\tdressed, as, bi, ##ble, man, ., ., ., ., ., [SEP], yeah, that, ', s, right, [SEP], you, heard, it,\n",
            "\t\t., ., ., [SEP], bi, ##ble, man, ., [SEP], bi, ##ble, man, bi, ##ble, man, spin, me, a, verse, as,\n",
            "\t\tfast, as, you, can, [SEP], [MASK], reward, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 95. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no reward [SEP] saving souls is [MASK] only reward [SEP] [[CLS], no, reward, [SEP], saving, souls, is, [MASK], only, reward, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], no, reward, [SEP], saving, souls, is, [MASK], only, reward, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] `` out damned spot ! [SEP] out i say ! '' [SEP] http://yooha.meepzorp.com/out-spot/index_files/out-spot.jpg [SEP] anonymous 3:10pm said [SEP] heaven is filled with children ... and the people who were not afraid to protect them at any cost . [SEP] so by definition your precious god is not there with them in heaven since [MASK] failed to protect them at all costs or any cost for that matter . [SEP] [[CLS], `, `, out, damned, spot, !, [SEP], out, i, say, !, ', ', [SEP], http, :, /, /, yo, ##oh, ##a, ., me, ##ep, ##zor, ##p, ., com, /, out, -, spot, /, index, _, files, /, out, -, spot, ., j, ##p, ##g, [SEP], anonymous, 3, :, 10, ##pm, said, [SEP], heaven, is, filled, with, children, ., ., ., and, the, people, who, were, not, afraid, to, protect, them, at, any, cost, ., [SEP], so, by, definition, your, precious, god, is, not, there, with, them, in, heaven, since, [MASK], failed, to, protect, them, at, all, costs, or, any, cost, for, that, matter, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 106 with text: \n",
            " \t\t[[CLS], `, `, out, damned, spot, !, [SEP], out, i, say, !, ', ', [SEP], http, :, /, /, yo, ##oh,\n",
            "\t\t##a, ., me, ##ep, ##zor, ##p, ., com, /, out, -, spot, /, index, _, files, /, out, -, spot, ., j,\n",
            "\t\t##p, ##g, [SEP], anonymous, 3, :, 10, ##pm, said, [SEP], heaven, is, filled, with, children, ., .,\n",
            "\t\t., and, the, people, who, were, not, afraid, to, protect, them, at, any, cost, ., [SEP], so, by,\n",
            "\t\tdefinition, your, precious, god, is, not, there, with, them, in, heaven, since, [MASK], failed, to,\n",
            "\t\tprotect, them, at, all, costs, or, any, cost, for, that, matter, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 90. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] is cost even relevant to this god of yours ? [SEP] either way your god is disqualified by your qualifications for entrance into heaven . [SEP] i have a couple of questions . [SEP] is E2404 E5162 in heaven ? [SEP] was E310 benatar incorrect when [MASK] said `` hell is for children ? '' [SEP] [[CLS], is, cost, even, relevant, to, this, god, of, yours, ?, [SEP], either, way, your, god, is, disqualified, by, your, qualifications, for, entrance, into, heaven, ., [SEP], i, have, a, couple, of, questions, ., [SEP], is, E, ##24, ##0, ##4, E, ##51, ##6, ##2, in, heaven, ?, [SEP], was, E, ##31, ##0, ben, ##ata, ##r, incorrect, when, [MASK], said, `, `, hell, is, for, children, ?, ', ', [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 69 with text: \n",
            " \t\t[[CLS], is, cost, even, relevant, to, this, god, of, yours, ?, [SEP], either, way, your, god, is,\n",
            "\t\tdisqualified, by, your, qualifications, for, entrance, into, heaven, ., [SEP], i, have, a, couple,\n",
            "\t\tof, questions, ., [SEP], is, E, ##24, ##0, ##4, E, ##51, ##6, ##2, in, heaven, ?, [SEP], was, E,\n",
            "\t\t##31, ##0, ben, ##ata, ##r, incorrect, when, [MASK], said, `, `, hell, is, for, children, ?, ', ',\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 57. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1089it [00:08, 116.59it/s][CLS] hear hear . [SEP] start making the connections people between http://www.abortionno.org/resources/pictures_2.html this and http://newsimg.bbc.co.uk/media/images/41186000/jpg/_41186472_kar-child-ap416.jpg this : [SEP] that 's what ' intuition ' is all about ... [SEP] dude . [SEP] E310 benatar ? [SEP] hell is paved with the skulls of crapazoid musicians like [MASK] . [SEP] [[CLS], hear, hear, ., [SEP], start, making, the, connections, people, between, http, :, /, /, www, ., abortion, ##no, ., org, /, resources, /, pictures, _, 2, ., html, this, and, http, :, /, /, news, ##im, ##g, ., b, ##b, ##c, ., co, ., uk, /, media, /, images, /, 41, ##18, ##60, ##00, /, j, ##p, ##g, /, _, 41, ##18, ##64, ##7, ##2, _, ka, ##r, -, child, -, a, ##p, ##41, ##6, ., j, ##p, ##g, this, :, [SEP], that, ', s, what, ', in, ##tu, ##ition, ', is, all, about, ., ., ., [SEP], dude, ., [SEP], E, ##31, ##0, ben, ##ata, ##r, ?, [SEP], hell, is, paved, with, the, skull, ##s, of, crap, ##az, ##oid, musicians, like, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 126 with text: \n",
            " \t\t[[CLS], hear, hear, ., [SEP], start, making, the, connections, people, between, http, :, /, /, www,\n",
            "\t\t., abortion, ##no, ., org, /, resources, /, pictures, _, 2, ., html, this, and, http, :, /, /, news,\n",
            "\t\t##im, ##g, ., b, ##b, ##c, ., co, ., uk, /, media, /, images, /, 41, ##18, ##60, ##00, /, j, ##p,\n",
            "\t\t##g, /, _, 41, ##18, ##64, ##7, ##2, _, ka, ##r, -, child, -, a, ##p, ##41, ##6, ., j, ##p, ##g,\n",
            "\t\tthis, :, [SEP], that, ', s, what, ', in, ##tu, ##ition, ', is, all, about, ., ., ., [SEP], dude, .,\n",
            "\t\t[SEP], E, ##31, ##0, ben, ##ata, ##r, ?, [SEP], hell, is, paved, with, the, skull, ##s, of, crap,\n",
            "\t\t##az, ##oid, musicians, like, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 123. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh stop with blaming god for the shithole humanity has made of this world -- [SEP] all of you who 'd like to bash god or demons or angels or aliens need to look into the mirror to find out why the ground you walk on is cursed . [SEP] `` god '' never did shit to you that you know of [SEP] but you 're quick to blame [MASK] for your fucked up life . [SEP] [[CLS], oh, stop, with, b, ##laming, god, for, the, shit, ##hole, humanity, has, made, of, this, world, -, -, [SEP], all, of, you, who, ', d, like, to, b, ##ash, god, or, demons, or, angels, or, aliens, need, to, look, into, the, mirror, to, find, out, why, the, ground, you, walk, on, is, cursed, ., [SEP], `, `, god, ', ', never, did, shit, to, you, that, you, know, of, [SEP], but, you, ', re, quick, to, blame, [MASK], for, your, fucked, up, life, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 86 with text: \n",
            " \t\t[[CLS], oh, stop, with, b, ##laming, god, for, the, shit, ##hole, humanity, has, made, of, this,\n",
            "\t\tworld, -, -, [SEP], all, of, you, who, ', d, like, to, b, ##ash, god, or, demons, or, angels, or,\n",
            "\t\taliens, need, to, look, into, the, mirror, to, find, out, why, the, ground, you, walk, on, is,\n",
            "\t\tcursed, ., [SEP], `, `, god, ', ', never, did, shit, to, you, that, you, know, of, [SEP], but, you,\n",
            "\t\t', re, quick, to, blame, [MASK], for, your, fucked, up, life, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 78. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] there is a connection between this and this [SEP] moloch indeed ... this [SEP] it 's memorial day ! [SEP] wall - to - wall war movies ! [SEP] i just saw again that quaint little scene in ' bridge over the river kwai ' where E6602 guinness tries to read the E4315 convention to the fanatic japanese commander . [SEP] the japanese throws the book down and slaps E6602 guinness in the face . [SEP] imagine a prisoner at abu graib or gitmo trying to read the g.c. to [MASK] american interrogator . [SEP] [[CLS], there, is, a, connection, between, this, and, this, [SEP], m, ##olo, ##ch, indeed, ., ., ., this, [SEP], it, ', s, memorial, day, !, [SEP], wall, -, to, -, wall, war, movies, !, [SEP], i, just, saw, again, that, q, ##ua, ##int, little, scene, in, ', bridge, over, the, river, k, ##wai, ', where, E, ##6, ##60, ##2, g, ##uin, ##ness, tries, to, read, the, E, ##43, ##15, convention, to, the, fan, ##atic, j, ##apa, ##nese, commander, ., [SEP], the, j, ##apa, ##nese, throws, the, book, down, and, slap, ##s, E, ##6, ##60, ##2, g, ##uin, ##ness, in, the, face, ., [SEP], imagine, a, prisoner, at, a, ##bu, g, ##rai, ##b, or, g, ##it, ##mo, trying, to, read, the, g, ., c, ., to, [MASK], am, ##eric, ##an, inter, ##rog, ##ator, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 134 with text: \n",
            " \t\t[[CLS], there, is, a, connection, between, this, and, this, [SEP], m, ##olo, ##ch, indeed, ., ., .,\n",
            "\t\tthis, [SEP], it, ', s, memorial, day, !, [SEP], wall, -, to, -, wall, war, movies, !, [SEP], i,\n",
            "\t\tjust, saw, again, that, q, ##ua, ##int, little, scene, in, ', bridge, over, the, river, k, ##wai, ',\n",
            "\t\twhere, E, ##6, ##60, ##2, g, ##uin, ##ness, tries, to, read, the, E, ##43, ##15, convention, to,\n",
            "\t\tthe, fan, ##atic, j, ##apa, ##nese, commander, ., [SEP], the, j, ##apa, ##nese, throws, the, book,\n",
            "\t\tdown, and, slap, ##s, E, ##6, ##60, ##2, g, ##uin, ##ness, in, the, face, ., [SEP], imagine, a,\n",
            "\t\tprisoner, at, a, ##bu, g, ##rai, ##b, or, g, ##it, ##mo, trying, to, read, the, g, ., c, ., to,\n",
            "\t\t[MASK], am, ##eric, ##an, inter, ##rog, ##ator, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 125. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i believe there is something more than me or us but what that something constitutes is pure conjecture . [SEP] the concept of god made in man 's image is a conspiracy plain and simple . [SEP] a time - honored strategy to control the masses . [SEP] E4153 koresh may very well be the next E7232 figure several hundred years from now [SEP] and if we could be there then we would tell the orthodoxy created around [MASK] legend that they are insane fools ...... [SEP] [[CLS], i, believe, there, is, something, more, than, me, or, us, but, what, that, something, constitutes, is, pure, conjecture, ., [SEP], the, concept, of, god, made, in, man, ', s, image, is, a, conspiracy, plain, and, simple, ., [SEP], a, time, -, honored, strategy, to, control, the, masses, ., [SEP], E, ##41, ##53, k, ##ores, ##h, may, very, well, be, the, next, E, ##7, ##23, ##2, figure, several, hundred, years, from, now, [SEP], and, if, we, could, be, there, then, we, would, tell, the, orthodox, ##y, created, around, [MASK], legend, that, they, are, insane, fool, ##s, ., ., ., ., ., ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], i, believe, there, is, something, more, than, me, or, us, but, what, that, something,\n",
            "\t\tconstitutes, is, pure, conjecture, ., [SEP], the, concept, of, god, made, in, man, ', s, image, is,\n",
            "\t\ta, conspiracy, plain, and, simple, ., [SEP], a, time, -, honored, strategy, to, control, the,\n",
            "\t\tmasses, ., [SEP], E, ##41, ##53, k, ##ores, ##h, may, very, well, be, the, next, E, ##7, ##23, ##2,\n",
            "\t\tfigure, several, hundred, years, from, now, [SEP], and, if, we, could, be, there, then, we, would,\n",
            "\t\ttell, the, orthodox, ##y, created, around, [MASK], legend, that, they, are, insane, fool, ##s, ., .,\n",
            "\t\t., ., ., ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 88. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and we would promptly be burned at the stake . [SEP] now as for the psalms and other enlightening anecdotes i dig them baby wholeheartedly so long as everybody 's embraced it completely . [SEP] if they have n't and only some have then the some are lambs to the slaughter . [SEP] hit me with your best shot . [SEP] fire away . [SEP] i 'm waiting for an answer on E5162 . [SEP] is [MASK] in heaven ? [SEP] [[CLS], and, we, would, promptly, be, burned, at, the, stake, ., [SEP], now, as, for, the, ps, ##al, ##ms, and, other, en, ##light, ##ening, an, ##ec, ##dote, ##s, i, dig, them, baby, whole, ##heart, ##edly, so, long, as, everybody, ', s, embraced, it, completely, ., [SEP], if, they, have, n, ', t, and, only, some, have, then, the, some, are, la, ##mbs, to, the, slaughter, ., [SEP], hit, me, with, your, best, shot, ., [SEP], fire, away, ., [SEP], i, ', m, waiting, for, an, answer, on, E, ##51, ##6, ##2, ., [SEP], is, [MASK], in, heaven, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 99 with text: \n",
            " \t\t[[CLS], and, we, would, promptly, be, burned, at, the, stake, ., [SEP], now, as, for, the, ps, ##al,\n",
            "\t\t##ms, and, other, en, ##light, ##ening, an, ##ec, ##dote, ##s, i, dig, them, baby, whole, ##heart,\n",
            "\t\t##edly, so, long, as, everybody, ', s, embraced, it, completely, ., [SEP], if, they, have, n, ', t,\n",
            "\t\tand, only, some, have, then, the, some, are, la, ##mbs, to, the, slaughter, ., [SEP], hit, me, with,\n",
            "\t\tyour, best, shot, ., [SEP], fire, away, ., [SEP], i, ', m, waiting, for, an, answer, on, E, ##51,\n",
            "\t\t##6, ##2, ., [SEP], is, [MASK], in, heaven, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 94. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1094it [00:08, 129.45it/s]\n",
            "2021-10-08 13:53:03,730 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - min_count = None\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-10-08 13:53:03,731 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 10324it [00:00, 121036.10it/s]\n",
            "2021-10-08 13:53:03,817 - INFO - allennlp.common.params - model.type = masked_language_model_ALS\n",
            "2021-10-08 13:53:03,817 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-10-08 13:53:03,818 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-10-08 13:53:03,819 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-10-08 13:53:03,820 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-10-08 13:53:03,821 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer\n",
            "2021-10-08 13:53:03,821 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer\n",
            "2021-10-08 13:53:03,821 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = bert-base-cased\n",
            "2021-10-08 13:53:03,821 - INFO - allennlp.common.params - type = bert-base-cased\n",
            "2021-10-08 13:53:03,821 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.eval_mode = False\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-10-08 13:53:03,822 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-10-08 13:53:06,561 - INFO - allennlp.common.params - model.language_model_head.type = bert\n",
            "2021-10-08 13:53:06,562 - INFO - allennlp.common.params - model.language_model_head.type = bert\n",
            "2021-10-08 13:53:06,562 - INFO - allennlp.common.params - model.language_model_head.model_name = bert-base-cased\n",
            "2021-10-08 13:53:06,562 - INFO - allennlp.common.params - type = bert-base-cased\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2021-10-08 13:53:10,589 - INFO - allennlp.common.params - model.contextualizer = None\n",
            "2021-10-08 13:53:10,590 - INFO - allennlp.common.params - model.target_namespace = bert\n",
            "2021-10-08 13:53:10,590 - INFO - allennlp.common.params - model.dropout = 0.0\n",
            "2021-10-08 13:53:10,590 - INFO - allennlp.common.params - model.initializer = None\n",
            "2021-10-08 13:53:11,000 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2021-10-08 13:53:11,000 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2021-10-08 13:53:11,000 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2021-10-08 13:53:11,000 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.patience = None\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.validation_metric = +perplexity\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - type = +perplexity\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - type = +perplexity\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.num_epochs = 10\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.grad_norm = False\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2021-10-08 13:53:11,001 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2021-10-08 13:53:11,002 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f5930876090>\n",
            "2021-10-08 13:53:11,003 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2021-10-08 13:53:11,003 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2021-10-08 13:53:11,003 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2021-10-08 13:53:11,003 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2021-10-08 13:53:14,553 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.lr = 2e-05\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2021-10-08 13:53:14,554 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = True\n",
            "2021-10-08 13:53:14,555 - INFO - allennlp.training.optimizers - Number of trainable parameters: 131200324\n",
            "2021-10-08 13:53:14,555 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-10-08 13:53:14,556 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-10-08 13:53:14,557 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,558 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-10-08 13:53:14,559 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,560 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,567 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,568 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-10-08 13:53:14,569 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-10-08 13:53:14,570 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-10-08 13:53:14,571 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.pooler.dense.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.bias\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.transform.dense.weight\n",
            "2021-10-08 13:53:14,572 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.transform.dense.bias\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.transform.LayerNorm.weight\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.transform.LayerNorm.bias\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.util - _language_model_head.bert_lm_head.predictions.decoder.weight\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.06\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2021-10-08 13:53:14,573 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - type = default\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - save_completed_epochs = True\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - save_every_num_seconds = None\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - save_every_num_batches = None\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - keep_most_recent_by_count = 2\n",
            "2021-10-08 13:53:14,574 - INFO - allennlp.common.params - keep_most_recent_by_age = None\n",
            "2021-10-08 13:53:14,578 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2021-10-08 13:53:14,578 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/9\n",
            "2021-10-08 13:53:14,578 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 6.6G\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  FutureWarning)\n",
            "2021-10-08 13:53:14,578 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 501M\n",
            "2021-10-08 13:53:14,579 - INFO - allennlp.training.gradient_descent_trainer - Training\n",
            "  0%|          | 0/289 [00:00<?, ?it/s]2021-10-08 13:53:14,585 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one\n",
            "2021-10-08 13:53:14,585 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys\n",
            "{'transformer': {'token_ids': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64), 'mask': tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]], device='cuda:0'), 'type_ids': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64), 'segment_concat_mask': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64)}}\n",
            "tensor([[[44]],\n",
            "\n",
            "        [[31]],\n",
            "\n",
            "        [[27]],\n",
            "\n",
            "        [[45]],\n",
            "\n",
            "        [[58]],\n",
            "\n",
            "        [[55]],\n",
            "\n",
            "        [[59]],\n",
            "\n",
            "        [[36]],\n",
            "\n",
            "        [[51]],\n",
            "\n",
            "        [[59]],\n",
            "\n",
            "        [[61]],\n",
            "\n",
            "        [[61]],\n",
            "\n",
            "        [[62]],\n",
            "\n",
            "        [[35]],\n",
            "\n",
            "        [[58]],\n",
            "\n",
            "        [[43]],\n",
            "\n",
            "        [[58]],\n",
            "\n",
            "        [[40]],\n",
            "\n",
            "        [[51]],\n",
            "\n",
            "        [[31]],\n",
            "\n",
            "        [[52]],\n",
            "\n",
            "        [[ 3]],\n",
            "\n",
            "        [[52]],\n",
            "\n",
            "        [[61]],\n",
            "\n",
            "        [[51]],\n",
            "\n",
            "        [[61]],\n",
            "\n",
            "        [[57]],\n",
            "\n",
            "        [[45]],\n",
            "\n",
            "        [[48]],\n",
            "\n",
            "        [[60]],\n",
            "\n",
            "        [[47]],\n",
            "\n",
            "        [[47]]], device='cuda:0')\n",
            "{'token_ids': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64), 'mask': tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]], device='cuda:0'), 'type_ids': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64), 'segment_concat_mask': tensor([], device='cuda:0', size=(32, 0), dtype=torch.int64)}\n",
            "  0%|          | 0/289 [00:00<?, ?it/s]\n",
            "2021-10-08 13:53:14,623 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/__main__.py\", line 46, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/__init__.py\", line 122, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 121, in train_model_from_args\n",
            "    file_friendly_logging=args.file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 187, in train_model_from_file\n",
            "    return_model=return_model,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 260, in train_model\n",
            "    file_friendly_logging=file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 504, in _train_worker\n",
            "    metrics = train_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 577, in run\n",
            "    return self.trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 750, in train\n",
            "    metrics, epoch = self._try_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 773, in _try_train\n",
            "    train_metrics = self._train_epoch(epoch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 490, in _train_epoch\n",
            "    batch_outputs = self.batch_outputs(batch, for_training=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 383, in batch_outputs\n",
            "    output_dict = self._pytorch_model(**batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1071, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/content/masked_language_model_ALS.py\", line 119, in forward\n",
            "    if targets is not None and targets.size() != mask_positions.size():\n",
            "AttributeError: 'dict' object has no attribute 'size'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX3UAav0meqN",
        "outputId": "a01ad2c3-b67e-4824-bd25-af11fb6bcc15"
      },
      "source": [
        "# training mlm with adversary \n",
        "\n",
        "!allennlp train -f adversarial_mlm_new.jsonnet.txt --include-package masked_language_modelling_ALS -s /content/AdversarialBiasMitigator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\t\tnot, done, that, we, would, now, be, like, so, ##dom, and, we, would, be, like, go, ##mor, ##rah, .,\n",
            "\t\t', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 92. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they failed because they tried to make themselves right by the things they did . [SEP] they did not trust in god to make them right . [SEP] they fell over the stone that makes people fall . [SEP] the scriptures talk about that stone : `` look i put in zion a stone that will make people stumble . [SEP] it is a rock that will make people fall . [SEP] but anyone who trusts in [MASK] will never be disappointed . '' [SEP] [[CLS], they, failed, because, they, tried, to, make, themselves, right, by, the, things, they, did, ., [SEP], they, did, not, trust, in, god, to, make, them, right, ., [SEP], they, fell, over, the, stone, that, makes, people, fall, ., [SEP], the, script, ##ures, talk, about, that, stone, :, `, `, look, i, put, in, z, ##ion, a, stone, that, will, make, people, stumble, ., [SEP], it, is, a, rock, that, will, make, people, fall, ., [SEP], but, anyone, who, trusts, in, [MASK], will, never, be, disappointed, ., ', ', [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 90 with text: \n",
            " \t\t[[CLS], they, failed, because, they, tried, to, make, themselves, right, by, the, things, they, did,\n",
            "\t\t., [SEP], they, did, not, trust, in, god, to, make, them, right, ., [SEP], they, fell, over, the,\n",
            "\t\tstone, that, makes, people, fall, ., [SEP], the, script, ##ures, talk, about, that, stone, :, `, `,\n",
            "\t\tlook, i, put, in, z, ##ion, a, stone, that, will, make, people, stumble, ., [SEP], it, is, a, rock,\n",
            "\t\tthat, will, make, people, fall, ., [SEP], but, anyone, who, trusts, in, [MASK], will, never, be,\n",
            "\t\tdisappointed, ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 81. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you are proof that i am an apostle in the lord . [SEP] some people want to judge me . [SEP] so this is the answer i give them : we have the right to eat and drink do n't we ? [SEP] we have the right to bring a believing wife with us when we travel do n't we ? [SEP] the other apostles and the lord 's brothers and E3520 all do this . [SEP] and are E5120 and i the only ones who must work to earn our living ? [SEP] no soldier ever serves in the army and pays [MASK] own salary . [SEP] [[CLS], you, are, proof, that, i, am, an, a, ##postle, in, the, lord, ., [SEP], some, people, want, to, judge, me, ., [SEP], so, this, is, the, answer, i, give, them, :, we, have, the, right, to, eat, and, drink, do, n, ', t, we, ?, [SEP], we, have, the, right, to, bring, a, believing, wife, with, us, when, we, travel, do, n, ', t, we, ?, [SEP], the, other, a, ##postle, ##s, and, the, lord, ', s, brothers, and, E, ##35, ##20, all, do, this, ., [SEP], and, are, E, ##51, ##20, and, i, the, only, ones, who, must, work, to, earn, our, living, ?, [SEP], no, soldier, ever, serves, in, the, army, and, pays, [MASK], own, salary, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 121 with text: \n",
            " \t\t[[CLS], you, are, proof, that, i, am, an, a, ##postle, in, the, lord, ., [SEP], some, people, want,\n",
            "\t\tto, judge, me, ., [SEP], so, this, is, the, answer, i, give, them, :, we, have, the, right, to, eat,\n",
            "\t\tand, drink, do, n, ', t, we, ?, [SEP], we, have, the, right, to, bring, a, believing, wife, with,\n",
            "\t\tus, when, we, travel, do, n, ', t, we, ?, [SEP], the, other, a, ##postle, ##s, and, the, lord, ', s,\n",
            "\t\tbrothers, and, E, ##35, ##20, all, do, this, ., [SEP], and, are, E, ##51, ##20, and, i, the, only,\n",
            "\t\tones, who, must, work, to, earn, our, living, ?, [SEP], no, soldier, ever, serves, in, the, army,\n",
            "\t\tand, pays, [MASK], own, salary, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 116. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no one ever plants a vineyard without eating some of the grapes himself . [SEP] no one takes care of a flock of sheep without drinking some of the milk himself . [SEP] these are n't just my own thoughts . [SEP] god 's law says the same thing . [SEP] yes it is written in the law of E5222 : `` [SEP] when a work animal is being used to separate grain do n't keep it from eating the grain . '' [SEP] when god said this was [MASK] thinking only about work animals ? [SEP] [[CLS], no, one, ever, plants, a, vine, ##yard, without, eating, some, of, the, grapes, himself, ., [SEP], no, one, takes, care, of, a, flock, of, sheep, without, drinking, some, of, the, milk, himself, ., [SEP], these, are, n, ', t, just, my, own, thoughts, ., [SEP], god, ', s, law, says, the, same, thing, ., [SEP], yes, it, is, written, in, the, law, of, E, ##5, ##22, ##2, :, `, `, [SEP], when, a, work, animal, is, being, used, to, separate, grain, do, n, ', t, keep, it, from, eating, the, grain, ., ', ', [SEP], when, god, said, this, was, [MASK], thinking, only, about, work, animals, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 109 with text: \n",
            " \t\t[[CLS], no, one, ever, plants, a, vine, ##yard, without, eating, some, of, the, grapes, himself, .,\n",
            "\t\t[SEP], no, one, takes, care, of, a, flock, of, sheep, without, drinking, some, of, the, milk,\n",
            "\t\thimself, ., [SEP], these, are, n, ', t, just, my, own, thoughts, ., [SEP], god, ', s, law, says,\n",
            "\t\tthe, same, thing, ., [SEP], yes, it, is, written, in, the, law, of, E, ##5, ##22, ##2, :, `, `,\n",
            "\t\t[SEP], when, a, work, animal, is, being, used, to, separate, grain, do, n, ', t, keep, it, from,\n",
            "\t\teating, the, grain, ., ', ', [SEP], when, god, said, this, was, [MASK], thinking, only, about, work,\n",
            "\t\tanimals, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 101. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no . [SEP] [MASK] was really talking about us . [SEP] [[CLS], no, ., [SEP], [MASK], was, really, talking, about, us, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], no, ., [SEP], [MASK], was, really, talking, about, us, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] god is the one who gives seed to those who plant and [MASK] gives bread for food . [SEP] [[CLS], god, is, the, one, who, gives, seed, to, those, who, plant, and, [MASK], gives, bread, for, food, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], god, is, the, one, who, gives, seed, to, those, who, plant, and, [MASK], gives, bread, for,\n",
            "\t\tfood, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and god will give you spiritual seed and make that seed grow . [SEP] [MASK] will produce a great harvest from your goodness . [SEP] [[CLS], and, god, will, give, you, spiritual, seed, and, make, that, seed, grow, ., [SEP], [MASK], will, produce, a, great, harvest, from, your, goodness, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], and, god, will, give, you, spiritual, seed, and, make, that, seed, grow, ., [SEP], [MASK],\n",
            "\t\twill, produce, a, great, harvest, from, your, goodness, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and your giving through us will make people give thanks to god . [SEP] the service you are offering helps god 's people with their needs but that is not all it does . [SEP] it is also bringing more and more thanks to god . [SEP] this service is a proof of your faith and people will praise god because of it . [SEP] they will praise [MASK] because you are following the message about E2839 -- the message you say you believe . [SEP] [[CLS], and, your, giving, through, us, will, make, people, give, thanks, to, god, ., [SEP], the, service, you, are, offering, helps, god, ', s, people, with, their, needs, but, that, is, not, all, it, does, ., [SEP], it, is, also, bringing, more, and, more, thanks, to, god, ., [SEP], this, service, is, a, proof, of, your, faith, and, people, will, praise, god, because, of, it, ., [SEP], they, will, praise, [MASK], because, you, are, following, the, message, about, E, ##28, ##39, -, -, the, message, you, say, you, believe, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 91 with text: \n",
            " \t\t[[CLS], and, your, giving, through, us, will, make, people, give, thanks, to, god, ., [SEP], the,\n",
            "\t\tservice, you, are, offering, helps, god, ', s, people, with, their, needs, but, that, is, not, all,\n",
            "\t\tit, does, ., [SEP], it, is, also, bringing, more, and, more, thanks, to, god, ., [SEP], this,\n",
            "\t\tservice, is, a, proof, of, your, faith, and, people, will, praise, god, because, of, it, ., [SEP],\n",
            "\t\tthey, will, praise, [MASK], because, you, are, following, the, message, about, E, ##28, ##39, -, -,\n",
            "\t\tthe, message, you, say, you, believe, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 70. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 747it [00:05, 189.13it/s]\u001b[A[CLS] they will praise god because you freely share with them and with all people . [SEP] and when they pray they will wish they could be with you . [SEP] they will feel this way because of the great grace that god gave you . [SEP] thanks be to god for [MASK] gift that is too wonderful to describe . [SEP] [[CLS], they, will, praise, god, because, you, freely, share, with, them, and, with, all, people, ., [SEP], and, when, they, pray, they, will, wish, they, could, be, with, you, ., [SEP], they, will, feel, this, way, because, of, the, great, grace, that, god, gave, you, ., [SEP], thanks, be, to, god, for, [MASK], gift, that, is, too, wonderful, to, describe, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], they, will, praise, god, because, you, freely, share, with, them, and, with, all, people, .,\n",
            "\t\t[SEP], and, when, they, pray, they, will, wish, they, could, be, with, you, ., [SEP], they, will,\n",
            "\t\tfeel, this, way, because, of, the, great, grace, that, god, gave, you, ., [SEP], thanks, be, to,\n",
            "\t\tgod, for, [MASK], gift, that, is, too, wonderful, to, describe, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] above the box were the cherub angels that showed god 's glory . [SEP] these cherub angels were over the place of mercy . [SEP] but we can not say everything about this now . [SEP] everything in the tent was made ready in the way i have explained . [SEP] then the priests went into the first room every day to do their worship duties . [SEP] but only the high priest could go into the second room and [MASK] went in only once a year . [SEP] [[CLS], above, the, box, were, the, ch, ##er, ##ub, angels, that, showed, god, ', s, glory, ., [SEP], these, ch, ##er, ##ub, angels, were, over, the, place, of, mercy, ., [SEP], but, we, can, not, say, everything, about, this, now, ., [SEP], everything, in, the, tent, was, made, ready, in, the, way, i, have, explained, ., [SEP], then, the, priests, went, into, the, first, room, every, day, to, do, their, worship, duties, ., [SEP], but, only, the, high, priest, could, go, into, the, second, room, and, [MASK], went, in, only, once, a, year, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 95 with text: \n",
            " \t\t[[CLS], above, the, box, were, the, ch, ##er, ##ub, angels, that, showed, god, ', s, glory, .,\n",
            "\t\t[SEP], these, ch, ##er, ##ub, angels, were, over, the, place, of, mercy, ., [SEP], but, we, can,\n",
            "\t\tnot, say, everything, about, this, now, ., [SEP], everything, in, the, tent, was, made, ready, in,\n",
            "\t\tthe, way, i, have, explained, ., [SEP], then, the, priests, went, into, the, first, room, every,\n",
            "\t\tday, to, do, their, worship, duties, ., [SEP], but, only, the, high, priest, could, go, into, the,\n",
            "\t\tsecond, room, and, [MASK], went, in, only, once, a, year, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 86. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] offered that blood to god for himself and for the sins the people committed without knowing they were sinning . [SEP] [[CLS], [MASK], offered, that, blood, to, god, for, himself, and, for, the, sins, the, people, committed, without, knowing, they, were, sin, ##ning, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], [MASK], offered, that, blood, to, god, for, himself, and, for, the, sins, the, people,\n",
            "\t\tcommitted, without, knowing, they, were, sin, ##ning, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but E2839 has already come to be the high priest . [SEP] [MASK] is the high priest of the good things we now have . [SEP] [[CLS], but, E, ##28, ##39, has, already, come, to, be, the, high, priest, ., [SEP], [MASK], is, the, high, priest, of, the, good, things, we, now, have, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], but, E, ##28, ##39, has, already, come, to, be, the, high, priest, ., [SEP], [MASK], is,\n",
            "\t\tthe, high, priest, of, the, good, things, we, now, have, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but E2839 does not serve in a place like the tent that those other priests served in . [SEP] [MASK] serves in a better place . [SEP] [[CLS], but, E, ##28, ##39, does, not, serve, in, a, place, like, the, tent, that, those, other, priests, served, in, ., [SEP], [MASK], serves, in, a, better, place, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 30 with text: \n",
            " \t\t[[CLS], but, E, ##28, ##39, does, not, serve, in, a, place, like, the, tent, that, those, other,\n",
            "\t\tpriests, served, in, ., [SEP], [MASK], serves, in, a, better, place, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 22. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] entered there and made us free from sin forever . [SEP] [[CLS], [MASK], entered, there, and, made, us, free, from, sin, forever, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], [MASK], entered, there, and, made, us, free, from, sin, forever, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the blood of goats and bulls and the ashes of a cow were sprinkled on those who were no longer pure enough to enter the place of worship . [SEP] the blood and ashes made them pure again -- but only their bodies . [SEP] so surely the blood sacrifice of E2839 can do much more . [SEP] E2839 offered himself through the eternal spirit as a perfect sacrifice to god . [SEP] [MASK] blood will make us completely clean from the evil we have done . [SEP] [[CLS], the, blood, of, goats, and, bull, ##s, and, the, ashes, of, a, cow, were, s, ##p, ##rinkled, on, those, who, were, no, longer, pure, enough, to, enter, the, place, of, worship, ., [SEP], the, blood, and, ashes, made, them, pure, again, -, -, but, only, their, bodies, ., [SEP], so, surely, the, blood, sacrifice, of, E, ##28, ##39, can, do, much, more, ., [SEP], E, ##28, ##39, offered, himself, through, the, eternal, spirit, as, a, perfect, sacrifice, to, god, ., [SEP], [MASK], blood, will, make, us, completely, clean, from, the, evil, we, have, done, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 97 with text: \n",
            " \t\t[[CLS], the, blood, of, goats, and, bull, ##s, and, the, ashes, of, a, cow, were, s, ##p, ##rinkled,\n",
            "\t\ton, those, who, were, no, longer, pure, enough, to, enter, the, place, of, worship, ., [SEP], the,\n",
            "\t\tblood, and, ashes, made, them, pure, again, -, -, but, only, their, bodies, ., [SEP], so, surely,\n",
            "\t\tthe, blood, sacrifice, of, E, ##28, ##39, can, do, much, more, ., [SEP], E, ##28, ##39, offered,\n",
            "\t\thimself, through, the, eternal, spirit, as, a, perfect, sacrifice, to, god, ., [SEP], [MASK], blood,\n",
            "\t\twill, make, us, completely, clean, from, the, evil, we, have, done, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 82. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it will give us clear consciences so that we can worship the living god . [SEP] so E2839 brings a new agreement from god to [MASK] people . [SEP] [[CLS], it, will, give, us, clear, conscience, ##s, so, that, we, can, worship, the, living, god, ., [SEP], so, E, ##28, ##39, brings, a, new, agreement, from, god, to, [MASK], people, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], it, will, give, us, clear, conscience, ##s, so, that, we, can, worship, the, living, god, .,\n",
            "\t\t[SEP], so, E, ##28, ##39, brings, a, new, agreement, from, god, to, [MASK], people, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 29. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] brings this agreement so that those who are chosen by god can have the blessings god promised blessings that last forever . [SEP] [[CLS], [MASK], brings, this, agreement, so, that, those, who, are, chosen, by, god, can, have, the, blessing, ##s, god, promised, blessing, ##s, that, last, forever, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 27 with text: \n",
            " \t\t[[CLS], [MASK], brings, this, agreement, so, that, those, who, are, chosen, by, god, can, have, the,\n",
            "\t\tblessing, ##s, god, promised, blessing, ##s, that, last, forever, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] this can happen only because E2839 died to free people from sins committed against the commands of the first agreement . [SEP] when someone dies and leaves a will there must be proof that the one who wrote the will is dead . [SEP] a will means nothing while the one who wrote it is still living . [SEP] it can be used only after that person 's death . [SEP] that is why blood was needed to begin the first agreement between god and [MASK] people . [SEP] [[CLS], this, can, happen, only, because, E, ##28, ##39, died, to, free, people, from, sins, committed, against, the, commands, of, the, first, agreement, ., [SEP], when, someone, dies, and, leaves, a, will, there, must, be, proof, that, the, one, who, wrote, the, will, is, dead, ., [SEP], a, will, means, nothing, while, the, one, who, wrote, it, is, still, living, ., [SEP], it, can, be, used, only, after, that, person, ', s, death, ., [SEP], that, is, why, blood, was, needed, to, begin, the, first, agreement, between, god, and, [MASK], people, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 93 with text: \n",
            " \t\t[[CLS], this, can, happen, only, because, E, ##28, ##39, died, to, free, people, from, sins,\n",
            "\t\tcommitted, against, the, commands, of, the, first, agreement, ., [SEP], when, someone, dies, and,\n",
            "\t\tleaves, a, will, there, must, be, proof, that, the, one, who, wrote, the, will, is, dead, ., [SEP],\n",
            "\t\ta, will, means, nothing, while, the, one, who, wrote, it, is, still, living, ., [SEP], it, can, be,\n",
            "\t\tused, only, after, that, person, ', s, death, ., [SEP], that, is, why, blood, was, needed, to,\n",
            "\t\tbegin, the, first, agreement, between, god, and, [MASK], people, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 89. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] first E5222 told the people every command in the law . [SEP] then [MASK] took the blood of young bulls and mixed it with water . [SEP] [[CLS], first, E, ##5, ##22, ##2, told, the, people, every, command, in, the, law, ., [SEP], then, [MASK], took, the, blood, of, young, bull, ##s, and, mixed, it, with, water, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 32 with text: \n",
            " \t\t[[CLS], first, E, ##5, ##22, ##2, told, the, people, every, command, in, the, law, ., [SEP], then,\n",
            "\t\t[MASK], took, the, blood, of, young, bull, ##s, and, mixed, it, with, water, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 17. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] used red wool and a branch of hyssop to sprinkle the blood and water on the book of the law and on all the people . [SEP] [[CLS], [MASK], used, red, wool, and, a, branch, of, h, ##ys, ##so, ##p, to, s, ##p, ##rin, ##kle, the, blood, and, water, on, the, book, of, the, law, and, on, all, the, people, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 35 with text: \n",
            " \t\t[[CLS], [MASK], used, red, wool, and, a, branch, of, h, ##ys, ##so, ##p, to, s, ##p, ##rin, ##kle,\n",
            "\t\tthe, blood, and, water, on, the, book, of, the, law, and, on, all, the, people, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then [MASK] said `` this is the blood that makes the agreement good -- the agreement that god commanded you to follow . '' [SEP] [[CLS], then, [MASK], said, `, `, this, is, the, blood, that, makes, the, agreement, good, -, -, the, agreement, that, god, commanded, you, to, follow, ., ', ', [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], then, [MASK], said, `, `, this, is, the, blood, that, makes, the, agreement, good, -, -,\n",
            "\t\tthe, agreement, that, god, commanded, you, to, follow, ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in the same way E5222 sprinkled the blood on the holy tent . [SEP] [MASK] sprinkled the blood over everything used in worship . [SEP] [[CLS], in, the, same, way, E, ##5, ##22, ##2, s, ##p, ##rinkled, the, blood, on, the, holy, tent, ., [SEP], [MASK], s, ##p, ##rinkled, the, blood, over, everything, used, in, worship, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], in, the, same, way, E, ##5, ##22, ##2, s, ##p, ##rinkled, the, blood, on, the, holy, tent,\n",
            "\t\t., [SEP], [MASK], s, ##p, ##rinkled, the, blood, over, everything, used, in, worship, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 20. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E2839 went into heaven but not to offer himself many times like the high priest offers blood again and again . [SEP] if E2839 had offered himself many times [MASK] would have needed to suffer many times since the time the world was made . [SEP] [[CLS], E, ##28, ##39, went, into, heaven, but, not, to, offer, himself, many, times, like, the, high, priest, offers, blood, again, and, again, ., [SEP], if, E, ##28, ##39, had, offered, himself, many, times, [MASK], would, have, needed, to, suffer, many, times, since, the, time, the, world, was, made, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 51 with text: \n",
            " \t\t[[CLS], E, ##28, ##39, went, into, heaven, but, not, to, offer, himself, many, times, like, the,\n",
            "\t\thigh, priest, offers, blood, again, and, again, ., [SEP], if, E, ##28, ##39, had, offered, himself,\n",
            "\t\tmany, times, [MASK], would, have, needed, to, suffer, many, times, since, the, time, the, world,\n",
            "\t\twas, made, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] came to offer himself only once . [SEP] [[CLS], but, [MASK], came, to, offer, himself, only, once, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], but, [MASK], came, to, offer, himself, only, once, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and that once is enough for all time . [SEP] [MASK] came at a time when the world is nearing an end . [SEP] [[CLS], and, that, once, is, enough, for, all, time, ., [SEP], [MASK], came, at, a, time, when, the, world, is, near, ##ing, an, end, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], and, that, once, is, enough, for, all, time, ., [SEP], [MASK], came, at, a, time, when, the,\n",
            "\t\tworld, is, near, ##ing, an, end, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] came to take away all sin by offering himself as a sacrifice . [SEP] [[CLS], [MASK], came, to, take, away, all, sin, by, offering, himself, as, a, sacrifice, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], [MASK], came, to, take, away, all, sin, by, offering, himself, as, a, sacrifice, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] everyone must die once . [SEP] then they are judged . [SEP] so E2839 was offered as a sacrifice one time to take away the sins of many people . [SEP] and [MASK] will come a second time but not to offer himself for sin . [SEP] [[CLS], everyone, must, die, once, ., [SEP], then, they, are, judged, ., [SEP], so, E, ##28, ##39, was, offered, as, a, sacrifice, one, time, to, take, away, the, sins, of, many, people, ., [SEP], and, [MASK], will, come, a, second, time, but, not, to, offer, himself, for, sin, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], everyone, must, die, once, ., [SEP], then, they, are, judged, ., [SEP], so, E, ##28, ##39,\n",
            "\t\twas, offered, as, a, sacrifice, one, time, to, take, away, the, sins, of, many, people, ., [SEP],\n",
            "\t\tand, [MASK], will, come, a, second, time, but, not, to, offer, himself, for, sin, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 35. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 766it [00:05, 187.42it/s]\u001b[A[CLS] the fifth angel blew [MASK] trumpet . [SEP] [[CLS], the, fifth, angel, blew, [MASK], trumpet, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], the, fifth, angel, blew, [MASK], trumpet, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] their hair was like women 's hair . [SEP] their teeth were like lions ' teeth . [SEP] their chests looked like iron breastplates . [SEP] the sound their wings made was like the noise of many horses and chariots hurrying into battle . [SEP] the locusts had tails with stingers like scorpions . [SEP] the power they had to give people pain for five months was in their tails . [SEP] they had a ruler who was the angel of the bottomless pit . [SEP] [MASK] name in hebrew is abaddon . [SEP] [[CLS], their, hair, was, like, women, ', s, hair, ., [SEP], their, teeth, were, like, lions, ', teeth, ., [SEP], their, chest, ##s, looked, like, iron, breast, ##plate, ##s, ., [SEP], the, sound, their, wings, made, was, like, the, noise, of, many, horses, and, ch, ##ario, ##ts, hurry, ##ing, into, battle, ., [SEP], the, lo, ##cus, ##ts, had, tails, with, sting, ##ers, like, s, ##cor, ##pion, ##s, ., [SEP], the, power, they, had, to, give, people, pain, for, five, months, was, in, their, tails, ., [SEP], they, had, a, ruler, who, was, the, angel, of, the, bottom, ##less, pit, ., [SEP], [MASK], name, in, he, ##bre, ##w, is, a, ##bad, ##don, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 113 with text: \n",
            " \t\t[[CLS], their, hair, was, like, women, ', s, hair, ., [SEP], their, teeth, were, like, lions, ',\n",
            "\t\tteeth, ., [SEP], their, chest, ##s, looked, like, iron, breast, ##plate, ##s, ., [SEP], the, sound,\n",
            "\t\ttheir, wings, made, was, like, the, noise, of, many, horses, and, ch, ##ario, ##ts, hurry, ##ing,\n",
            "\t\tinto, battle, ., [SEP], the, lo, ##cus, ##ts, had, tails, with, sting, ##ers, like, s, ##cor,\n",
            "\t\t##pion, ##s, ., [SEP], the, power, they, had, to, give, people, pain, for, five, months, was, in,\n",
            "\t\ttheir, tails, ., [SEP], they, had, a, ruler, who, was, the, angel, of, the, bottom, ##less, pit, .,\n",
            "\t\t[SEP], [MASK], name, in, he, ##bre, ##w, is, a, ##bad, ##don, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 101. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in greek it is apollyon . [SEP] the first terror is now past . [SEP] there are still two other terrors to come . [SEP] the sixth angel blew [MASK] trumpet . [SEP] [[CLS], in, g, ##ree, ##k, it, is, a, ##pol, ##ly, ##on, ., [SEP], the, first, terror, is, now, past, ., [SEP], there, are, still, two, other, terror, ##s, to, come, ., [SEP], the, sixth, angel, blew, [MASK], trumpet, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 40 with text: \n",
            " \t\t[[CLS], in, g, ##ree, ##k, it, is, a, ##pol, ##ly, ##on, ., [SEP], the, first, terror, is, now,\n",
            "\t\tpast, ., [SEP], there, are, still, two, other, terror, ##s, to, come, ., [SEP], the, sixth, angel,\n",
            "\t\tblew, [MASK], trumpet, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they did not change their hearts and turn away from killing other people or from their evil magic their sexual sins and their stealing . [SEP] after this i heard what sounded like a large crowd of people in heaven . [SEP] the people were saying `` hallelujah ! [SEP] victory glory and power belong to our god . [SEP] [MASK] judgments are true and right . [SEP] [[CLS], they, did, not, change, their, hearts, and, turn, away, from, killing, other, people, or, from, their, evil, magic, their, sexual, sins, and, their, stealing, ., [SEP], after, this, i, heard, what, sounded, like, a, large, crowd, of, people, in, heaven, ., [SEP], the, people, were, saying, `, `, hall, ##el, ##u, ##jah, !, [SEP], victory, glory, and, power, belong, to, our, god, ., [SEP], [MASK], judgment, ##s, are, true, and, right, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 74 with text: \n",
            " \t\t[[CLS], they, did, not, change, their, hearts, and, turn, away, from, killing, other, people, or,\n",
            "\t\tfrom, their, evil, magic, their, sexual, sins, and, their, stealing, ., [SEP], after, this, i,\n",
            "\t\theard, what, sounded, like, a, large, crowd, of, people, in, heaven, ., [SEP], the, people, were,\n",
            "\t\tsaying, `, `, hall, ##el, ##u, ##jah, !, [SEP], victory, glory, and, power, belong, to, our, god, .,\n",
            "\t\t[SEP], [MASK], judgment, ##s, are, true, and, right, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 65. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then the 24 elders and the four living beings bowed down . [SEP] they worshiped god who sits on the throne . [SEP] they said `` amen ! [SEP] hallelujah ! '' [SEP] then a voice came from the throne and said `` praise our god all you who serve [MASK] ! [SEP] [[CLS], then, the, 24, elders, and, the, four, living, beings, bowed, down, ., [SEP], they, worship, ##ed, god, who, sits, on, the, throne, ., [SEP], they, said, `, `, am, ##en, !, [SEP], hall, ##el, ##u, ##jah, !, ', ', [SEP], then, a, voice, came, from, the, throne, and, said, `, `, praise, our, god, all, you, who, serve, [MASK], !, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], then, the, 24, elders, and, the, four, living, beings, bowed, down, ., [SEP], they, worship,\n",
            "\t\t##ed, god, who, sits, on, the, throne, ., [SEP], they, said, `, `, am, ##en, !, [SEP], hall, ##el,\n",
            "\t\t##u, ##jah, !, ', ', [SEP], then, a, voice, came, from, the, throne, and, said, `, `, praise, our,\n",
            "\t\tgod, all, you, who, serve, [MASK], !, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 59. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] praise our god all you small and great who honor [MASK] ! '' [SEP] [[CLS], praise, our, god, all, you, small, and, great, who, honor, [MASK], !, ', ', [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], praise, our, god, all, you, small, and, great, who, honor, [MASK], !, ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then i heard something that sounded like a large crowd of people . [SEP] it was as loud as crashing waves or claps of thunder . [SEP] the people were saying `` hallelujah ! [SEP] our lord god rules . [SEP] [MASK] is the all - powerful . [SEP] [[CLS], then, i, heard, something, that, sounded, like, a, large, crowd, of, people, ., [SEP], it, was, as, loud, as, crashing, waves, or, c, ##lap, ##s, of, thunder, ., [SEP], the, people, were, saying, `, `, hall, ##el, ##u, ##jah, !, [SEP], our, lord, god, rules, ., [SEP], [MASK], is, the, all, -, powerful, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 56 with text: \n",
            " \t\t[[CLS], then, i, heard, something, that, sounded, like, a, large, crowd, of, people, ., [SEP], it,\n",
            "\t\twas, as, loud, as, crashing, waves, or, c, ##lap, ##s, of, thunder, ., [SEP], the, people, were,\n",
            "\t\tsaying, `, `, hall, ##el, ##u, ##jah, !, [SEP], our, lord, god, rules, ., [SEP], [MASK], is, the,\n",
            "\t\tall, -, powerful, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 48. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] let us rejoice and be happy and give god glory ! [SEP] give god glory because the wedding of the E1600 has come . [SEP] and the E1600 's bride has made herself ready . [SEP] fine linen was given to the bride for [MASK] to wear . [SEP] [[CLS], let, us, re, ##jo, ##ice, and, be, happy, and, give, god, glory, !, [SEP], give, god, glory, because, the, wedding, of, the, E, ##16, ##00, has, come, ., [SEP], and, the, E, ##16, ##00, ', s, bride, has, made, herself, ready, ., [SEP], fine, linen, was, given, to, the, bride, for, [MASK], to, wear, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 57 with text: \n",
            " \t\t[[CLS], let, us, re, ##jo, ##ice, and, be, happy, and, give, god, glory, !, [SEP], give, god, glory,\n",
            "\t\tbecause, the, wedding, of, the, E, ##16, ##00, has, come, ., [SEP], and, the, E, ##16, ##00, ', s,\n",
            "\t\tbride, has, made, herself, ready, ., [SEP], fine, linen, was, given, to, the, bride, for, [MASK],\n",
            "\t\tto, wear, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the linen was bright and clean . '' [SEP] -lrb- the fine linen means the good things that god 's holy people did . -rrb- [SEP] then the angel said to me `` write this : great blessings belong to those who are invited to the wedding meal of the E1600 ! '' [SEP] then the angel said `` these are the true words of god . '' [SEP] then i bowed down before the angel 's feet to worship [MASK] . [SEP] [[CLS], the, linen, was, bright, and, clean, ., ', ', [SEP], -, l, ##rb, -, the, fine, linen, means, the, good, things, that, god, ', s, holy, people, did, ., -, r, ##rb, -, [SEP], then, the, angel, said, to, me, `, `, write, this, :, great, blessing, ##s, belong, to, those, who, are, invited, to, the, wedding, meal, of, the, E, ##16, ##00, !, ', ', [SEP], then, the, angel, said, `, `, these, are, the, true, words, of, god, ., ', ', [SEP], then, i, bowed, down, before, the, angel, ', s, feet, to, worship, [MASK], ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], the, linen, was, bright, and, clean, ., ', ', [SEP], -, l, ##rb, -, the, fine, linen, means,\n",
            "\t\tthe, good, things, that, god, ', s, holy, people, did, ., -, r, ##rb, -, [SEP], then, the, angel,\n",
            "\t\tsaid, to, me, `, `, write, this, :, great, blessing, ##s, belong, to, those, who, are, invited, to,\n",
            "\t\tthe, wedding, meal, of, the, E, ##16, ##00, !, ', ', [SEP], then, the, angel, said, `, `, these,\n",
            "\t\tare, the, true, words, of, god, ., ', ', [SEP], then, i, bowed, down, before, the, angel, ', s,\n",
            "\t\tfeet, to, worship, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 97. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] eyes were like burning fire . [SEP] [[CLS], [MASK], eyes, were, like, burning, fire, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], eyes, were, like, burning, fire, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the armies of heaven were following the rider on the white horse . [SEP] they were also riding white horses . [SEP] they were dressed in fine linen white and clean . [SEP] a sharp sword came out of the rider 's mouth a sword that [MASK] would use to defeat the nations . [SEP] [[CLS], the, armies, of, heaven, were, following, the, rider, on, the, white, horse, ., [SEP], they, were, also, riding, white, horses, ., [SEP], they, were, dressed, in, fine, linen, white, and, clean, ., [SEP], a, sharp, sword, came, out, of, the, rider, ', s, mouth, a, sword, that, [MASK], would, use, to, defeat, the, nations, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 57 with text: \n",
            " \t\t[[CLS], the, armies, of, heaven, were, following, the, rider, on, the, white, horse, ., [SEP], they,\n",
            "\t\twere, also, riding, white, horses, ., [SEP], they, were, dressed, in, fine, linen, white, and,\n",
            "\t\tclean, ., [SEP], a, sharp, sword, came, out, of, the, rider, ', s, mouth, a, sword, that, [MASK],\n",
            "\t\twould, use, to, defeat, the, nations, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 48. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] will rule the nations with a rod of iron . [SEP] [[CLS], and, [MASK], will, rule, the, nations, with, a, rod, of, iron, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], and, [MASK], will, rule, the, nations, with, a, rod, of, iron, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] will crush the grapes in the winepress of the terrible anger of god all - powerful . [SEP] [[CLS], [MASK], will, crush, the, grapes, in, the, wine, ##press, of, the, terrible, anger, of, god, all, -, powerful, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], [MASK], will, crush, the, grapes, in, the, wine, ##press, of, the, terrible, anger, of, god,\n",
            "\t\tall, -, powerful, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in a loud voice the angel said to all the birds flying in the sky `` come together for the great supper of god . [SEP] come together so that you can eat the bodies of rulers and army commanders and famous men . [SEP] come to eat the bodies of the horses and their riders and the bodies of all people -- free slave small and great . '' [SEP] then i saw the beast and the rulers of the earth . [SEP] their armies were gathered together to make war against the rider on the horse and [MASK] army . [SEP] [[CLS], in, a, loud, voice, the, angel, said, to, all, the, birds, flying, in, the, sky, `, `, come, together, for, the, great, supper, of, god, ., [SEP], come, together, so, that, you, can, eat, the, bodies, of, rulers, and, army, commanders, and, famous, men, ., [SEP], come, to, eat, the, bodies, of, the, horses, and, their, riders, and, the, bodies, of, all, people, -, -, free, slave, small, and, great, ., ', ', [SEP], then, i, saw, the, beast, and, the, rulers, of, the, earth, ., [SEP], their, armies, were, gathered, together, to, make, war, against, the, rider, on, the, horse, and, [MASK], army, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 107 with text: \n",
            " \t\t[[CLS], in, a, loud, voice, the, angel, said, to, all, the, birds, flying, in, the, sky, `, `, come,\n",
            "\t\ttogether, for, the, great, supper, of, god, ., [SEP], come, together, so, that, you, can, eat, the,\n",
            "\t\tbodies, of, rulers, and, army, commanders, and, famous, men, ., [SEP], come, to, eat, the, bodies,\n",
            "\t\tof, the, horses, and, their, riders, and, the, bodies, of, all, people, -, -, free, slave, small,\n",
            "\t\tand, great, ., ', ', [SEP], then, i, saw, the, beast, and, the, rulers, of, the, earth, ., [SEP],\n",
            "\t\ttheir, armies, were, gathered, together, to, make, war, against, the, rider, on, the, horse, and,\n",
            "\t\t[MASK], army, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 103. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but the beast was captured and the false prophet was also captured . [SEP] [MASK] was the one who did the miracles for the beast . [SEP] [[CLS], but, the, beast, was, captured, and, the, false, prophet, was, also, captured, ., [SEP], [MASK], was, the, one, who, did, the, miracles, for, the, beast, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], but, the, beast, was, captured, and, the, false, prophet, was, also, captured, ., [SEP],\n",
            "\t\t[MASK], was, the, one, who, did, the, miracles, for, the, beast, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] had used these miracles to trick those who had the mark of the beast and worshiped its idol . [SEP] [[CLS], [MASK], had, used, these, miracles, to, trick, those, who, had, the, mark, of, the, beast, and, worship, ##ed, its, idol, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 23 with text: \n",
            " \t\t[[CLS], [MASK], had, used, these, miracles, to, trick, those, who, had, the, mark, of, the, beast,\n",
            "\t\tand, worship, ##ed, its, idol, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] feldmessen is %um surveying . [SEP] %hm . [SEP] %uh so [MASK] 's been away for two weeks . [SEP] [[CLS], f, ##eld, ##mes, ##sen, is, %, um, surveying, ., [SEP], %, h, ##m, ., [SEP], %, uh, so, [MASK], ', s, been, away, for, two, weeks, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], f, ##eld, ##mes, ##sen, is, %, um, surveying, ., [SEP], %, h, ##m, ., [SEP], %, uh, so,\n",
            "\t\t[MASK], ', s, been, away, for, two, weeks, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's coming back tonight . [SEP] [[CLS], [MASK], ', s, coming, back, tonight, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], ', s, coming, back, tonight, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] ha . [SEP] so we 're looking forward to that . [SEP] oh i 'm sorry we 're going to miss [MASK] . [SEP] [[CLS], ha, ., [SEP], so, we, ', re, looking, forward, to, that, ., [SEP], oh, i, ', m, sorry, we, ', re, going, to, miss, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], ha, ., [SEP], so, we, ', re, looking, forward, to, that, ., [SEP], oh, i, ', m, sorry, we,\n",
            "\t\t', re, going, to, miss, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 25. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 785it [00:05, 186.15it/s]\u001b[A[CLS] yeah [SEP] oh gosh . i bet you 're getting lonely . [SEP] yeah [SEP] well . E7501 's been here [SEP] and we 've been doing okay . [SEP] %mm . [SEP] and i 'm actually waiting for E7501 to show up because we were supposed to go into town and meet some people in town . [SEP] i 've got a student here who 's checking out rooms and stuff . [SEP] %mm . [SEP] [MASK] 's coming in the fall [SEP] [[CLS], yeah, [SEP], oh, go, ##sh, ., i, bet, you, ', re, getting, lonely, ., [SEP], yeah, [SEP], well, ., E, ##75, ##01, ', s, been, here, [SEP], and, we, ', ve, been, doing, okay, ., [SEP], %, mm, ., [SEP], and, i, ', m, actually, waiting, for, E, ##75, ##01, to, show, up, because, we, were, supposed, to, go, into, town, and, meet, some, people, in, town, ., [SEP], i, ', ve, got, a, student, here, who, ', s, checking, out, rooms, and, stuff, ., [SEP], %, mm, ., [SEP], [MASK], ', s, coming, in, the, fall, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 99 with text: \n",
            " \t\t[[CLS], yeah, [SEP], oh, go, ##sh, ., i, bet, you, ', re, getting, lonely, ., [SEP], yeah, [SEP],\n",
            "\t\twell, ., E, ##75, ##01, ', s, been, here, [SEP], and, we, ', ve, been, doing, okay, ., [SEP], %, mm,\n",
            "\t\t., [SEP], and, i, ', m, actually, waiting, for, E, ##75, ##01, to, show, up, because, we, were,\n",
            "\t\tsupposed, to, go, into, town, and, meet, some, people, in, town, ., [SEP], i, ', ve, got, a,\n",
            "\t\tstudent, here, who, ', s, checking, out, rooms, and, stuff, ., [SEP], %, mm, ., [SEP], [MASK], ', s,\n",
            "\t\tcoming, in, the, fall, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 91. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] was in a place called taiwa . [SEP] [[CLS], but, [MASK], was, in, a, place, called, ta, ##i, ##wa, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], but, [MASK], was, in, a, place, called, ta, ##i, ##wa, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know tufts yeah . sent [MASK] to taiwa [SEP] [[CLS], you, know, t, ##uf, ##ts, yeah, ., sent, [MASK], to, ta, ##i, ##wa, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], you, know, t, ##uf, ##ts, yeah, ., sent, [MASK], to, ta, ##i, ##wa, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i had friends there . [MASK] did a course in taiwa [SEP] [[CLS], i, had, friends, there, ., [MASK], did, a, course, in, ta, ##i, ##wa, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], i, had, friends, there, ., [MASK], did, a, course, in, ta, ##i, ##wa, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and then [MASK] came over here and %um checked out a room here . [SEP] [[CLS], and, then, [MASK], came, over, here, and, %, um, checked, out, a, room, here, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], and, then, [MASK], came, over, here, and, %, um, checked, out, a, room, here, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no it 's [SEP] i do n't know what it is . [SEP] it 's just %uh people get sick [SEP] and they get sick pretty young [SEP] and [SEP] yeah . [SEP] and you know my sister was n't that old [SEP] and [MASK] just had bad luck i guess is all you can say . [SEP] [[CLS], no, it, ', s, [SEP], i, do, n, ', t, know, what, it, is, ., [SEP], it, ', s, just, %, uh, people, get, sick, [SEP], and, they, get, sick, pretty, young, [SEP], and, [SEP], yeah, ., [SEP], and, you, know, my, sister, was, n, ', t, that, old, [SEP], and, [MASK], just, had, bad, luck, i, guess, is, all, you, can, say, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 66 with text: \n",
            " \t\t[[CLS], no, it, ', s, [SEP], i, do, n, ', t, know, what, it, is, ., [SEP], it, ', s, just, %, uh,\n",
            "\t\tpeople, get, sick, [SEP], and, they, get, sick, pretty, young, [SEP], and, [SEP], yeah, ., [SEP],\n",
            "\t\tand, you, know, my, sister, was, n, ', t, that, old, [SEP], and, [MASK], just, had, bad, luck, i,\n",
            "\t\tguess, is, all, you, can, say, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] i 'm going to put you on after fifteen minutes . [SEP] i 've got to [SEP] and there was n't anything anybody could do . [SEP] so . [SEP] and then an aunt of E4168 's aunt E6066 up in maine passed away . [SEP] that had n't happ- [SEP] had that happened already ? [SEP] no [SEP] no that had not happened no . [SEP] [MASK] passed away in about march or [SEP] [[CLS], yeah, ., [SEP], i, ', m, going, to, put, you, on, after, fifteen, minutes, ., [SEP], i, ', ve, got, to, [SEP], and, there, was, n, ', t, anything, anybody, could, do, ., [SEP], so, ., [SEP], and, then, an, aunt, of, E, ##41, ##6, ##8, ', s, aunt, E, ##60, ##6, ##6, up, in, main, ##e, passed, away, ., [SEP], that, had, n, ', t, ha, ##pp, -, [SEP], had, that, happened, already, ?, [SEP], no, [SEP], no, that, had, not, happened, no, ., [SEP], [MASK], passed, away, in, about, march, or, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 95 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], i, ', m, going, to, put, you, on, after, fifteen, minutes, ., [SEP], i, ',\n",
            "\t\tve, got, to, [SEP], and, there, was, n, ', t, anything, anybody, could, do, ., [SEP], so, ., [SEP],\n",
            "\t\tand, then, an, aunt, of, E, ##41, ##6, ##8, ', s, aunt, E, ##60, ##6, ##6, up, in, main, ##e,\n",
            "\t\tpassed, away, ., [SEP], that, had, n, ', t, ha, ##pp, -, [SEP], had, that, happened, already, ?,\n",
            "\t\t[SEP], no, [SEP], no, that, had, not, happened, no, ., [SEP], [MASK], passed, away, in, about,\n",
            "\t\tmarch, or, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 87. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah i think that there was some concern about [MASK] when i talked to you in september or october . [SEP] [[CLS], yeah, i, think, that, there, was, some, concern, about, [MASK], when, i, talked, to, you, in, se, ##pt, ##em, ##ber, or, o, ##ct, ##obe, ##r, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], yeah, i, think, that, there, was, some, concern, about, [MASK], when, i, talked, to, you,\n",
            "\t\tin, se, ##pt, ##em, ##ber, or, o, ##ct, ##obe, ##r, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] . [SEP] yeah . [SEP] no [MASK] was not doing well . [SEP] [[CLS], ., [SEP], yeah, ., [SEP], no, [MASK], was, not, doing, well, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], ., [SEP], yeah, ., [SEP], no, [MASK], was, not, doing, well, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] had a brain tumor . [SEP] [[CLS], [MASK], had, a, brain, tumor, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], had, a, brain, tumor, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh . [SEP] i mean [MASK] did n't know that i was there . [SEP] [[CLS], oh, ., [SEP], i, mean, [MASK], did, n, ', t, know, that, i, was, there, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], oh, ., [SEP], i, mean, [MASK], did, n, ', t, know, that, i, was, there, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] if that was you yeah . yeah [MASK] was just not in [SEP] [[CLS], if, that, was, you, yeah, ., yeah, [MASK], was, just, not, in, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], if, that, was, you, yeah, ., yeah, [MASK], was, just, not, in, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but they 're winding down so you know them really well by this point . [SEP] they 're winding down . [SEP] yeah [SEP] they 're very good . [SEP] they learned a lot of german . [SEP] they learned an amazing amount of german . [SEP] before i forget i saw E861 silinas a couple of days ago . [SEP] oh . how 's [MASK] doing ? [SEP] [[CLS], but, they, ', re, winding, down, so, you, know, them, really, well, by, this, point, ., [SEP], they, ', re, winding, down, ., [SEP], yeah, [SEP], they, ', re, very, good, ., [SEP], they, learned, a, lot, of, g, ##erman, ., [SEP], they, learned, an, amazing, amount, of, g, ##erman, ., [SEP], before, i, forget, i, saw, E, ##86, ##1, si, ##lina, ##s, a, couple, of, days, ago, ., [SEP], oh, ., how, ', s, [MASK], doing, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 80 with text: \n",
            " \t\t[[CLS], but, they, ', re, winding, down, so, you, know, them, really, well, by, this, point, .,\n",
            "\t\t[SEP], they, ', re, winding, down, ., [SEP], yeah, [SEP], they, ', re, very, good, ., [SEP], they,\n",
            "\t\tlearned, a, lot, of, g, ##erman, ., [SEP], they, learned, an, amazing, amount, of, g, ##erman, .,\n",
            "\t\t[SEP], before, i, forget, i, saw, E, ##86, ##1, si, ##lina, ##s, a, couple, of, days, ago, ., [SEP],\n",
            "\t\toh, ., how, ', s, [MASK], doing, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 76. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's had [SEP] [[CLS], and, [MASK], ', s, had, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, had, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well [MASK] 's working for an american firm over here [SEP] [[CLS], well, [MASK], ', s, working, for, an, am, ##eric, ##an, firm, over, here, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], well, [MASK], ', s, working, for, an, am, ##eric, ##an, firm, over, here, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's doing very very well . %um [SEP] [[CLS], and, [MASK], ', s, doing, very, very, well, ., %, um, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, doing, very, very, well, ., %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] wife just had their fourth child . [SEP] [[CLS], and, [MASK], wife, just, had, their, fourth, child, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, [MASK], wife, just, had, their, fourth, child, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] wow . [SEP] a little boy . E1811 . [SEP] yeah . [SEP] wow . [SEP] %um about %uh two weeks ago maybe . [SEP] %huh . [SEP] tops . [SEP] and i got a card from [SEP] you remember E4871 gleason ? [SEP] yeah . [SEP] yeah . [SEP] well [MASK] just had a baby a couple of [SEP] [[CLS], w, ##ow, ., [SEP], a, little, boy, ., E, ##18, ##11, ., [SEP], yeah, ., [SEP], w, ##ow, ., [SEP], %, um, about, %, uh, two, weeks, ago, maybe, ., [SEP], %, huh, ., [SEP], tops, ., [SEP], and, i, got, a, card, from, [SEP], you, remember, E, ##48, ##7, ##1, g, ##lea, ##son, ?, [SEP], yeah, ., [SEP], yeah, ., [SEP], well, [MASK], just, had, a, baby, a, couple, of, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 73 with text: \n",
            " \t\t[[CLS], w, ##ow, ., [SEP], a, little, boy, ., E, ##18, ##11, ., [SEP], yeah, ., [SEP], w, ##ow, .,\n",
            "\t\t[SEP], %, um, about, %, uh, two, weeks, ago, maybe, ., [SEP], %, huh, ., [SEP], tops, ., [SEP], and,\n",
            "\t\ti, got, a, card, from, [SEP], you, remember, E, ##48, ##7, ##1, g, ##lea, ##son, ?, [SEP], yeah, .,\n",
            "\t\t[SEP], yeah, ., [SEP], well, [MASK], just, had, a, baby, a, couple, of, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 64. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] heard that . [SEP] well this is number two . [SEP] okay . [SEP] i just got a card from E3698 and E6645 [SEP] oh . [SEP] and they see E4871 gleason on occasion . [SEP] and so i think they told me that . [SEP] yeah . [SEP] have you heard from E3698 and E6645 ? [SEP] not in a while no . [SEP] they are now living in rhode island . [SEP] you probably knew that . [SEP] well . yeah . [SEP] and [MASK] 's teaching at u r i . [SEP] [[CLS], heard, that, ., [SEP], well, this, is, number, two, ., [SEP], okay, ., [SEP], i, just, got, a, card, from, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, [SEP], oh, ., [SEP], and, they, see, E, ##48, ##7, ##1, g, ##lea, ##son, on, occasion, ., [SEP], and, so, i, think, they, told, me, that, ., [SEP], yeah, ., [SEP], have, you, heard, from, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, ?, [SEP], not, in, a, while, no, ., [SEP], they, are, now, living, in, r, ##ho, ##de, island, ., [SEP], you, probably, knew, that, ., [SEP], well, ., yeah, ., [SEP], and, [MASK], ', s, teaching, at, u, r, i, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 116 with text: \n",
            " \t\t[[CLS], heard, that, ., [SEP], well, this, is, number, two, ., [SEP], okay, ., [SEP], i, just, got,\n",
            "\t\ta, card, from, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, [SEP], oh, ., [SEP], and, they, see, E,\n",
            "\t\t##48, ##7, ##1, g, ##lea, ##son, on, occasion, ., [SEP], and, so, i, think, they, told, me, that, .,\n",
            "\t\t[SEP], yeah, ., [SEP], have, you, heard, from, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, ?, [SEP],\n",
            "\t\tnot, in, a, while, no, ., [SEP], they, are, now, living, in, r, ##ho, ##de, island, ., [SEP], you,\n",
            "\t\tprobably, knew, that, ., [SEP], well, ., yeah, ., [SEP], and, [MASK], ', s, teaching, at, u, r, i,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 106. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 804it [00:05, 185.90it/s]\u001b[A[CLS] yeah . [SEP] and it was a very nice letter because it sounded like [MASK] went through very similar things that i did . [SEP] [[CLS], yeah, ., [SEP], and, it, was, a, very, nice, letter, because, it, sounded, like, [MASK], went, through, very, similar, things, that, i, did, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], and, it, was, a, very, nice, letter, because, it, sounded, like, [MASK],\n",
            "\t\twent, through, very, similar, things, that, i, did, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know i went from chicago to u c s g to here . [SEP] right . [SEP] and it 's just such an intellectual letdown . [SEP] right . [SEP] and [MASK] went from E6592 %um to columbia . to columbia to u r i . [SEP] [[CLS], you, know, i, went, from, ch, ##ica, ##go, to, u, c, s, g, to, here, ., [SEP], right, ., [SEP], and, it, ', s, just, such, an, intellectual, let, ##down, ., [SEP], right, ., [SEP], and, [MASK], went, from, E, ##65, ##9, ##2, %, um, to, co, ##lum, ##bia, ., to, co, ##lum, ##bia, to, u, r, i, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 61 with text: \n",
            " \t\t[[CLS], you, know, i, went, from, ch, ##ica, ##go, to, u, c, s, g, to, here, ., [SEP], right, .,\n",
            "\t\t[SEP], and, it, ', s, just, such, an, intellectual, let, ##down, ., [SEP], right, ., [SEP], and,\n",
            "\t\t[MASK], went, from, E, ##65, ##9, ##2, %, um, to, co, ##lum, ##bia, ., to, co, ##lum, ##bia, to, u,\n",
            "\t\tr, i, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 37. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well columbia 's okay . [SEP] yeah . [SEP] and [MASK] said the students are just [SEP] [[CLS], well, co, ##lum, ##bia, ', s, okay, ., [SEP], yeah, ., [SEP], and, [MASK], said, the, students, are, just, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], well, co, ##lum, ##bia, ', s, okay, ., [SEP], yeah, ., [SEP], and, [MASK], said, the,\n",
            "\t\tstudents, are, just, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know they do n't care [SEP] and they 're not very good [SEP] and all of a sudden [MASK] 's getting conference papers rejected [SEP] [[CLS], you, know, they, do, n, ', t, care, [SEP], and, they, ', re, not, very, good, [SEP], and, all, of, a, sudden, [MASK], ', s, getting, conference, papers, rejected, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 31 with text: \n",
            " \t\t[[CLS], you, know, they, do, n, ', t, care, [SEP], and, they, ', re, not, very, good, [SEP], and,\n",
            "\t\tall, of, a, sudden, [MASK], ', s, getting, conference, papers, rejected, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] thinks it 's the affiliation . [SEP] [[CLS], and, [MASK], thinks, it, ', s, the, affiliation, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, [MASK], thinks, it, ', s, the, affiliation, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and yeah . [MASK] 's accepted that there 's a lot of good things in rhode island . [SEP] [[CLS], and, yeah, ., [MASK], ', s, accepted, that, there, ', s, a, lot, of, good, things, in, r, ##ho, ##de, island, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], and, yeah, ., [MASK], ', s, accepted, that, there, ', s, a, lot, of, good, things, in, r,\n",
            "\t\t##ho, ##de, island, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %um you know their life is a lot less complicated [SEP] and [MASK] has more time to be a parent . [SEP] [[CLS], and, %, um, you, know, their, life, is, a, lot, less, complicated, [SEP], and, [MASK], has, more, time, to, be, a, parent, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], and, %, um, you, know, their, life, is, a, lot, less, complicated, [SEP], and, [MASK], has,\n",
            "\t\tmore, time, to, be, a, parent, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E3698 is working as a financial consultant for %um E4384 lynch . [SEP] and [MASK] [SEP] [[CLS], E, ##36, ##9, ##8, is, working, as, a, financial, consultant, for, %, um, E, ##43, ##8, ##4, l, ##ync, ##h, ., [SEP], and, [MASK], [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], E, ##36, ##9, ##8, is, working, as, a, financial, consultant, for, %, um, E, ##43, ##8, ##4,\n",
            "\t\tl, ##ync, ##h, ., [SEP], and, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh . [SEP] yeah [MASK] had been doing work for some environmental firm as a consultant . [SEP] [[CLS], oh, ., [SEP], yeah, [MASK], had, been, doing, work, for, some, environmental, firm, as, a, consultant, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], oh, ., [SEP], yeah, [MASK], had, been, doing, work, for, some, environmental, firm, as, a,\n",
            "\t\tconsultant, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh . [SEP] and it did n't work out . [SEP] %um [MASK] was changing projects every couple of weeks [SEP] [[CLS], uh, -, huh, ., [SEP], and, it, did, n, ', t, work, out, ., [SEP], %, um, [MASK], was, changing, projects, every, couple, of, weeks, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 27 with text: \n",
            " \t\t[[CLS], uh, -, huh, ., [SEP], and, it, did, n, ', t, work, out, ., [SEP], %, um, [MASK], was,\n",
            "\t\tchanging, projects, every, couple, of, weeks, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] could n't learn the whole new area %mm . that fast each time . [SEP] [[CLS], [MASK], could, n, ', t, learn, the, whole, new, area, %, mm, ., that, fast, each, time, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], [MASK], could, n, ', t, learn, the, whole, new, area, %, mm, ., that, fast, each, time, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] was diagnosed as having attention deficit disorder . [SEP] [[CLS], and, [MASK], was, diagnosed, as, having, attention, deficit, disorder, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], and, [MASK], was, diagnosed, as, having, attention, deficit, disorder, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %mm . i wonder about it . [SEP] but anyway . [SEP] yeah [SEP] but that 's what [MASK] said . [SEP] [[CLS], %, mm, ., i, wonder, about, it, ., [SEP], but, anyway, ., [SEP], yeah, [SEP], but, that, ', s, what, [MASK], said, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], %, mm, ., i, wonder, about, it, ., [SEP], but, anyway, ., [SEP], yeah, [SEP], but, that, ',\n",
            "\t\ts, what, [MASK], said, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 21. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %um [SEP] yeah . [SEP] [MASK] %um [SEP] [[CLS], and, %, um, [SEP], yeah, ., [SEP], [MASK], %, um, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], and, %, um, [SEP], yeah, ., [SEP], [MASK], %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] whatever 's helpful . [SEP] right . [SEP] so [MASK] found this new job as a financial consultant and seems to be happy with that . [SEP] [[CLS], whatever, ', s, helpful, ., [SEP], right, ., [SEP], so, [MASK], found, this, new, job, as, a, financial, consultant, and, seems, to, be, happy, with, that, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], whatever, ', s, helpful, ., [SEP], right, ., [SEP], so, [MASK], found, this, new, job, as,\n",
            "\t\ta, financial, consultant, and, seems, to, be, happy, with, that, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] good . [SEP] and then we saw E488 and E733 at christmas time . [SEP] uh-huh . [SEP] and they 're doing great . [SEP] %um they had just moved to [MASK] 's in new york now right ? a really nice house in westchester . [SEP] [[CLS], good, ., [SEP], and, then, we, saw, E, ##48, ##8, and, E, ##7, ##33, at, ch, ##rist, ##mas, time, ., [SEP], uh, -, huh, ., [SEP], and, they, ', re, doing, great, ., [SEP], %, um, they, had, just, moved, to, [MASK], ', s, in, new, yo, ##rk, now, right, ?, a, really, nice, house, in, west, ##chester, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 61 with text: \n",
            " \t\t[[CLS], good, ., [SEP], and, then, we, saw, E, ##48, ##8, and, E, ##7, ##33, at, ch, ##rist, ##mas,\n",
            "\t\ttime, ., [SEP], uh, -, huh, ., [SEP], and, they, ', re, doing, great, ., [SEP], %, um, they, had,\n",
            "\t\tjust, moved, to, [MASK], ', s, in, new, yo, ##rk, now, right, ?, a, really, nice, house, in, west,\n",
            "\t\t##chester, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 42. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and they mutually agreed that [MASK] should leave . [SEP] [[CLS], and, they, mutually, agreed, that, [MASK], should, leave, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, they, mutually, agreed, that, [MASK], should, leave, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and so [MASK] was looking for another job . [SEP] [[CLS], and, so, [MASK], was, looking, for, another, job, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, so, [MASK], was, looking, for, another, job, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %um i 'm sure [MASK] 'll find one . [SEP] [[CLS], and, %, um, i, ', m, sure, [MASK], ', ll, find, one, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], and, %, um, i, ', m, sure, [MASK], ', ll, find, one, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 823it [00:05, 185.69it/s]\u001b[A[CLS] [MASK] 's so talented . [SEP] [[CLS], [MASK], ', s, so, talented, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], ', s, so, talented, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] may have found one by now . %um [SEP] [[CLS], [MASK], may, have, found, one, by, now, ., %, um, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], may, have, found, one, by, now, ., %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah [SEP] oh yeah . [SEP] amazing [SEP] yeah . [SEP] yeah [SEP] and so we met %um %uh E4917 their kid who was three uh-huh . and just a really lovely little boy . [SEP] right [SEP] yeah [SEP] [MASK] 's so big . [SEP] [[CLS], yeah, [SEP], oh, yeah, ., [SEP], amazing, [SEP], yeah, ., [SEP], yeah, [SEP], and, so, we, met, %, um, %, uh, E, ##4, ##9, ##17, their, kid, who, was, three, uh, -, huh, ., and, just, a, really, lovely, little, boy, ., [SEP], right, [SEP], yeah, [SEP], [MASK], ', s, so, big, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 55 with text: \n",
            " \t\t[[CLS], yeah, [SEP], oh, yeah, ., [SEP], amazing, [SEP], yeah, ., [SEP], yeah, [SEP], and, so, we,\n",
            "\t\tmet, %, um, %, uh, E, ##4, ##9, ##17, their, kid, who, was, three, uh, -, huh, ., and, just, a,\n",
            "\t\treally, lovely, little, boy, ., [SEP], right, [SEP], yeah, [SEP], [MASK], ', s, so, big, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 48. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well E488 's big . you know [SEP] [MASK] 's got E488 's genes . [SEP] [[CLS], well, E, ##48, ##8, ', s, big, ., you, know, [SEP], [MASK], ', s, got, E, ##48, ##8, ', s, genes, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], well, E, ##48, ##8, ', s, big, ., you, know, [SEP], [MASK], ', s, got, E, ##48, ##8, ', s,\n",
            "\t\tgenes, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] yeah . [SEP] and %uh E3698 and E6645 sent us a picture of theirs [SEP] and [MASK] has E3698 's expressions . [SEP] [[CLS], yeah, ., [SEP], yeah, ., [SEP], and, %, uh, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, sent, us, a, picture, of, theirs, [SEP], and, [MASK], has, E, ##36, ##9, ##8, ', s, expressions, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], yeah, ., [SEP], and, %, uh, E, ##36, ##9, ##8, and, E, ##6, ##64, ##5, sent,\n",
            "\t\tus, a, picture, of, theirs, [SEP], and, [MASK], has, E, ##36, ##9, ##8, ', s, expressions, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 27. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] really looks like E3698 . [SEP] [[CLS], [MASK], really, looks, like, E, ##36, ##9, ##8, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], [MASK], really, looks, like, E, ##36, ##9, ##8, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh that 's life . [SEP] %um and [MASK] looked really big too . [SEP] [[CLS], oh, that, ', s, life, ., [SEP], %, um, and, [MASK], looked, really, big, too, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], oh, that, ', s, life, ., [SEP], %, um, and, [MASK], looked, really, big, too, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in this picture [MASK] 's three and a half . [SEP] [[CLS], in, this, picture, [MASK], ', s, three, and, a, half, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], in, this, picture, [MASK], ', s, three, and, a, half, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] %huh . [SEP] %um but E488 and E733 just seemed %um sort of happy with the decisions they 've made . [SEP] and [SEP] good . [SEP] %um like every [SEP] now is [MASK] with a publishing house now ? [SEP] [[CLS], yeah, ., [SEP], %, huh, ., [SEP], %, um, but, E, ##48, ##8, and, E, ##7, ##33, just, seemed, %, um, sort, of, happy, with, the, decisions, they, ', ve, made, ., [SEP], and, [SEP], good, ., [SEP], %, um, like, every, [SEP], now, is, [MASK], with, a, publishing, house, now, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 54 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], %, huh, ., [SEP], %, um, but, E, ##48, ##8, and, E, ##7, ##33, just, seemed,\n",
            "\t\t%, um, sort, of, happy, with, the, decisions, they, ', ve, made, ., [SEP], and, [SEP], good, .,\n",
            "\t\t[SEP], %, um, like, every, [SEP], now, is, [MASK], with, a, publishing, house, now, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 46. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] which one is [MASK] with . [SEP] [[CLS], which, one, is, [MASK], with, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], which, one, is, [MASK], with, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes which one %um [SEP] well i can [SEP] yeah [SEP] it 's one of these sort of universi- the public university book stuff . [SEP] it might be hard core brace . [SEP] i 'm not sure . [SEP] oh yeah . [SEP] i have two friends in publishing . [SEP] it may be the other one that just took the job with hard core brace [SEP] i do n't remember which one . [SEP] yeah . [SEP] anyway [MASK] 's enjoying it and liking it [SEP] [[CLS], yes, which, one, %, um, [SEP], well, i, can, [SEP], yeah, [SEP], it, ', s, one, of, these, sort, of, un, ##ivers, ##i, -, the, public, university, book, stuff, ., [SEP], it, might, be, hard, core, brace, ., [SEP], i, ', m, not, sure, ., [SEP], oh, yeah, ., [SEP], i, have, two, friends, in, publishing, ., [SEP], it, may, be, the, other, one, that, just, took, the, job, with, hard, core, brace, [SEP], i, do, n, ', t, remember, which, one, ., [SEP], yeah, ., [SEP], anyway, [MASK], ', s, enjoying, it, and, liking, it, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 98 with text: \n",
            " \t\t[[CLS], yes, which, one, %, um, [SEP], well, i, can, [SEP], yeah, [SEP], it, ', s, one, of, these,\n",
            "\t\tsort, of, un, ##ivers, ##i, -, the, public, university, book, stuff, ., [SEP], it, might, be, hard,\n",
            "\t\tcore, brace, ., [SEP], i, ', m, not, sure, ., [SEP], oh, yeah, ., [SEP], i, have, two, friends, in,\n",
            "\t\tpublishing, ., [SEP], it, may, be, the, other, one, that, just, took, the, job, with, hard, core,\n",
            "\t\tbrace, [SEP], i, do, n, ', t, remember, which, one, ., [SEP], yeah, ., [SEP], anyway, [MASK], ', s,\n",
            "\t\tenjoying, it, and, liking, it, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 89. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] is great still in academic publishing . [SEP] [[CLS], and, [MASK], is, great, still, in, academic, publishing, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, [MASK], is, great, still, in, academic, publishing, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] my other friend just moved to trade publishing after years in academic publishing [SEP] and mhm %um you know . it feels like maybe [MASK] made a compromise [SEP] [[CLS], my, other, friend, just, moved, to, trade, publishing, after, years, in, academic, publishing, [SEP], and, m, ##hm, %, um, you, know, ., it, feels, like, maybe, [MASK], made, a, compromise, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 32 with text: \n",
            " \t\t[[CLS], my, other, friend, just, moved, to, trade, publishing, after, years, in, academic,\n",
            "\t\tpublishing, [SEP], and, m, ##hm, %, um, you, know, ., it, feels, like, maybe, [MASK], made, a,\n",
            "\t\tcompromise, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 27. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but it 's like three times the money as with everything else [SEP] so [SEP] wow wow [SEP] industry and academics %um [SEP] terrible [SEP] yeah it 's amazing [SEP] and %um you know ages ago i got a card from E5100 gana . [SEP] i told you this . [SEP] because %uh you showed us [MASK] picture . [SEP] [[CLS], but, it, ', s, like, three, times, the, money, as, with, everything, else, [SEP], so, [SEP], w, ##ow, w, ##ow, [SEP], industry, and, academics, %, um, [SEP], terrible, [SEP], yeah, it, ', s, amazing, [SEP], and, %, um, you, know, ages, ago, i, got, a, card, from, E, ##51, ##00, g, ##ana, ., [SEP], i, told, you, this, ., [SEP], because, %, uh, you, showed, us, [MASK], picture, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 71 with text: \n",
            " \t\t[[CLS], but, it, ', s, like, three, times, the, money, as, with, everything, else, [SEP], so, [SEP],\n",
            "\t\tw, ##ow, w, ##ow, [SEP], industry, and, academics, %, um, [SEP], terrible, [SEP], yeah, it, ', s,\n",
            "\t\tamazing, [SEP], and, %, um, you, know, ages, ago, i, got, a, card, from, E, ##51, ##00, g, ##ana, .,\n",
            "\t\t[SEP], i, told, you, this, ., [SEP], because, %, uh, you, showed, us, [MASK], picture, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 67. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %eh oh okay . [SEP] okay so %um [SEP] %eh you told me that you were leaving the company that you were at [SEP] and you were entertaining offers . [SEP] and %uh [SEP] well it seemed to make sense since i had a client to just kind of set up my own company of sorts . [SEP] %um oh . so things are going extremely well . [SEP] so i had one client who said that [MASK] 'd pay me a minimum amount . [SEP] [[CLS], %, eh, oh, okay, ., [SEP], okay, so, %, um, [SEP], %, eh, you, told, me, that, you, were, leaving, the, company, that, you, were, at, [SEP], and, you, were, entertaining, offers, ., [SEP], and, %, uh, [SEP], well, it, seemed, to, make, sense, since, i, had, a, client, to, just, kind, of, set, up, my, own, company, of, sorts, ., [SEP], %, um, oh, ., so, things, are, going, extremely, well, ., [SEP], so, i, had, one, client, who, said, that, [MASK], ', d, pay, me, a, minimum, amount, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 93 with text: \n",
            " \t\t[[CLS], %, eh, oh, okay, ., [SEP], okay, so, %, um, [SEP], %, eh, you, told, me, that, you, were,\n",
            "\t\tleaving, the, company, that, you, were, at, [SEP], and, you, were, entertaining, offers, ., [SEP],\n",
            "\t\tand, %, uh, [SEP], well, it, seemed, to, make, sense, since, i, had, a, client, to, just, kind, of,\n",
            "\t\tset, up, my, own, company, of, sorts, ., [SEP], %, um, oh, ., so, things, are, going, extremely,\n",
            "\t\twell, ., [SEP], so, i, had, one, client, who, said, that, [MASK], ', d, pay, me, a, minimum, amount,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 83. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you know [MASK] %uh [SEP] [[CLS], you, know, [MASK], %, uh, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], you, know, [MASK], %, uh, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you see the one very annoying thing about all of this was my advisor has been in E2648 for a year and a half . [SEP] yeah . [SEP] and this made writing my thesis very annoying because you know there was n't much discussion . [SEP] yeah . [SEP] yup [SEP] yup . [SEP] but %um anyway i had been kind of telling [MASK] that you know from my reading of the literature i thought that this one little alteration i had made in my model was kind of clever and unique . [SEP] [[CLS], you, see, the, one, very, annoying, thing, about, all, of, this, was, my, advisor, has, been, in, E, ##26, ##48, for, a, year, and, a, half, ., [SEP], yeah, ., [SEP], and, this, made, writing, my, thesis, very, annoying, because, you, know, there, was, n, ', t, much, discussion, ., [SEP], yeah, ., [SEP], y, ##up, [SEP], y, ##up, ., [SEP], but, %, um, anyway, i, had, been, kind, of, telling, [MASK], that, you, know, from, my, reading, of, the, literature, i, thought, that, this, one, little, alter, ##ation, i, had, made, in, my, model, was, kind, of, clever, and, unique, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 104 with text: \n",
            " \t\t[[CLS], you, see, the, one, very, annoying, thing, about, all, of, this, was, my, advisor, has,\n",
            "\t\tbeen, in, E, ##26, ##48, for, a, year, and, a, half, ., [SEP], yeah, ., [SEP], and, this, made,\n",
            "\t\twriting, my, thesis, very, annoying, because, you, know, there, was, n, ', t, much, discussion, .,\n",
            "\t\t[SEP], yeah, ., [SEP], y, ##up, [SEP], y, ##up, ., [SEP], but, %, um, anyway, i, had, been, kind,\n",
            "\t\tof, telling, [MASK], that, you, know, from, my, reading, of, the, literature, i, thought, that,\n",
            "\t\tthis, one, little, alter, ##ation, i, had, made, in, my, model, was, kind, of, clever, and, unique,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 72. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and what 's actually kind of interesting is i kept telling E7133 that i thought that this was kind of significant . [SEP] and E7133 kept writing back that i should tone this part of the thesis down that you know people see the anti phased motion [SEP] and it 's true [SEP] they do in the transients [SEP] and i was having a lot of trouble getting across to [MASK] that all of the literature has this frequency in the transients yes . %eh and not driven . [SEP] [[CLS], and, what, ', s, actually, kind, of, interesting, is, i, kept, telling, E, ##7, ##13, ##3, that, i, thought, that, this, was, kind, of, significant, ., [SEP], and, E, ##7, ##13, ##3, kept, writing, back, that, i, should, tone, this, part, of, the, thesis, down, that, you, know, people, see, the, anti, phased, motion, [SEP], and, it, ', s, true, [SEP], they, do, in, the, trans, ##ients, [SEP], and, i, was, having, a, lot, of, trouble, getting, across, to, [MASK], that, all, of, the, literature, has, this, frequency, in, the, trans, ##ients, yes, ., %, eh, and, not, driven, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 102 with text: \n",
            " \t\t[[CLS], and, what, ', s, actually, kind, of, interesting, is, i, kept, telling, E, ##7, ##13, ##3,\n",
            "\t\tthat, i, thought, that, this, was, kind, of, significant, ., [SEP], and, E, ##7, ##13, ##3, kept,\n",
            "\t\twriting, back, that, i, should, tone, this, part, of, the, thesis, down, that, you, know, people,\n",
            "\t\tsee, the, anti, phased, motion, [SEP], and, it, ', s, true, [SEP], they, do, in, the, trans,\n",
            "\t\t##ients, [SEP], and, i, was, having, a, lot, of, trouble, getting, across, to, [MASK], that, all,\n",
            "\t\tof, the, literature, has, this, frequency, in, the, trans, ##ients, yes, ., %, eh, and, not, driven,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 80. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %uh what happened was after i had turned in my thesis but before i saw E7133 again [MASK] went to a conference in E159 and there this was a big issue among the people who were doing this work was that their models could n't show this as a driven frequency [SEP] [[CLS], and, %, uh, what, happened, was, after, i, had, turned, in, my, thesis, but, before, i, saw, E, ##7, ##13, ##3, again, [MASK], went, to, a, conference, in, E, ##15, ##9, and, there, this, was, a, big, issue, among, the, people, who, were, doing, this, work, was, that, their, models, could, n, ', t, show, this, as, a, driven, frequency, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], and, %, uh, what, happened, was, after, i, had, turned, in, my, thesis, but, before, i, saw,\n",
            "\t\tE, ##7, ##13, ##3, again, [MASK], went, to, a, conference, in, E, ##15, ##9, and, there, this, was,\n",
            "\t\ta, big, issue, among, the, people, who, were, doing, this, work, was, that, their, models, could, n,\n",
            "\t\t', t, show, this, as, a, driven, frequency, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 842it [00:05, 186.02it/s]\u001b[A[CLS] yet their experiments showed it as a driven frequency . [SEP] oh cool . [SEP] and so E7133 then -- said oh just a second i -- like a week later came to E5821 mawr [SEP] and [MASK] said hey E4689 we got to write this up . [SEP] [[CLS], yet, their, experiments, showed, it, as, a, driven, frequency, ., [SEP], oh, cool, ., [SEP], and, so, E, ##7, ##13, ##3, then, -, -, said, oh, just, a, second, i, -, -, like, a, week, later, came, to, E, ##5, ##8, ##21, ma, ##w, ##r, [SEP], and, [MASK], said, hey, E, ##46, ##8, ##9, we, got, to, write, this, up, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 63 with text: \n",
            " \t\t[[CLS], yet, their, experiments, showed, it, as, a, driven, frequency, ., [SEP], oh, cool, ., [SEP],\n",
            "\t\tand, so, E, ##7, ##13, ##3, then, -, -, said, oh, just, a, second, i, -, -, like, a, week, later,\n",
            "\t\tcame, to, E, ##5, ##8, ##21, ma, ##w, ##r, [SEP], and, [MASK], said, hey, E, ##46, ##8, ##9, we,\n",
            "\t\tgot, to, write, this, up, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 48. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yes [MASK] did . [SEP] [[CLS], yes, [MASK], did, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], yes, [MASK], did, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and ? [SEP] yes [MASK] did . [SEP] [[CLS], and, ?, [SEP], yes, [MASK], did, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], and, ?, [SEP], yes, [MASK], did, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and ? [SEP] %um %um which we had already anticipated that it was going to happen . [SEP] i spoke to [MASK] mom on thursday [SEP] [[CLS], and, ?, [SEP], %, um, %, um, which, we, had, already, anticipated, that, it, was, going, to, happen, ., [SEP], i, spoke, to, [MASK], mom, on, th, ##urs, ##day, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 31 with text: \n",
            " \t\t[[CLS], and, ?, [SEP], %, um, %, um, which, we, had, already, anticipated, that, it, was, going, to,\n",
            "\t\thappen, ., [SEP], i, spoke, to, [MASK], mom, on, th, ##urs, ##day, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %um and i asked [MASK] [SEP] [[CLS], %, um, and, i, asked, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], %, um, and, i, asked, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i said listen [SEP] i said %um please come to shabbat [SEP] things have come to a point where i may have to relinquish my %uh parenting . [SEP] %uh would you mind %uh taking [MASK] back [SEP] [[CLS], i, said, listen, [SEP], i, said, %, um, please, come, to, s, ##hab, ##bat, [SEP], things, have, come, to, a, point, where, i, may, have, to, re, ##lin, ##quis, ##h, my, %, uh, parent, ##ing, ., [SEP], %, uh, would, you, mind, %, uh, taking, [MASK], back, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 49 with text: \n",
            " \t\t[[CLS], i, said, listen, [SEP], i, said, %, um, please, come, to, s, ##hab, ##bat, [SEP], things,\n",
            "\t\thave, come, to, a, point, where, i, may, have, to, re, ##lin, ##quis, ##h, my, %, uh, parent, ##ing,\n",
            "\t\t., [SEP], %, uh, would, you, mind, %, uh, taking, [MASK], back, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 46. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] said no . [SEP] [[CLS], and, [MASK], said, no, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], and, [MASK], said, no, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] really ? [SEP] this girl has everything in [MASK] dreams you know . [SEP] [[CLS], really, ?, [SEP], this, girl, has, everything, in, [MASK], dreams, you, know, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], really, ?, [SEP], this, girl, has, everything, in, [MASK], dreams, you, know, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's in a two year program . [SEP] [[CLS], [MASK], ', s, in, a, two, year, program, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], [MASK], ', s, in, a, two, year, program, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's in the program now . [SEP] [[CLS], [MASK], ', s, in, the, program, now, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], [MASK], ', s, in, the, program, now, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's doing very well . [SEP] [[CLS], [MASK], ', s, doing, very, well, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], ', s, doing, very, well, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i said i feel like i know you even though i never met you . [SEP] they seemed surprised that [SEP] i mean most women you lose your baby that 's it . [SEP] but almighty god is [SEP] you never even get a chance to see them again . [SEP] that 's true . [SEP] that 's true . [SEP] but that 's what [MASK] had wrote me . [SEP] [[CLS], i, said, i, feel, like, i, know, you, even, though, i, never, met, you, ., [SEP], they, seemed, surprised, that, [SEP], i, mean, most, women, you, lose, your, baby, that, ', s, it, ., [SEP], but, al, ##mi, ##ghty, god, is, [SEP], you, never, even, get, a, chance, to, see, them, again, ., [SEP], that, ', s, true, ., [SEP], that, ', s, true, ., [SEP], but, that, ', s, what, [MASK], had, wrote, me, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 78 with text: \n",
            " \t\t[[CLS], i, said, i, feel, like, i, know, you, even, though, i, never, met, you, ., [SEP], they,\n",
            "\t\tseemed, surprised, that, [SEP], i, mean, most, women, you, lose, your, baby, that, ', s, it, .,\n",
            "\t\t[SEP], but, al, ##mi, ##ghty, god, is, [SEP], you, never, even, get, a, chance, to, see, them,\n",
            "\t\tagain, ., [SEP], that, ', s, true, ., [SEP], that, ', s, true, ., [SEP], but, that, ', s, what,\n",
            "\t\t[MASK], had, wrote, me, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 72. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no . [SEP] we have n't really talked about that [SEP] but i 'm planning on making a trip down to visit [MASK] . [SEP] [[CLS], no, ., [SEP], we, have, n, ', t, really, talked, about, that, [SEP], but, i, ', m, planning, on, making, a, trip, down, to, visit, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], no, ., [SEP], we, have, n, ', t, really, talked, about, that, [SEP], but, i, ', m, planning,\n",
            "\t\ton, making, a, trip, down, to, visit, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 26. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well how does akina feel ? [SEP] [MASK] 's very happy about it . [SEP] [[CLS], well, how, does, akin, ##a, feel, ?, [SEP], [MASK], ', s, very, happy, about, it, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], well, how, does, akin, ##a, feel, ?, [SEP], [MASK], ', s, very, happy, about, it, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] we 've spent the night with [MASK] sisters and things [SEP] [[CLS], we, ', ve, spent, the, night, with, [MASK], sisters, and, things, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], we, ', ve, spent, the, night, with, [MASK], sisters, and, things, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-oh ! [SEP] E3737 the research did n't hear that ! [SEP] uh-oh ! [SEP] wait [SEP] wait [SEP] alita i 'm telling you nobody can handle [MASK] but you okay . [SEP] [[CLS], uh, -, oh, !, [SEP], E, ##37, ##37, the, research, did, n, ', t, hear, that, !, [SEP], uh, -, oh, !, [SEP], wait, [SEP], wait, [SEP], al, ##ita, i, ', m, telling, you, nobody, can, handle, [MASK], but, you, okay, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], uh, -, oh, !, [SEP], E, ##37, ##37, the, research, did, n, ', t, hear, that, !, [SEP], uh,\n",
            "\t\t-, oh, !, [SEP], wait, [SEP], wait, [SEP], al, ##ita, i, ', m, telling, you, nobody, can, handle,\n",
            "\t\t[MASK], but, you, okay, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 38. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and %um [SEP] but [MASK] was very concerned about me and everything %um because %um the child had told some lies about me and about you . [SEP] [[CLS], and, %, um, [SEP], but, [MASK], was, very, concerned, about, me, and, everything, %, um, because, %, um, the, child, had, told, some, lies, about, me, and, about, you, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 32 with text: \n",
            " \t\t[[CLS], and, %, um, [SEP], but, [MASK], was, very, concerned, about, me, and, everything, %, um,\n",
            "\t\tbecause, %, um, the, child, had, told, some, lies, about, me, and, about, you, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh . [SEP] oh 296.72_297.91_a: oh no . [SEP] and about [SEP] oh yeah mhm [SEP] oh goodness . [SEP] yeah . [SEP] that we were abusing [SEP] that %um you know [SEP] oh yeah [SEP] [MASK] did some %eh [SEP] [[CLS], oh, ., [SEP], oh, 29, ##6, ., 72, _, 29, ##7, ., 91, _, a, :, oh, no, ., [SEP], and, about, [SEP], oh, yeah, m, ##hm, [SEP], oh, goodness, ., [SEP], yeah, ., [SEP], that, we, were, a, ##bus, ##ing, [SEP], that, %, um, you, know, [SEP], oh, yeah, [SEP], [MASK], did, some, %, eh, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 58 with text: \n",
            " \t\t[[CLS], oh, ., [SEP], oh, 29, ##6, ., 72, _, 29, ##7, ., 91, _, a, :, oh, no, ., [SEP], and, about,\n",
            "\t\t[SEP], oh, yeah, m, ##hm, [SEP], oh, goodness, ., [SEP], yeah, ., [SEP], that, we, were, a, ##bus,\n",
            "\t\t##ing, [SEP], that, %, um, you, know, [SEP], oh, yeah, [SEP], [MASK], did, some, %, eh, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and it was very very bad . [SEP] my daughter - in - law E1194 got on [MASK] case real serious yesterday . [SEP] [[CLS], and, it, was, very, very, bad, ., [SEP], my, daughter, -, in, -, law, E, ##11, ##9, ##4, got, on, [MASK], case, real, serious, yesterday, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], and, it, was, very, very, bad, ., [SEP], my, daughter, -, in, -, law, E, ##11, ##9, ##4,\n",
            "\t\tgot, on, [MASK], case, real, serious, yesterday, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 21. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 861it [00:05, 187.14it/s]\u001b[A[CLS] and [MASK] said that if it was n't for jetta you know E3551 's daddy would n't be [SEP] [[CLS], and, [MASK], said, that, if, it, was, n, ', t, for, jet, ##ta, you, know, E, ##35, ##51, ', s, daddy, would, n, ', t, be, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], and, [MASK], said, that, if, it, was, n, ', t, for, jet, ##ta, you, know, E, ##35, ##51, ',\n",
            "\t\ts, daddy, would, n, ', t, be, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh [MASK] 'd [SEP] [[CLS], oh, [MASK], ', d, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], oh, [MASK], ', d, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh man %uh it was [SEP] aw . [SEP] you know that has always been very scary to me even after . [SEP] i said to E4263 oh my god . what 's [MASK] going to say [SEP] [[CLS], oh, man, %, uh, it, was, [SEP], a, ##w, ., [SEP], you, know, that, has, always, been, very, scary, to, me, even, after, ., [SEP], i, said, to, E, ##42, ##6, ##3, oh, my, god, ., what, ', s, [MASK], going, to, say, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], oh, man, %, uh, it, was, [SEP], a, ##w, ., [SEP], you, know, that, has, always, been, very,\n",
            "\t\tscary, to, me, even, after, ., [SEP], i, said, to, E, ##42, ##6, ##3, oh, my, god, ., what, ', s,\n",
            "\t\t[MASK], going, to, say, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 40. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i [SEP] you know in the climate of today yes . people saying something like that . [SEP] that well [SEP] you know that 's [SEP] and E3709 was very concerned in reference to you know [MASK] was in school and things [SEP] [[CLS], and, i, [SEP], you, know, in, the, climate, of, today, yes, ., people, saying, something, like, that, ., [SEP], that, well, [SEP], you, know, that, ', s, [SEP], and, E, ##37, ##0, ##9, was, very, concerned, in, reference, to, you, know, [MASK], was, in, school, and, things, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 49 with text: \n",
            " \t\t[[CLS], and, i, [SEP], you, know, in, the, climate, of, today, yes, ., people, saying, something,\n",
            "\t\tlike, that, ., [SEP], that, well, [SEP], you, know, that, ', s, [SEP], and, E, ##37, ##0, ##9, was,\n",
            "\t\tvery, concerned, in, reference, to, you, know, [MASK], was, in, school, and, things, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 42. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] was very very concerned . [SEP] [[CLS], and, [MASK], was, very, very, concerned, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], and, [MASK], was, very, very, concerned, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] alita [MASK] said this little girl has a serious problem . [SEP] [[CLS], al, ##ita, [MASK], said, this, little, girl, has, a, serious, problem, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], al, ##ita, [MASK], said, this, little, girl, has, a, serious, problem, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh well the thing is your whole life has been around [MASK] mhm . more or less [SEP] [[CLS], oh, well, the, thing, is, your, whole, life, has, been, around, [MASK], m, ##hm, ., more, or, less, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], oh, well, the, thing, is, your, whole, life, has, been, around, [MASK], m, ##hm, ., more,\n",
            "\t\tor, less, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i have no idea . [SEP] i really have no idea . [SEP] but [SEP] %um but you know the [SEP] if this does happen and i do believe it may happen %um you know we will have an agreement [MASK] and i you know [SEP] [[CLS], i, have, no, idea, ., [SEP], i, really, have, no, idea, ., [SEP], but, [SEP], %, um, but, you, know, the, [SEP], if, this, does, happen, and, i, do, believe, it, may, happen, %, um, you, know, we, will, have, an, agreement, [MASK], and, i, you, know, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 49 with text: \n",
            " \t\t[[CLS], i, have, no, idea, ., [SEP], i, really, have, no, idea, ., [SEP], but, [SEP], %, um, but,\n",
            "\t\tyou, know, the, [SEP], if, this, does, happen, and, i, do, believe, it, may, happen, %, um, you,\n",
            "\t\tknow, we, will, have, an, agreement, [MASK], and, i, you, know, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %um [MASK] will become the legal guardian and that %um [SEP] [[CLS], %, um, [MASK], will, become, the, legal, guardian, and, that, %, um, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], %, um, [MASK], will, become, the, legal, guardian, and, that, %, um, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %uh you know i just want [MASK] to understand everything that has happened in the last several years yeah . and things like that . [SEP] [[CLS], %, uh, you, know, i, just, want, [MASK], to, understand, everything, that, has, happened, in, the, last, several, years, yeah, ., and, things, like, that, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], %, uh, you, know, i, just, want, [MASK], to, understand, everything, that, has, happened,\n",
            "\t\tin, the, last, several, years, yeah, ., and, things, like, that, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] needs to be clear about that because this child wo n't be like the other two . [SEP] [[CLS], [MASK], needs, to, be, clear, about, that, because, this, child, w, ##o, n, ', t, be, like, the, other, two, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 23 with text: \n",
            " \t\t[[CLS], [MASK], needs, to, be, clear, about, that, because, this, child, w, ##o, n, ', t, be, like,\n",
            "\t\tthe, other, two, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E211 said that [MASK] was . [SEP] [[CLS], E, ##21, ##1, said, that, [MASK], was, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], E, ##21, ##1, said, that, [MASK], was, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %ah . [SEP] and they 're twelve and [SEP] no they 're nine and thirteen or something like that . [SEP] yup [SEP] but [MASK] 's working very hard . [SEP] [[CLS], %, ah, ., [SEP], and, they, ', re, twelve, and, [SEP], no, they, ', re, nine, and, thirteen, or, something, like, that, ., [SEP], y, ##up, [SEP], but, [MASK], ', s, working, very, hard, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 37 with text: \n",
            " \t\t[[CLS], %, ah, ., [SEP], and, they, ', re, twelve, and, [SEP], no, they, ', re, nine, and, thirteen,\n",
            "\t\tor, something, like, that, ., [SEP], y, ##up, [SEP], but, [MASK], ', s, working, very, hard, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 29. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's trying to become a counselor in this drug program and things [SEP] [[CLS], [MASK], ', s, trying, to, become, a, counselor, in, this, drug, program, and, things, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], [MASK], ', s, trying, to, become, a, counselor, in, this, drug, program, and, things, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i just believe it 's you know time you know [SEP] and who would have ever thought it though [SEP] you know what i mean ? [SEP] does [MASK] know that you 're muslim ? [SEP] [[CLS], and, i, just, believe, it, ', s, you, know, time, you, know, [SEP], and, who, would, have, ever, thought, it, though, [SEP], you, know, what, i, mean, ?, [SEP], does, [MASK], know, that, you, ', re, m, ##us, ##lim, ?, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 42 with text: \n",
            " \t\t[[CLS], and, i, just, believe, it, ', s, you, know, time, you, know, [SEP], and, who, would, have,\n",
            "\t\tever, thought, it, though, [SEP], you, know, what, i, mean, ?, [SEP], does, [MASK], know, that, you,\n",
            "\t\t', re, m, ##us, ##lim, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 31. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but you know i always worried [SEP] you know even after you left i said i just hope arlita knows that you know i tried my best you know [SEP] and everybody has to keep them selema everybody everybody you know [SEP] and i told [MASK] [SEP] [[CLS], but, you, know, i, always, worried, [SEP], you, know, even, after, you, left, i, said, i, just, hope, a, ##rl, ##ita, knows, that, you, know, i, tried, my, best, you, know, [SEP], and, everybody, has, to, keep, them, se, ##lem, ##a, everybody, everybody, you, know, [SEP], and, i, told, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 52 with text: \n",
            " \t\t[[CLS], but, you, know, i, always, worried, [SEP], you, know, even, after, you, left, i, said, i,\n",
            "\t\tjust, hope, a, ##rl, ##ita, knows, that, you, know, i, tried, my, best, you, know, [SEP], and,\n",
            "\t\teverybody, has, to, keep, them, se, ##lem, ##a, everybody, everybody, you, know, [SEP], and, i,\n",
            "\t\ttold, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i said you know it 's a shame that you ca n't go no place and i can get a good report on you you know %um and by [MASK] going to E3709 and things and E3709 seeing for herself [SEP] [[CLS], i, said, you, know, it, ', s, a, shame, that, you, ca, n, ', t, go, no, place, and, i, can, get, a, good, report, on, you, you, know, %, um, and, by, [MASK], going, to, E, ##37, ##0, ##9, and, things, and, E, ##37, ##0, ##9, seeing, for, herself, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 52 with text: \n",
            " \t\t[[CLS], i, said, you, know, it, ', s, a, shame, that, you, ca, n, ', t, go, no, place, and, i, can,\n",
            "\t\tget, a, good, report, on, you, you, know, %, um, and, by, [MASK], going, to, E, ##37, ##0, ##9, and,\n",
            "\t\tthings, and, E, ##37, ##0, ##9, seeing, for, herself, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] was like whoa . [SEP] [[CLS], [MASK], was, like, who, ##a, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], was, like, who, ##a, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] did E3709 feel [MASK] had changed a lot ? [SEP] [[CLS], did, E, ##37, ##0, ##9, feel, [MASK], had, changed, a, lot, ?, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], did, E, ##37, ##0, ##9, feel, [MASK], had, changed, a, lot, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 880it [00:06, 187.72it/s]\u001b[A[CLS] oh yes . [SEP] t- [SEP] oh yes . [SEP] you know mhm mhm . [SEP] a- [SEP] %ah . [SEP] %um %um but you know i feel very at peace about everything . [SEP] uh-huh . [SEP] i feel very at peace you know . [SEP] well it 's good to hear from E3709 who 's had a lot of child- [SEP] yes . [SEP] over six hundred . [SEP] oh . [SEP] over six hundred foster children . [SEP] oh . [SEP] yes . [SEP] mhm . [SEP] you know [MASK] would 've seen it all . [SEP] [[CLS], oh, yes, ., [SEP], t, -, [SEP], oh, yes, ., [SEP], you, know, m, ##hm, m, ##hm, ., [SEP], a, -, [SEP], %, ah, ., [SEP], %, um, %, um, but, you, know, i, feel, very, at, peace, about, everything, ., [SEP], uh, -, huh, ., [SEP], i, feel, very, at, peace, you, know, ., [SEP], well, it, ', s, good, to, hear, from, E, ##37, ##0, ##9, who, ', s, had, a, lot, of, child, -, [SEP], yes, ., [SEP], over, six, hundred, ., [SEP], oh, ., [SEP], over, six, hundred, foster, children, ., [SEP], oh, ., [SEP], yes, ., [SEP], m, ##hm, ., [SEP], you, know, [MASK], would, ', ve, seen, it, all, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 118 with text: \n",
            " \t\t[[CLS], oh, yes, ., [SEP], t, -, [SEP], oh, yes, ., [SEP], you, know, m, ##hm, m, ##hm, ., [SEP], a,\n",
            "\t\t-, [SEP], %, ah, ., [SEP], %, um, %, um, but, you, know, i, feel, very, at, peace, about,\n",
            "\t\teverything, ., [SEP], uh, -, huh, ., [SEP], i, feel, very, at, peace, you, know, ., [SEP], well, it,\n",
            "\t\t', s, good, to, hear, from, E, ##37, ##0, ##9, who, ', s, had, a, lot, of, child, -, [SEP], yes, .,\n",
            "\t\t[SEP], over, six, hundred, ., [SEP], oh, ., [SEP], over, six, hundred, foster, children, ., [SEP],\n",
            "\t\toh, ., [SEP], yes, ., [SEP], m, ##hm, ., [SEP], you, know, [MASK], would, ', ve, seen, it, all, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 109. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] called mom to talk to mom [SEP] [[CLS], [MASK], called, mom, to, talk, to, mom, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], called, mom, to, talk, to, mom, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] let 's see [MASK] bought [SEP] [[CLS], let, ', s, see, [MASK], bought, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], let, ', s, see, [MASK], bought, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 5. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i met [MASK] in albany [SEP] [[CLS], i, met, [MASK], in, al, ##ban, ##y, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], i, met, [MASK], in, al, ##ban, ##y, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %um [MASK] caught [SEP] [[CLS], %, um, [MASK], caught, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], %, um, [MASK], caught, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] that was on friday last friday [SEP] [MASK] called mom monday and talked to mom for two hours [SEP] [[CLS], that, was, on, f, ##rida, ##y, last, f, ##rida, ##y, [SEP], [MASK], called, mom, mon, ##day, and, talked, to, mom, for, two, hours, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], that, was, on, f, ##rida, ##y, last, f, ##rida, ##y, [SEP], [MASK], called, mom, mon, ##day,\n",
            "\t\tand, talked, to, mom, for, two, hours, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh . [SEP] %mm . [SEP] [MASK] talked to mom here in E159 . [SEP] [[CLS], uh, -, huh, ., [SEP], %, mm, ., [SEP], [MASK], talked, to, mom, here, in, E, ##15, ##9, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 21 with text: \n",
            " \t\t[[CLS], uh, -, huh, ., [SEP], %, mm, ., [SEP], [MASK], talked, to, mom, here, in, E, ##15, ##9, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh you 're kidding [SEP] i was sick up in syracuse . [SEP] [MASK] talked to mom . [SEP] [[CLS], oh, you, ', re, kidding, [SEP], i, was, sick, up, in, s, ##yra, ##cus, ##e, ., [SEP], [MASK], talked, to, mom, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], oh, you, ', re, kidding, [SEP], i, was, sick, up, in, s, ##yra, ##cus, ##e, ., [SEP],\n",
            "\t\t[MASK], talked, to, mom, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 18. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] something must 've been on [MASK] mind . [SEP] [[CLS], something, must, ', ve, been, on, [MASK], mind, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], something, must, ', ve, been, on, [MASK], mind, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] told mom [SEP] [[CLS], [MASK], told, mom, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 5 with text: \n",
            " \t\t[[CLS], [MASK], told, mom, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh . [SEP] and [MASK] was just really concerned about you know everything . mhm you know . what the possibilities of like [SEP] [[CLS], uh, -, huh, ., [SEP], and, [MASK], was, just, really, concerned, about, you, know, everything, ., m, ##hm, you, know, ., what, the, possibilities, of, like, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 28 with text: \n",
            " \t\t[[CLS], uh, -, huh, ., [SEP], and, [MASK], was, just, really, concerned, about, you, know,\n",
            "\t\teverything, ., m, ##hm, you, know, ., what, the, possibilities, of, like, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %ah [SEP] i said well you call selema [SEP] and you tell [MASK] . [SEP] [[CLS], %, ah, [SEP], i, said, well, you, call, se, ##lem, ##a, [SEP], and, you, tell, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], %, ah, [SEP], i, said, well, you, call, se, ##lem, ##a, [SEP], and, you, tell, [MASK], .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 16. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yup [MASK] sure is . [SEP] [[CLS], y, ##up, [MASK], sure, is, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], y, ##up, [MASK], sure, is, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh my goodness . [SEP] goodness . [SEP] E687 [SEP] wait [SEP] did they do like %uh the same thing they had to do last time ? [SEP] no . [SEP] you know what ? [SEP] that same thing happened to aliah . [SEP] no . [SEP] [MASK] had to take all that stuff the first time [SEP] [[CLS], oh, my, goodness, ., [SEP], goodness, ., [SEP], E, ##6, ##8, ##7, [SEP], wait, [SEP], did, they, do, like, %, uh, the, same, thing, they, had, to, do, last, time, ?, [SEP], no, ., [SEP], you, know, what, ?, [SEP], that, same, thing, happened, to, al, ##iah, ., [SEP], no, ., [SEP], [MASK], had, to, take, all, that, stuff, the, first, time, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 64 with text: \n",
            " \t\t[[CLS], oh, my, goodness, ., [SEP], goodness, ., [SEP], E, ##6, ##8, ##7, [SEP], wait, [SEP], did,\n",
            "\t\tthey, do, like, %, uh, the, same, thing, they, had, to, do, last, time, ?, [SEP], no, ., [SEP], you,\n",
            "\t\tknow, what, ?, [SEP], that, same, thing, happened, to, al, ##iah, ., [SEP], no, ., [SEP], [MASK],\n",
            "\t\thad, to, take, all, that, stuff, the, first, time, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 53. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but then the second time [SEP] nope . [SEP] [MASK] [SEP] [[CLS], but, then, the, second, time, [SEP], no, ##pe, ., [SEP], [MASK], [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], but, then, the, second, time, [SEP], no, ##pe, ., [SEP], [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yup . [SEP] yup . [SEP] j- j- [SEP] %uh and mom said [SEP] i mean E3551 was about two months old [SEP] mom said oh goodness . [SEP] %eh i mean cause E3551 is not nowhere like my other little babies . [SEP] [MASK] is not cuddly sweet . [SEP] [[CLS], y, ##up, ., [SEP], y, ##up, ., [SEP], j, -, j, -, [SEP], %, uh, and, mom, said, [SEP], i, mean, E, ##35, ##51, was, about, two, months, old, [SEP], mom, said, oh, goodness, ., [SEP], %, eh, i, mean, cause, E, ##35, ##51, is, not, nowhere, like, my, other, little, babies, ., [SEP], [MASK], is, not, cu, ##dd, ##ly, sweet, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 64 with text: \n",
            " \t\t[[CLS], y, ##up, ., [SEP], y, ##up, ., [SEP], j, -, j, -, [SEP], %, uh, and, mom, said, [SEP], i,\n",
            "\t\tmean, E, ##35, ##51, was, about, two, months, old, [SEP], mom, said, oh, goodness, ., [SEP], %, eh,\n",
            "\t\ti, mean, cause, E, ##35, ##51, is, not, nowhere, like, my, other, little, babies, ., [SEP], [MASK],\n",
            "\t\tis, not, cu, ##dd, ##ly, sweet, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 55. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %mm %mm no w- [SEP] [MASK] do n't have time for that %uh . [SEP] [[CLS], %, mm, %, mm, no, w, -, [SEP], [MASK], do, n, ', t, have, time, for, that, %, uh, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], %, mm, %, mm, no, w, -, [SEP], [MASK], do, n, ', t, have, time, for, that, %, uh, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] this little girl moves . [SEP] i mean you lay [MASK] down on the bed [SEP] [[CLS], this, little, girl, moves, ., [SEP], i, mean, you, lay, [MASK], down, on, the, bed, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], this, little, girl, moves, ., [SEP], i, mean, you, lay, [MASK], down, on, the, bed, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] gets up [SEP] [[CLS], [MASK], gets, up, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 5 with text: \n",
            " \t\t[[CLS], [MASK], gets, up, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] runs from [SEP] [[CLS], and, [MASK], runs, from, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], and, [MASK], runs, from, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well [MASK] does n't run [SEP] [[CLS], well, [MASK], does, n, ', t, run, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], well, [MASK], does, n, ', t, run, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 901it [00:06, 194.20it/s]\u001b[A[CLS] but [MASK] crawls real fast . [SEP] [[CLS], but, [MASK], crawl, ##s, real, fast, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], but, [MASK], crawl, ##s, real, fast, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] has t- aw [SEP] [[CLS], and, [MASK], has, t, -, a, ##w, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], and, [MASK], has, t, -, a, ##w, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and you 're like okay . [SEP] and [MASK] 's not a cuddly sweet little thing [SEP] [[CLS], and, you, ', re, like, okay, ., [SEP], and, [MASK], ', s, not, a, cu, ##dd, ##ly, sweet, little, thing, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], and, you, ', re, like, okay, ., [SEP], and, [MASK], ', s, not, a, cu, ##dd, ##ly, sweet,\n",
            "\t\tlittle, thing, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] [SEP] [[CLS], [MASK], [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 3 with text: \n",
            " \t\t[[CLS], [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] climbs up the stairs selema . [SEP] [[CLS], and, [MASK], climbs, up, the, stairs, se, ##lem, ##a, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], and, [MASK], climbs, up, the, stairs, se, ##lem, ##a, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] 's seven months old [SEP] [[CLS], [MASK], ', s, seven, months, old, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 8 with text: \n",
            " \t\t[[CLS], [MASK], ', s, seven, months, old, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's climbing up the stairs . [SEP] [[CLS], and, [MASK], ', s, climbing, up, the, stairs, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 11 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, climbing, up, the, stairs, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] climbs down the stairs [SEP] and mom said so we found out [SEP] when did we find out E1194 was pregnant ? [SEP] last week we found out [SEP] and mom told [MASK] [SEP] [[CLS], climbs, down, the, stairs, [SEP], and, mom, said, so, we, found, out, [SEP], when, did, we, find, out, E, ##11, ##9, ##4, was, pregnant, ?, [SEP], last, week, we, found, out, [SEP], and, mom, told, [MASK], [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], climbs, down, the, stairs, [SEP], and, mom, said, so, we, found, out, [SEP], when, did, we,\n",
            "\t\tfind, out, E, ##11, ##9, ##4, was, pregnant, ?, [SEP], last, week, we, found, out, [SEP], and, mom,\n",
            "\t\ttold, [MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mom said E1194 E3551 is just moving [SEP] [MASK] 's moving out the way too quickly [SEP] [[CLS], mom, said, E, ##11, ##9, ##4, E, ##35, ##51, is, just, moving, [SEP], [MASK], ', s, moving, out, the, way, too, quickly, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], mom, said, E, ##11, ##9, ##4, E, ##35, ##51, is, just, moving, [SEP], [MASK], ', s, moving,\n",
            "\t\tout, the, way, too, quickly, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] just was not a little sweet little cuddly you know [SEP] [[CLS], [MASK], just, was, not, a, little, sweet, little, cu, ##dd, ##ly, you, know, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], [MASK], just, was, not, a, little, sweet, little, cu, ##dd, ##ly, you, know, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but i still need to go more often . [SEP] well how often are you going ? [SEP] only four days a week . [SEP] . [SEP] yeah . [SEP] got to get that up to five . [SEP] %mm . [SEP] but i do n't really like going with E5489 E4427 because [MASK] does n't like to stay very long . [SEP] [[CLS], but, i, still, need, to, go, more, often, ., [SEP], well, how, often, are, you, going, ?, [SEP], only, four, days, a, week, ., [SEP], ., [SEP], yeah, ., [SEP], got, to, get, that, up, to, five, ., [SEP], %, mm, ., [SEP], but, i, do, n, ', t, really, like, going, with, E, ##5, ##48, ##9, E, ##44, ##27, because, [MASK], does, n, ', t, like, to, stay, very, long, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 74 with text: \n",
            " \t\t[[CLS], but, i, still, need, to, go, more, often, ., [SEP], well, how, often, are, you, going, ?,\n",
            "\t\t[SEP], only, four, days, a, week, ., [SEP], ., [SEP], yeah, ., [SEP], got, to, get, that, up, to,\n",
            "\t\tfive, ., [SEP], %, mm, ., [SEP], but, i, do, n, ', t, really, like, going, with, E, ##5, ##48, ##9,\n",
            "\t\tE, ##44, ##27, because, [MASK], does, n, ', t, like, to, stay, very, long, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 62. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] why ? [SEP] i do n't know . [SEP] [MASK] kind of [SEP] [[CLS], why, ?, [SEP], i, do, n, ', t, know, ., [SEP], [MASK], kind, of, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], why, ?, [SEP], i, do, n, ', t, know, ., [SEP], [MASK], kind, of, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well unfortunately for some reason [MASK] 's not like into cardio vascular . [SEP] [[CLS], well, unfortunately, for, some, reason, [MASK], ', s, not, like, into, card, ##io, vascular, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], well, unfortunately, for, some, reason, [MASK], ', s, not, like, into, card, ##io, vascular,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mhm . [SEP] and like i have n't even started mine . mhm . you know [SEP] and i kind of hinted to [MASK] that really the only way to lose weight is cardio vascular . mhm . it 's true . and that lifting the weights wo n't lose weight . [SEP] [[CLS], m, ##hm, ., [SEP], and, like, i, have, n, ', t, even, started, mine, ., m, ##hm, ., you, know, [SEP], and, i, kind, of, hinted, to, [MASK], that, really, the, only, way, to, lose, weight, is, card, ##io, vascular, ., m, ##hm, ., it, ', s, true, ., and, that, lifting, the, weights, w, ##o, n, ', t, lose, weight, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 64 with text: \n",
            " \t\t[[CLS], m, ##hm, ., [SEP], and, like, i, have, n, ', t, even, started, mine, ., m, ##hm, ., you,\n",
            "\t\tknow, [SEP], and, i, kind, of, hinted, to, [MASK], that, really, the, only, way, to, lose, weight,\n",
            "\t\tis, card, ##io, vascular, ., m, ##hm, ., it, ', s, true, ., and, that, lifting, the, weights, w,\n",
            "\t\t##o, n, ', t, lose, weight, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 28. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %mm %mm . [SEP] you wo n't lose it you know [SEP] but [SEP] no [SEP] i mean you might lose some [SEP] but that 's only when yo- . [SEP] for a woman who is lifting not that much i do n't think yeah . you would . [SEP] i mean yeah [SEP] i mean the only thing is %um just getting your heart rate up for extended periods of time . [SEP] yeah [SEP] so i do n't know [SEP] maybe [MASK] 'll catch on soon . [SEP] [[CLS], %, mm, %, mm, ., [SEP], you, w, ##o, n, ', t, lose, it, you, know, [SEP], but, [SEP], no, [SEP], i, mean, you, might, lose, some, [SEP], but, that, ', s, only, when, yo, -, ., [SEP], for, a, woman, who, is, lifting, not, that, much, i, do, n, ', t, think, yeah, ., you, would, ., [SEP], i, mean, yeah, [SEP], i, mean, the, only, thing, is, %, um, just, getting, your, heart, rate, up, for, extended, periods, of, time, ., [SEP], yeah, [SEP], so, i, do, n, ', t, know, [SEP], maybe, [MASK], ', ll, catch, on, soon, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 104 with text: \n",
            " \t\t[[CLS], %, mm, %, mm, ., [SEP], you, w, ##o, n, ', t, lose, it, you, know, [SEP], but, [SEP], no,\n",
            "\t\t[SEP], i, mean, you, might, lose, some, [SEP], but, that, ', s, only, when, yo, -, ., [SEP], for, a,\n",
            "\t\twoman, who, is, lifting, not, that, much, i, do, n, ', t, think, yeah, ., you, would, ., [SEP], i,\n",
            "\t\tmean, yeah, [SEP], i, mean, the, only, thing, is, %, um, just, getting, your, heart, rate, up, for,\n",
            "\t\textended, periods, of, time, ., [SEP], yeah, [SEP], so, i, do, n, ', t, know, [SEP], maybe, [MASK],\n",
            "\t\t', ll, catch, on, soon, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 96. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's at the hospital nearby . [SEP] well that 's a deal . [SEP] yeah . [SEP] yeah . [SEP] so uh-huh . i think %um tonight E583 and i are going to go over and join up . [SEP] even E583 's going to join ? [SEP] no [SEP] but it has to be debited from a bank account so it 's got to be debited from [MASK] . [SEP] [[CLS], it, ', s, at, the, hospital, nearby, ., [SEP], well, that, ', s, a, deal, ., [SEP], yeah, ., [SEP], yeah, ., [SEP], so, uh, -, huh, ., i, think, %, um, tonight, E, ##5, ##8, ##3, and, i, are, going, to, go, over, and, join, up, ., [SEP], even, E, ##5, ##8, ##3, ', s, going, to, join, ?, [SEP], no, [SEP], but, it, has, to, be, de, ##bit, ##ed, from, a, bank, account, so, it, ', s, got, to, be, de, ##bit, ##ed, from, [MASK], ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 90 with text: \n",
            " \t\t[[CLS], it, ', s, at, the, hospital, nearby, ., [SEP], well, that, ', s, a, deal, ., [SEP], yeah, .,\n",
            "\t\t[SEP], yeah, ., [SEP], so, uh, -, huh, ., i, think, %, um, tonight, E, ##5, ##8, ##3, and, i, are,\n",
            "\t\tgoing, to, go, over, and, join, up, ., [SEP], even, E, ##5, ##8, ##3, ', s, going, to, join, ?,\n",
            "\t\t[SEP], no, [SEP], but, it, has, to, be, de, ##bit, ##ed, from, a, bank, account, so, it, ', s, got,\n",
            "\t\tto, be, de, ##bit, ##ed, from, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 87. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] in your letter you said that [MASK] eats constantly . [SEP] [[CLS], yeah, ., [SEP], in, your, letter, you, said, that, [MASK], eats, constantly, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], in, your, letter, you, said, that, [MASK], eats, constantly, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 10. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] oh my god . [SEP] really ? [SEP] E5995 i 've never seen anybody eat as much as [MASK] does [SEP] [[CLS], oh, my, god, ., [SEP], really, ?, [SEP], E, ##5, ##9, ##9, ##5, i, ', ve, never, seen, anybody, eat, as, much, as, [MASK], does, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 27 with text: \n",
            " \t\t[[CLS], oh, my, god, ., [SEP], really, ?, [SEP], E, ##5, ##9, ##9, ##5, i, ', ve, never, seen,\n",
            "\t\tanybody, eat, as, much, as, [MASK], does, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 24. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] 's so skinny . [SEP] [[CLS], and, [MASK], ', s, so, skinny, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], and, [MASK], ', s, so, skinny, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mhm . [SEP] i mean the amount of cereal [MASK] eats like between meals and after meals [SEP] [[CLS], m, ##hm, ., [SEP], i, mean, the, amount, of, cereal, [MASK], eats, like, between, meals, and, after, meals, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], m, ##hm, ., [SEP], i, mean, the, amount, of, cereal, [MASK], eats, like, between, meals,\n",
            "\t\tand, after, meals, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 921it [00:06, 195.75it/s]\u001b[A[CLS] yeah . [SEP] and we go through a box of cereal every two days . [SEP] really ? [SEP] yeah . [SEP] wow . [SEP] [MASK] eats at least like four of cereal a day like in addition to like meals . [SEP] [[CLS], yeah, ., [SEP], and, we, go, through, a, box, of, cereal, every, two, days, ., [SEP], really, ?, [SEP], yeah, ., [SEP], w, ##ow, ., [SEP], [MASK], eats, at, least, like, four, of, cereal, a, day, like, in, addition, to, like, meals, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], and, we, go, through, a, box, of, cereal, every, two, days, ., [SEP],\n",
            "\t\treally, ?, [SEP], yeah, ., [SEP], w, ##ow, ., [SEP], [MASK], eats, at, least, like, four, of,\n",
            "\t\tcereal, a, day, like, in, addition, to, like, meals, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 27. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and oh my god it 's just crazy . [SEP] mhm . [SEP] it 's crazy how much [MASK] eats . [SEP] [[CLS], and, oh, my, god, it, ', s, just, crazy, ., [SEP], m, ##hm, ., [SEP], it, ', s, crazy, how, much, [MASK], eats, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], and, oh, my, god, it, ', s, just, crazy, ., [SEP], m, ##hm, ., [SEP], it, ', s, crazy, how,\n",
            "\t\tmuch, [MASK], eats, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 22. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] i think what we got to do is just when [MASK] comes just have lots of cereal . [SEP] [[CLS], yeah, ., [SEP], i, think, what, we, got, to, do, is, just, when, [MASK], comes, just, have, lots, of, cereal, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 23 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], i, think, what, we, got, to, do, is, just, when, [MASK], comes, just, have,\n",
            "\t\tlots, of, cereal, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] likes cocoa pops . [SEP] [[CLS], [MASK], likes, co, ##coa, pop, ##s, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 9 with text: \n",
            " \t\t[[CLS], [MASK], likes, co, ##coa, pop, ##s, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so [SEP] yeah . [SEP] but %uh 291.01_291.28_b: %hm . [SEP] %hm . [SEP] so i assume it 's hard for you not to eat as much as [MASK] does . [SEP] [[CLS], so, [SEP], yeah, ., [SEP], but, %, uh, 29, ##1, ., 01, _, 29, ##1, ., 28, _, b, :, %, h, ##m, ., [SEP], %, h, ##m, ., [SEP], so, i, assume, it, ', s, hard, for, you, not, to, eat, as, much, as, [MASK], does, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], so, [SEP], yeah, ., [SEP], but, %, uh, 29, ##1, ., 01, _, 29, ##1, ., 28, _, b, :, %, h,\n",
            "\t\t##m, ., [SEP], %, h, ##m, ., [SEP], so, i, assume, it, ', s, hard, for, you, not, to, eat, as, much,\n",
            "\t\tas, [MASK], does, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 46. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no . [SEP] i learned quick fast in a hurry because i 've gained so much weight . [SEP] oh really ? [SEP] yeah . [SEP] actually in the first week i was here -- mhm . -- i gained a lot [SEP] so . [SEP] well it 's hard to you know not eat when somebody else is eating . [SEP] well i 'm figuring it out fast because -- okay -- with not working out and all yeah . it 's pretty miserable my crunch tape . [SEP] %mm . [SEP] so i called this woman today [SEP] and %um [MASK] 's a dressmaker [SEP] [[CLS], no, ., [SEP], i, learned, quick, fast, in, a, hurry, because, i, ', ve, gained, so, much, weight, ., [SEP], oh, really, ?, [SEP], yeah, ., [SEP], actually, in, the, first, week, i, was, here, -, -, m, ##hm, ., -, -, i, gained, a, lot, [SEP], so, ., [SEP], well, it, ', s, hard, to, you, know, not, eat, when, somebody, else, is, eating, ., [SEP], well, i, ', m, figuring, it, out, fast, because, -, -, okay, -, -, with, not, working, out, and, all, yeah, ., it, ', s, pretty, miserable, my, c, ##runch, tape, ., [SEP], %, mm, ., [SEP], so, i, called, this, woman, today, [SEP], and, %, um, [MASK], ', s, a, dress, ##maker, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 122 with text: \n",
            " \t\t[[CLS], no, ., [SEP], i, learned, quick, fast, in, a, hurry, because, i, ', ve, gained, so, much,\n",
            "\t\tweight, ., [SEP], oh, really, ?, [SEP], yeah, ., [SEP], actually, in, the, first, week, i, was,\n",
            "\t\there, -, -, m, ##hm, ., -, -, i, gained, a, lot, [SEP], so, ., [SEP], well, it, ', s, hard, to, you,\n",
            "\t\tknow, not, eat, when, somebody, else, is, eating, ., [SEP], well, i, ', m, figuring, it, out, fast,\n",
            "\t\tbecause, -, -, okay, -, -, with, not, working, out, and, all, yeah, ., it, ', s, pretty, miserable,\n",
            "\t\tmy, c, ##runch, tape, ., [SEP], %, mm, ., [SEP], so, i, called, this, woman, today, [SEP], and, %,\n",
            "\t\tum, [MASK], ', s, a, dress, ##maker, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 115. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so . [SEP] mhm . [SEP] now daddy got %um the invitations together for you today . [SEP] uh-huh . [SEP] and in it i put %um paint samples from E1730 williams uh-huh . of all the different like shades of what color i thought you might be thinking of for the bride 's maids . [SEP] uh-huh . [SEP] oh i already told them all black [SEP] but dad does n't want black [MASK] said . [SEP] [[CLS], so, ., [SEP], m, ##hm, ., [SEP], now, daddy, got, %, um, the, invitation, ##s, together, for, you, today, ., [SEP], uh, -, huh, ., [SEP], and, in, it, i, put, %, um, paint, samples, from, E, ##17, ##30, will, ##iam, ##s, uh, -, huh, ., of, all, the, different, like, shades, of, what, color, i, thought, you, might, be, thinking, of, for, the, bride, ', s, maid, ##s, ., [SEP], uh, -, huh, ., [SEP], oh, i, already, told, them, all, black, [SEP], but, dad, does, n, ', t, want, black, [MASK], said, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 97 with text: \n",
            " \t\t[[CLS], so, ., [SEP], m, ##hm, ., [SEP], now, daddy, got, %, um, the, invitation, ##s, together,\n",
            "\t\tfor, you, today, ., [SEP], uh, -, huh, ., [SEP], and, in, it, i, put, %, um, paint, samples, from,\n",
            "\t\tE, ##17, ##30, will, ##iam, ##s, uh, -, huh, ., of, all, the, different, like, shades, of, what,\n",
            "\t\tcolor, i, thought, you, might, be, thinking, of, for, the, bride, ', s, maid, ##s, ., [SEP], uh, -,\n",
            "\t\thuh, ., [SEP], oh, i, already, told, them, all, black, [SEP], but, dad, does, n, ', t, want, black,\n",
            "\t\t[MASK], said, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 93. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mom and dad wo n't have black . [SEP] they do n't want black ? [SEP] they 're really opposed to it . [SEP] did you call E2217 and tell [MASK] that ? [SEP] [[CLS], mom, and, dad, w, ##o, n, ', t, have, black, ., [SEP], they, do, n, ', t, want, black, ?, [SEP], they, ', re, really, opposed, to, it, ., [SEP], did, you, call, E, ##22, ##17, and, tell, [MASK], that, ?, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 43 with text: \n",
            " \t\t[[CLS], mom, and, dad, w, ##o, n, ', t, have, black, ., [SEP], they, do, n, ', t, want, black, ?,\n",
            "\t\t[SEP], they, ', re, really, opposed, to, it, ., [SEP], did, you, call, E, ##22, ##17, and, tell,\n",
            "\t\t[MASK], that, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] no . [SEP] could you do that ? [SEP] okay . [SEP] could you call E479 and tell [MASK] that ? [SEP] [[CLS], no, ., [SEP], could, you, do, that, ?, [SEP], okay, ., [SEP], could, you, call, E, ##47, ##9, and, tell, [MASK], that, ?, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], no, ., [SEP], could, you, do, that, ?, [SEP], okay, ., [SEP], could, you, call, E, ##47,\n",
            "\t\t##9, and, tell, [MASK], that, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 21. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but %uh [SEP] yeah [SEP] and E3683 still talks about the black dress . [SEP] what ? [SEP] how hideous they were . [SEP] E3683 was n't at E5013 manachi 's . [SEP] E3683 went to the mass wedding . [SEP] [MASK] did ? [SEP] [[CLS], but, %, uh, [SEP], yeah, [SEP], and, E, ##36, ##8, ##3, still, talks, about, the, black, dress, ., [SEP], what, ?, [SEP], how, hideous, they, were, ., [SEP], E, ##36, ##8, ##3, was, n, ', t, at, E, ##50, ##13, man, ##achi, ', s, ., [SEP], E, ##36, ##8, ##3, went, to, the, mass, wedding, ., [SEP], [MASK], did, ?, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], but, %, uh, [SEP], yeah, [SEP], and, E, ##36, ##8, ##3, still, talks, about, the, black,\n",
            "\t\tdress, ., [SEP], what, ?, [SEP], how, hideous, they, were, ., [SEP], E, ##36, ##8, ##3, was, n, ',\n",
            "\t\tt, at, E, ##50, ##13, man, ##achi, ', s, ., [SEP], E, ##36, ##8, ##3, went, to, the, mass, wedding,\n",
            "\t\t., [SEP], [MASK], did, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 58. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and i refused to go [SEP] and [MASK] got mad . [SEP] [[CLS], and, i, refused, to, go, [SEP], and, [MASK], got, mad, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], and, i, refused, to, go, [SEP], and, [MASK], got, mad, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] mhm . [SEP] i think that would be obnoxious [SEP] and i think they would then feel guilty for not inviting me . [SEP] right . [SEP] to -- -- you know the other thing the reception . [SEP] i just would n't go . [SEP] %mm . [SEP] so they had to take it down and drive [MASK] home . [SEP] [[CLS], m, ##hm, ., [SEP], i, think, that, would, be, o, ##b, ##no, ##xious, [SEP], and, i, think, they, would, then, feel, guilty, for, not, inviting, me, ., [SEP], right, ., [SEP], to, -, -, -, -, you, know, the, other, thing, the, reception, ., [SEP], i, just, would, n, ', t, go, ., [SEP], %, mm, ., [SEP], so, they, had, to, take, it, down, and, drive, [MASK], home, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 72 with text: \n",
            " \t\t[[CLS], m, ##hm, ., [SEP], i, think, that, would, be, o, ##b, ##no, ##xious, [SEP], and, i, think,\n",
            "\t\tthey, would, then, feel, guilty, for, not, inviting, me, ., [SEP], right, ., [SEP], to, -, -, -, -,\n",
            "\t\tyou, know, the, other, thing, the, reception, ., [SEP], i, just, would, n, ', t, go, ., [SEP], %,\n",
            "\t\tmm, ., [SEP], so, they, had, to, take, it, down, and, drive, [MASK], home, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 68. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] had to keep reminding herself . [SEP] [[CLS], and, [MASK], had, to, keep, reminding, herself, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], and, [MASK], had, to, keep, reminding, herself, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] %mm . [SEP] so now you think my wedding 's going to be pretty tasteful right ? [SEP] but [SEP] oh yeah . [SEP] now what about the matchbooks ? [SEP] well i 'll work on it . [SEP] i still think about the matchbooks . [SEP] yeah . [SEP] i 'll tell dad that you still think about the [SEP] i have n't forgotten those [SEP] and they 'll still be enough time when i come home for me to order those . [SEP] yeah . [SEP] i 'll have remind [MASK] of that little -- -- item . [SEP] [[CLS], %, mm, ., [SEP], so, now, you, think, my, wedding, ', s, going, to, be, pretty, taste, ##ful, right, ?, [SEP], but, [SEP], oh, yeah, ., [SEP], now, what, about, the, match, ##books, ?, [SEP], well, i, ', ll, work, on, it, ., [SEP], i, still, think, about, the, match, ##books, ., [SEP], yeah, ., [SEP], i, ', ll, tell, dad, that, you, still, think, about, the, [SEP], i, have, n, ', t, forgotten, those, [SEP], and, they, ', ll, still, be, enough, time, when, i, come, home, for, me, to, order, those, ., [SEP], yeah, ., [SEP], i, ', ll, have, remind, [MASK], of, that, little, -, -, -, -, item, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 115 with text: \n",
            " \t\t[[CLS], %, mm, ., [SEP], so, now, you, think, my, wedding, ', s, going, to, be, pretty, taste,\n",
            "\t\t##ful, right, ?, [SEP], but, [SEP], oh, yeah, ., [SEP], now, what, about, the, match, ##books, ?,\n",
            "\t\t[SEP], well, i, ', ll, work, on, it, ., [SEP], i, still, think, about, the, match, ##books, .,\n",
            "\t\t[SEP], yeah, ., [SEP], i, ', ll, tell, dad, that, you, still, think, about, the, [SEP], i, have, n,\n",
            "\t\t', t, forgotten, those, [SEP], and, they, ', ll, still, be, enough, time, when, i, come, home, for,\n",
            "\t\tme, to, order, those, ., [SEP], yeah, ., [SEP], i, ', ll, have, remind, [MASK], of, that, little, -,\n",
            "\t\t-, -, -, item, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 104. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and at [MASK] sister 's wedding %uh they provided champagne and beer . [SEP] [[CLS], and, at, [MASK], sister, ', s, wedding, %, uh, they, provided, champagne, and, beer, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], and, at, [MASK], sister, ', s, wedding, %, uh, they, provided, champagne, and, beer, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but if you wanted any other kind of drink you had to pay for it . [SEP] mhm . [SEP] yeah . [SEP] that 's actually kind of common [SEP] but -- oh it is . it 's pretty de classe . [SEP] well i kind of thou- [SEP] i mean of course i did n't say anything to [MASK] . [SEP] [[CLS], but, if, you, wanted, any, other, kind, of, drink, you, had, to, pay, for, it, ., [SEP], m, ##hm, ., [SEP], yeah, ., [SEP], that, ', s, actually, kind, of, common, [SEP], but, -, -, oh, it, is, ., it, ', s, pretty, de, class, ##e, ., [SEP], well, i, kind, of, thou, -, [SEP], i, mean, of, course, i, did, n, ', t, say, anything, to, [MASK], ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 71 with text: \n",
            " \t\t[[CLS], but, if, you, wanted, any, other, kind, of, drink, you, had, to, pay, for, it, ., [SEP], m,\n",
            "\t\t##hm, ., [SEP], yeah, ., [SEP], that, ', s, actually, kind, of, common, [SEP], but, -, -, oh, it,\n",
            "\t\tis, ., it, ', s, pretty, de, class, ##e, ., [SEP], well, i, kind, of, thou, -, [SEP], i, mean, of,\n",
            "\t\tcourse, i, did, n, ', t, say, anything, to, [MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 68. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i can hardly hear you . [SEP] oh . [SEP] i thought you were talking to somebody else and like put the phone down or something . [SEP] i 'm talking to you . [SEP] oh . [SEP] yeah . [SEP] no dad told me that . [SEP] oh [MASK] did tell you . [SEP] [[CLS], i, can, hardly, hear, you, ., [SEP], oh, ., [SEP], i, thought, you, were, talking, to, somebody, else, and, like, put, the, phone, down, or, something, ., [SEP], i, ', m, talking, to, you, ., [SEP], oh, ., [SEP], yeah, ., [SEP], no, dad, told, me, that, ., [SEP], oh, [MASK], did, tell, you, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 57 with text: \n",
            " \t\t[[CLS], i, can, hardly, hear, you, ., [SEP], oh, ., [SEP], i, thought, you, were, talking, to,\n",
            "\t\tsomebody, else, and, like, put, the, phone, down, or, something, ., [SEP], i, ', m, talking, to,\n",
            "\t\tyou, ., [SEP], oh, ., [SEP], yeah, ., [SEP], no, dad, told, me, that, ., [SEP], oh, [MASK], did,\n",
            "\t\ttell, you, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 51. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's all chilean . [SEP] yeah . [SEP] yeah . [SEP] walnut crest or walnut creek or -- %mm . -- something like that . [SEP] i do n't know . [SEP] %mm . [SEP] there 's a pretty good sale . %mm . so now we have to keep our eyes open for a champagne sale . [SEP] an- [SEP] do you know if E6876 called E4564 yet ? [SEP] i told [MASK] this morning to do it this week . [SEP] [[CLS], it, ', s, all, ch, ##ile, ##an, ., [SEP], yeah, ., [SEP], yeah, ., [SEP], wa, ##ln, ##ut, crest, or, wa, ##ln, ##ut, creek, or, -, -, %, mm, ., -, -, something, like, that, ., [SEP], i, do, n, ', t, know, ., [SEP], %, mm, ., [SEP], there, ', s, a, pretty, good, sale, ., %, mm, ., so, now, we, have, to, keep, our, eyes, open, for, a, champagne, sale, ., [SEP], an, -, [SEP], do, you, know, if, E, ##6, ##8, ##7, ##6, called, E, ##45, ##64, yet, ?, [SEP], i, told, [MASK], this, morning, to, do, it, this, week, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 107 with text: \n",
            " \t\t[[CLS], it, ', s, all, ch, ##ile, ##an, ., [SEP], yeah, ., [SEP], yeah, ., [SEP], wa, ##ln, ##ut,\n",
            "\t\tcrest, or, wa, ##ln, ##ut, creek, or, -, -, %, mm, ., -, -, something, like, that, ., [SEP], i, do,\n",
            "\t\tn, ', t, know, ., [SEP], %, mm, ., [SEP], there, ', s, a, pretty, good, sale, ., %, mm, ., so, now,\n",
            "\t\twe, have, to, keep, our, eyes, open, for, a, champagne, sale, ., [SEP], an, -, [SEP], do, you, know,\n",
            "\t\tif, E, ##6, ##8, ##7, ##6, called, E, ##45, ##64, yet, ?, [SEP], i, told, [MASK], this, morning, to,\n",
            "\t\tdo, it, this, week, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 97. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] uh-huh . [SEP] and [MASK] just keeps forgetting . [SEP] [[CLS], uh, -, huh, ., [SEP], and, [MASK], just, keeps, forgetting, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], uh, -, huh, ., [SEP], and, [MASK], just, keeps, forgetting, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 7. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yeah . [SEP] but o- [SEP] how 's [MASK] doing ? [SEP] [[CLS], yeah, ., [SEP], but, o, -, [SEP], how, ', s, [MASK], doing, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], yeah, ., [SEP], but, o, -, [SEP], how, ', s, [MASK], doing, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 941it [00:06, 194.36it/s]\u001b[A[CLS] much better . [SEP] yeah ? [SEP] yeah . [SEP] [MASK] 's often chipper . [SEP] [[CLS], much, better, ., [SEP], yeah, ?, [SEP], yeah, ., [SEP], [MASK], ', s, often, chip, ##per, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], much, better, ., [SEP], yeah, ?, [SEP], yeah, ., [SEP], [MASK], ', s, often, chip, ##per, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 11. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] went for a walk this morning at five seventeen . [SEP] [[CLS], [MASK], went, for, a, walk, this, morning, at, five, seventeen, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], [MASK], went, for, a, walk, this, morning, at, five, seventeen, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] pe- [SEP] oh my god . [SEP] is that because [MASK] 's not sleeping and [SEP] [[CLS], p, ##e, -, [SEP], oh, my, god, ., [SEP], is, that, because, [MASK], ', s, not, sleeping, and, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], p, ##e, -, [SEP], oh, my, god, ., [SEP], is, that, because, [MASK], ', s, not, sleeping,\n",
            "\t\tand, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 13. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] well because [MASK] could n't breathe because all the cats were in the bedroom . [SEP] [[CLS], well, because, [MASK], could, n, ', t, breathe, because, all, the, cats, were, in, the, bedroom, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], well, because, [MASK], could, n, ', t, breathe, because, all, the, cats, were, in, the,\n",
            "\t\tbedroom, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and my taxes here are fifty percent of income . [SEP] so the taxes here are so much higher that yeah i would never have to worry about it . [SEP] unless you became c e o of the company then maybe like a thousand dollars would be taxable right ? [SEP] yeah . [SEP] somehow it raises a small amount . [SEP] the problem 's going to %uh be E6259 's american salary is taxable here and at fifty percent [SEP] so we 're going to owe some money to the dutch government on [MASK] salary . [SEP] [[CLS], and, my, taxes, here, are, fifty, percent, of, income, ., [SEP], so, the, taxes, here, are, so, much, higher, that, yeah, i, would, never, have, to, worry, about, it, ., [SEP], unless, you, became, c, e, o, of, the, company, then, maybe, like, a, thousand, dollars, would, be, taxa, ##ble, right, ?, [SEP], yeah, ., [SEP], somehow, it, raises, a, small, amount, ., [SEP], the, problem, ', s, going, to, %, uh, be, E, ##6, ##25, ##9, ', s, am, ##eric, ##an, salary, is, taxa, ##ble, here, and, at, fifty, percent, [SEP], so, we, ', re, going, to, owe, some, money, to, the, du, ##tch, government, on, [MASK], salary, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 112 with text: \n",
            " \t\t[[CLS], and, my, taxes, here, are, fifty, percent, of, income, ., [SEP], so, the, taxes, here, are,\n",
            "\t\tso, much, higher, that, yeah, i, would, never, have, to, worry, about, it, ., [SEP], unless, you,\n",
            "\t\tbecame, c, e, o, of, the, company, then, maybe, like, a, thousand, dollars, would, be, taxa, ##ble,\n",
            "\t\tright, ?, [SEP], yeah, ., [SEP], somehow, it, raises, a, small, amount, ., [SEP], the, problem, ',\n",
            "\t\ts, going, to, %, uh, be, E, ##6, ##25, ##9, ', s, am, ##eric, ##an, salary, is, taxa, ##ble, here,\n",
            "\t\tand, at, fifty, percent, [SEP], so, we, ', re, going, to, owe, some, money, to, the, du, ##tch,\n",
            "\t\tgovernment, on, [MASK], salary, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 108. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] they do n't have the quicken dutch version ? [SEP] yeah . [SEP] %ah you know i would n't trust it because it 's so complicated with this international garbage or else the turbo tax with the dutch modules . that i just would n't trust it . [SEP] i do n't know %eh . [SEP] yeah . [SEP] so complicated when you 're dealing with a dutch tax form . [SEP] E6259 had to read it [SEP] and E6259 and [MASK] father did n't understand the damn thing . [SEP] [[CLS], they, do, n, ', t, have, the, quick, ##en, du, ##tch, version, ?, [SEP], yeah, ., [SEP], %, ah, you, know, i, would, n, ', t, trust, it, because, it, ', s, so, complicated, with, this, international, garbage, or, else, the, t, ##ur, ##bo, tax, with, the, du, ##tch, modules, ., that, i, just, would, n, ', t, trust, it, ., [SEP], i, do, n, ', t, know, %, eh, ., [SEP], yeah, ., [SEP], so, complicated, when, you, ', re, dealing, with, a, du, ##tch, tax, form, ., [SEP], E, ##6, ##25, ##9, had, to, read, it, [SEP], and, E, ##6, ##25, ##9, and, [MASK], father, did, n, ', t, understand, the, damn, thing, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 118 with text: \n",
            " \t\t[[CLS], they, do, n, ', t, have, the, quick, ##en, du, ##tch, version, ?, [SEP], yeah, ., [SEP], %,\n",
            "\t\tah, you, know, i, would, n, ', t, trust, it, because, it, ', s, so, complicated, with, this,\n",
            "\t\tinternational, garbage, or, else, the, t, ##ur, ##bo, tax, with, the, du, ##tch, modules, ., that,\n",
            "\t\ti, just, would, n, ', t, trust, it, ., [SEP], i, do, n, ', t, know, %, eh, ., [SEP], yeah, ., [SEP],\n",
            "\t\tso, complicated, when, you, ', re, dealing, with, a, du, ##tch, tax, form, ., [SEP], E, ##6, ##25,\n",
            "\t\t##9, had, to, read, it, [SEP], and, E, ##6, ##25, ##9, and, [MASK], father, did, n, ', t,\n",
            "\t\tunderstand, the, damn, thing, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 106. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] if it 's during the work hours fine call me at work . [SEP] if not call E6259 's parents if you do n't have a new phone number for me . [SEP] and if you do i 'll send it by email okay if i get it before this weekend [SEP] we 'll email you something [SEP] but if you do n't get email then just call E6259 's parents [SEP] if we have a phone by then they 'll be able to give you the number [SEP] okay [SEP] and just remember to ask for [MASK] mother . [SEP] [[CLS], if, it, ', s, during, the, work, hours, fine, call, me, at, work, ., [SEP], if, not, call, E, ##6, ##25, ##9, ', s, parents, if, you, do, n, ', t, have, a, new, phone, number, for, me, ., [SEP], and, if, you, do, i, ', ll, send, it, by, email, okay, if, i, get, it, before, this, weekend, [SEP], we, ', ll, email, you, something, [SEP], but, if, you, do, n, ', t, get, email, then, just, call, E, ##6, ##25, ##9, ', s, parents, [SEP], if, we, have, a, phone, by, then, they, ', ll, be, able, to, give, you, the, number, [SEP], okay, [SEP], and, just, remember, to, ask, for, [MASK], mother, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 118 with text: \n",
            " \t\t[[CLS], if, it, ', s, during, the, work, hours, fine, call, me, at, work, ., [SEP], if, not, call,\n",
            "\t\tE, ##6, ##25, ##9, ', s, parents, if, you, do, n, ', t, have, a, new, phone, number, for, me, .,\n",
            "\t\t[SEP], and, if, you, do, i, ', ll, send, it, by, email, okay, if, i, get, it, before, this, weekend,\n",
            "\t\t[SEP], we, ', ll, email, you, something, [SEP], but, if, you, do, n, ', t, get, email, then, just,\n",
            "\t\tcall, E, ##6, ##25, ##9, ', s, parents, [SEP], if, we, have, a, phone, by, then, they, ', ll, be,\n",
            "\t\table, to, give, you, the, number, [SEP], okay, [SEP], and, just, remember, to, ask, for, [MASK],\n",
            "\t\tmother, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 114. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and it 's goode right [SEP] bode [SEP] bode [SEP] bode as in abode [SEP] abode [SEP] minus the a [SEP] okay [SEP] great [SEP] and [SEP] yeah [SEP] and i think %uh %uh it 's not frau [SEP] %uh %ah %uh it 's mifrau [SEP] is [SEP] it 's frau in german [SEP] it 's frau [SEP] mhm [SEP] and then dutch it 's mifrau [SEP] i 'll probably never remember that [SEP] so hope they wo n't be offended . [SEP] nah do n't worry about it . [SEP] you can just call [MASK] missus [SEP] [[CLS], and, it, ', s, good, ##e, right, [SEP], b, ##ode, [SEP], b, ##ode, [SEP], b, ##ode, as, in, a, ##bo, ##de, [SEP], a, ##bo, ##de, [SEP], minus, the, a, [SEP], okay, [SEP], great, [SEP], and, [SEP], yeah, [SEP], and, i, think, %, uh, %, uh, it, ', s, not, f, ##ra, ##u, [SEP], %, uh, %, ah, %, uh, it, ', s, mi, ##fra, ##u, [SEP], is, [SEP], it, ', s, f, ##ra, ##u, in, g, ##erman, [SEP], it, ', s, f, ##ra, ##u, [SEP], m, ##hm, [SEP], and, then, du, ##tch, it, ', s, mi, ##fra, ##u, [SEP], i, ', ll, probably, never, remember, that, [SEP], so, hope, they, w, ##o, n, ', t, be, offended, ., [SEP], na, ##h, do, n, ', t, worry, about, it, ., [SEP], you, can, just, call, [MASK], miss, ##us, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 139 with text: \n",
            " \t\t[[CLS], and, it, ', s, good, ##e, right, [SEP], b, ##ode, [SEP], b, ##ode, [SEP], b, ##ode, as, in,\n",
            "\t\ta, ##bo, ##de, [SEP], a, ##bo, ##de, [SEP], minus, the, a, [SEP], okay, [SEP], great, [SEP], and,\n",
            "\t\t[SEP], yeah, [SEP], and, i, think, %, uh, %, uh, it, ', s, not, f, ##ra, ##u, [SEP], %, uh, %, ah,\n",
            "\t\t%, uh, it, ', s, mi, ##fra, ##u, [SEP], is, [SEP], it, ', s, f, ##ra, ##u, in, g, ##erman, [SEP],\n",
            "\t\tit, ', s, f, ##ra, ##u, [SEP], m, ##hm, [SEP], and, then, du, ##tch, it, ', s, mi, ##fra, ##u,\n",
            "\t\t[SEP], i, ', ll, probably, never, remember, that, [SEP], so, hope, they, w, ##o, n, ', t, be,\n",
            "\t\toffended, ., [SEP], na, ##h, do, n, ', t, worry, about, it, ., [SEP], you, can, just, call, [MASK],\n",
            "\t\tmiss, ##us, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 135. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] understands [SEP] [[CLS], [MASK], understands, [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 4 with text: \n",
            " \t\t[[CLS], [MASK], understands, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i mean the phones here are pretty good . once you 're in the country . [SEP] it 's just these to america [SEP] it 's always kind of iffy [SEP] the hot dialogue ... that went on between -lsb- -lsb- -lsb- E2809 E3964 and rumsfeld -rsb- -rsb- -rsb- at the airport prison . [SEP] collaborator [SEP] details of the minutes of the meeting between rumsfeld and E2809 E3964 at the airport prison ... [SEP] E4477 offers E2809 a television appearance to stop the resistance in return for [MASK] freedom . [SEP] [[CLS], i, mean, the, phones, here, are, pretty, good, ., once, you, ', re, in, the, country, ., [SEP], it, ', s, just, these, to, am, ##eric, ##a, [SEP], it, ', s, always, kind, of, if, ##fy, [SEP], the, hot, dialogue, ., ., ., that, went, on, between, -, l, ##s, ##b, -, -, l, ##s, ##b, -, -, l, ##s, ##b, -, E, ##28, ##0, ##9, E, ##39, ##64, and, r, ##ums, ##feld, -, r, ##s, ##b, -, -, r, ##s, ##b, -, -, r, ##s, ##b, -, at, the, airport, prison, ., [SEP], collaborator, [SEP], details, of, the, minutes, of, the, meeting, between, r, ##ums, ##feld, and, E, ##28, ##0, ##9, E, ##39, ##64, at, the, airport, prison, ., ., ., [SEP], E, ##44, ##7, ##7, offers, E, ##28, ##0, ##9, a, television, appearance, to, stop, the, resistance, in, return, for, [MASK], freedom, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 147 with text: \n",
            " \t\t[[CLS], i, mean, the, phones, here, are, pretty, good, ., once, you, ', re, in, the, country, .,\n",
            "\t\t[SEP], it, ', s, just, these, to, am, ##eric, ##a, [SEP], it, ', s, always, kind, of, if, ##fy,\n",
            "\t\t[SEP], the, hot, dialogue, ., ., ., that, went, on, between, -, l, ##s, ##b, -, -, l, ##s, ##b, -,\n",
            "\t\t-, l, ##s, ##b, -, E, ##28, ##0, ##9, E, ##39, ##64, and, r, ##ums, ##feld, -, r, ##s, ##b, -, -, r,\n",
            "\t\t##s, ##b, -, -, r, ##s, ##b, -, at, the, airport, prison, ., [SEP], collaborator, [SEP], details,\n",
            "\t\tof, the, minutes, of, the, meeting, between, r, ##ums, ##feld, and, E, ##28, ##0, ##9, E, ##39,\n",
            "\t\t##64, at, the, airport, prison, ., ., ., [SEP], E, ##44, ##7, ##7, offers, E, ##28, ##0, ##9, a,\n",
            "\t\ttelevision, appearance, to, stop, the, resistance, in, return, for, [MASK], freedom, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 143. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you have assaulted the sovereignty of an independent free sovereign country [SEP] you have committed crimes which history will record against you as evidence of your blood - stained civilization . [SEP] so what do you want after all that ? [SEP] rumsfeld -lrb- trying to suppress [MASK] anger -rrb- : what 's past is past i came especially to make you a clear and specific offer and i want to hear from you a clear and definite answer . [SEP] [[CLS], you, have, assaulted, the, sovereignty, of, an, independent, free, sovereign, country, [SEP], you, have, committed, crimes, which, history, will, record, against, you, as, evidence, of, your, blood, -, stained, civilization, ., [SEP], so, what, do, you, want, after, all, that, ?, [SEP], r, ##ums, ##feld, -, l, ##rb, -, trying, to, suppress, [MASK], anger, -, r, ##rb, -, :, what, ', s, past, is, past, i, came, especially, to, make, you, a, clear, and, specific, offer, and, i, want, to, hear, from, you, a, clear, and, definite, answer, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 91 with text: \n",
            " \t\t[[CLS], you, have, assaulted, the, sovereignty, of, an, independent, free, sovereign, country,\n",
            "\t\t[SEP], you, have, committed, crimes, which, history, will, record, against, you, as, evidence, of,\n",
            "\t\tyour, blood, -, stained, civilization, ., [SEP], so, what, do, you, want, after, all, that, ?,\n",
            "\t\t[SEP], r, ##ums, ##feld, -, l, ##rb, -, trying, to, suppress, [MASK], anger, -, r, ##rb, -, :, what,\n",
            "\t\t', s, past, is, past, i, came, especially, to, make, you, a, clear, and, specific, offer, and, i,\n",
            "\t\twant, to, hear, from, you, a, clear, and, definite, answer, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 53. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] great iraq is being ruled by talabani and jaafari . [SEP] is n't this ridiculous ? [SEP] then which elections are you talking about ? [SEP] is it permissible for what you call free elections to be held under your occupation of our country ? [SEP] mr rumsfeld we have learned from history that the occupier will not bring except [MASK] assistants and lackeys and you want after all that to convince me that the people of iraq enjoy freedom and democracy ? [SEP] [[CLS], great, i, ##ra, ##q, is, being, ruled, by, ta, ##la, ##ban, ##i, and, j, ##aa, ##fari, ., [SEP], is, n, ', t, this, ridiculous, ?, [SEP], then, which, elections, are, you, talking, about, ?, [SEP], is, it, per, ##missible, for, what, you, call, free, elections, to, be, held, under, your, occupation, of, our, country, ?, [SEP], m, ##r, r, ##ums, ##feld, we, have, learned, from, history, that, the, o, ##cc, ##up, ##ier, will, not, bring, except, [MASK], assistants, and, lack, ##ey, ##s, and, you, want, after, all, that, to, convince, me, that, the, people, of, i, ##ra, ##q, enjoy, freedom, and, democracy, ?, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], great, i, ##ra, ##q, is, being, ruled, by, ta, ##la, ##ban, ##i, and, j, ##aa, ##fari, .,\n",
            "\t\t[SEP], is, n, ', t, this, ridiculous, ?, [SEP], then, which, elections, are, you, talking, about, ?,\n",
            "\t\t[SEP], is, it, per, ##missible, for, what, you, call, free, elections, to, be, held, under, your,\n",
            "\t\toccupation, of, our, country, ?, [SEP], m, ##r, r, ##ums, ##feld, we, have, learned, from, history,\n",
            "\t\tthat, the, o, ##cc, ##up, ##ier, will, not, bring, except, [MASK], assistants, and, lack, ##ey, ##s,\n",
            "\t\tand, you, want, after, all, that, to, convince, me, that, the, people, of, i, ##ra, ##q, enjoy,\n",
            "\t\tfreedom, and, democracy, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 77. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you are truly delusional . [SEP] rumsfeld -lrb- barely able to contain [MASK] anger -rrb- : you are isolated and do not know the facts of what is happening on the outside . [SEP] [[CLS], you, are, truly, del, ##usion, ##al, ., [SEP], r, ##ums, ##feld, -, l, ##rb, -, barely, able, to, contain, [MASK], anger, -, r, ##rb, -, :, you, are, isolated, and, do, not, know, the, facts, of, what, is, happening, on, the, outside, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], you, are, truly, del, ##usion, ##al, ., [SEP], r, ##ums, ##feld, -, l, ##rb, -, barely,\n",
            "\t\table, to, contain, [MASK], anger, -, r, ##rb, -, :, you, are, isolated, and, do, not, know, the,\n",
            "\t\tfacts, of, what, is, happening, on, the, outside, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 20. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] death surrounds them everywhere and history will not be merciful to [MASK] . [SEP] [[CLS], death, surrounds, them, everywhere, and, history, will, not, be, me, ##rc, ##iful, to, [MASK], ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], death, surrounds, them, everywhere, and, history, will, not, be, me, ##rc, ##iful, to,\n",
            "\t\t[MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 14. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E2809 E3964 : have you obtained the consent of your president to this offer ? [SEP] rumsfeld : yes this offer was agreed to at a session in which the president [MASK] deputy the secretary of state and the head of the cia took part and i was commissioned to inform you of this offer . [SEP] [[CLS], E, ##28, ##0, ##9, E, ##39, ##64, :, have, you, obtained, the, consent, of, your, president, to, this, offer, ?, [SEP], r, ##ums, ##feld, :, yes, this, offer, was, agreed, to, at, a, session, in, which, the, president, [MASK], deputy, the, secretary, of, state, and, the, head, of, the, c, ##ia, took, part, and, i, was, commissioned, to, inform, you, of, this, offer, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 66 with text: \n",
            " \t\t[[CLS], E, ##28, ##0, ##9, E, ##39, ##64, :, have, you, obtained, the, consent, of, your, president,\n",
            "\t\tto, this, offer, ?, [SEP], r, ##ums, ##feld, :, yes, this, offer, was, agreed, to, at, a, session,\n",
            "\t\tin, which, the, president, [MASK], deputy, the, secretary, of, state, and, the, head, of, the, c,\n",
            "\t\t##ia, took, part, and, i, was, commissioned, to, inform, you, of, this, offer, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i will accept the assistance of an international and arab committee in estimating these losses . [SEP] fourthly i ask that you produce the money which your men looted from the treasuries of iraq and its oil especially this criminal bremmer and [MASK] henchmen from the traitors and renegades . [SEP] [[CLS], i, will, accept, the, assistance, of, an, international, and, a, ##rab, committee, in, est, ##imating, these, losses, ., [SEP], fourth, ##ly, i, ask, that, you, produce, the, money, which, your, men, lo, ##oted, from, the, t, ##rea, ##su, ##ries, of, i, ##ra, ##q, and, its, oil, especially, this, criminal, br, ##em, ##mer, and, [MASK], he, ##nch, ##men, from, the, traitor, ##s, and, re, ##ne, ##gade, ##s, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 69 with text: \n",
            " \t\t[[CLS], i, will, accept, the, assistance, of, an, international, and, a, ##rab, committee, in, est,\n",
            "\t\t##imating, these, losses, ., [SEP], fourth, ##ly, i, ask, that, you, produce, the, money, which,\n",
            "\t\tyour, men, lo, ##oted, from, the, t, ##rea, ##su, ##ries, of, i, ##ra, ##q, and, its, oil,\n",
            "\t\tespecially, this, criminal, br, ##em, ##mer, and, [MASK], he, ##nch, ##men, from, the, traitor, ##s,\n",
            "\t\tand, re, ##ne, ##gade, ##s, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 54. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E2809 E3964 : you would be successful if you would tell the truth . [SEP] i know the reality of your goals . [SEP] if you were really sincere you would begin straightaway you and your allies to withdraw from iraq and you also have to back down from your position of supporting E7074 . [SEP] i know that your president is stubborn and obstinate and not sincere . [SEP] rumsfeld : [MASK] is a democratically elected president and not a bloodthirsty ruler like you . [SEP] [[CLS], E, ##28, ##0, ##9, E, ##39, ##64, :, you, would, be, successful, if, you, would, tell, the, truth, ., [SEP], i, know, the, reality, of, your, goals, ., [SEP], if, you, were, really, sincere, you, would, begin, straight, ##away, you, and, your, allies, to, withdraw, from, i, ##ra, ##q, and, you, also, have, to, back, down, from, your, position, of, supporting, E, ##70, ##7, ##4, ., [SEP], i, know, that, your, president, is, stubborn, and, o, ##bs, ##tina, ##te, and, not, sincere, ., [SEP], r, ##ums, ##feld, :, [MASK], is, a, democratic, ##ally, elected, president, and, not, a, blood, ##thi, ##rst, ##y, ruler, like, you, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 108 with text: \n",
            " \t\t[[CLS], E, ##28, ##0, ##9, E, ##39, ##64, :, you, would, be, successful, if, you, would, tell, the,\n",
            "\t\ttruth, ., [SEP], i, know, the, reality, of, your, goals, ., [SEP], if, you, were, really, sincere,\n",
            "\t\tyou, would, begin, straight, ##away, you, and, your, allies, to, withdraw, from, i, ##ra, ##q, and,\n",
            "\t\tyou, also, have, to, back, down, from, your, position, of, supporting, E, ##70, ##7, ##4, ., [SEP],\n",
            "\t\ti, know, that, your, president, is, stubborn, and, o, ##bs, ##tina, ##te, and, not, sincere, .,\n",
            "\t\t[SEP], r, ##ums, ##feld, :, [MASK], is, a, democratic, ##ally, elected, president, and, not, a,\n",
            "\t\tblood, ##thi, ##rst, ##y, ruler, like, you, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 89. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i refused and i repeat the refusal once again . [SEP] rumsfeld : i do not want a refusal from you i want you to think about it . [SEP] we are currently re-evaluating our positions we want to stop the bloodshed on both sides and so our offer is made from the point of strength and not from the point of weakness . [SEP] we asked jalal talabani to make a statement in which [MASK] denies any intention to execute you as a sign of good will from us . [SEP] [[CLS], i, refused, and, i, repeat, the, refusal, once, again, ., [SEP], r, ##ums, ##feld, :, i, do, not, want, a, refusal, from, you, i, want, you, to, think, about, it, ., [SEP], we, are, currently, re, -, evaluating, our, positions, we, want, to, stop, the, blood, ##shed, on, both, sides, and, so, our, offer, is, made, from, the, point, of, strength, and, not, from, the, point, of, weakness, ., [SEP], we, asked, j, ##ala, ##l, ta, ##la, ##ban, ##i, to, make, a, statement, in, which, [MASK], denies, any, intention, to, execute, you, as, a, sign, of, good, will, from, us, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], i, refused, and, i, repeat, the, refusal, once, again, ., [SEP], r, ##ums, ##feld, :, i, do,\n",
            "\t\tnot, want, a, refusal, from, you, i, want, you, to, think, about, it, ., [SEP], we, are, currently,\n",
            "\t\tre, -, evaluating, our, positions, we, want, to, stop, the, blood, ##shed, on, both, sides, and, so,\n",
            "\t\tour, offer, is, made, from, the, point, of, strength, and, not, from, the, point, of, weakness, .,\n",
            "\t\t[SEP], we, asked, j, ##ala, ##l, ta, ##la, ##ban, ##i, to, make, a, statement, in, which, [MASK],\n",
            "\t\tdenies, any, intention, to, execute, you, as, a, sign, of, good, will, from, us, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 86. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] we will get our honor back whether E2809 E3964 survives or becomes a martyr . [SEP] rumsfeld : your followers with whom we entered into a dialogue have told us that the decisions will be yours first and last . [SEP] were they expecting your reaction ? [SEP] E2809 E3964 : for sure they know that E2809 hussain can not back down at the expense of [MASK] nation and its honor . [SEP] [[CLS], we, will, get, our, honor, back, whether, E, ##28, ##0, ##9, E, ##39, ##64, survives, or, becomes, a, martyr, ., [SEP], r, ##ums, ##feld, :, your, followers, with, whom, we, entered, into, a, dialogue, have, told, us, that, the, decisions, will, be, yours, first, and, last, ., [SEP], were, they, expecting, your, reaction, ?, [SEP], E, ##28, ##0, ##9, E, ##39, ##64, :, for, sure, they, know, that, E, ##28, ##0, ##9, h, ##uss, ##ain, can, not, back, down, at, the, expense, of, [MASK], nation, and, its, honor, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 91 with text: \n",
            " \t\t[[CLS], we, will, get, our, honor, back, whether, E, ##28, ##0, ##9, E, ##39, ##64, survives, or,\n",
            "\t\tbecomes, a, martyr, ., [SEP], r, ##ums, ##feld, :, your, followers, with, whom, we, entered, into,\n",
            "\t\ta, dialogue, have, told, us, that, the, decisions, will, be, yours, first, and, last, ., [SEP],\n",
            "\t\twere, they, expecting, your, reaction, ?, [SEP], E, ##28, ##0, ##9, E, ##39, ##64, :, for, sure,\n",
            "\t\tthey, know, that, E, ##28, ##0, ##9, h, ##uss, ##ain, can, not, back, down, at, the, expense, of,\n",
            "\t\t[MASK], nation, and, its, honor, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 84. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the amateur [SEP] rumsfeld : we have nothing to apologize for . [SEP] you were a danger to your neighbors and tried to acquire weapons of mass destruction . [SEP] and you practiced dictatorship over your people . [SEP] and so it was natural that we extended our hand to the people of iraq to rid them of the perils which had confronted them for more than three decades . [SEP] and did rumsfeld free iraq from dangers ? [SEP] [MASK] is really delusional ... [SEP] [[CLS], the, amateur, [SEP], r, ##ums, ##feld, :, we, have, nothing, to, apologize, for, ., [SEP], you, were, a, danger, to, your, neighbors, and, tried, to, acquire, weapons, of, mass, destruction, ., [SEP], and, you, practiced, dictatorship, over, your, people, ., [SEP], and, so, it, was, natural, that, we, extended, our, hand, to, the, people, of, i, ##ra, ##q, to, rid, them, of, the, per, ##ils, which, had, confronted, them, for, more, than, three, decades, ., [SEP], and, did, r, ##ums, ##feld, free, i, ##ra, ##q, from, dangers, ?, [SEP], [MASK], is, really, del, ##usion, ##al, ., ., ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], the, amateur, [SEP], r, ##ums, ##feld, :, we, have, nothing, to, apologize, for, ., [SEP],\n",
            "\t\tyou, were, a, danger, to, your, neighbors, and, tried, to, acquire, weapons, of, mass, destruction,\n",
            "\t\t., [SEP], and, you, practiced, dictatorship, over, your, people, ., [SEP], and, so, it, was,\n",
            "\t\tnatural, that, we, extended, our, hand, to, the, people, of, i, ##ra, ##q, to, rid, them, of, the,\n",
            "\t\tper, ##ils, which, had, confronted, them, for, more, than, three, decades, ., [SEP], and, did, r,\n",
            "\t\t##ums, ##feld, free, i, ##ra, ##q, from, dangers, ?, [SEP], [MASK], is, really, del, ##usion, ##al,\n",
            "\t\t., ., ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 90. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 961it [00:06, 151.00it/s]\u001b[A[CLS] motaawen@hotmail.com [SEP] qais the E5875 of [MASK] father [SEP] [[CLS], m, ##ota, ##aw, ##en, @, hot, ##mail, ., com, [SEP], q, ##ais, the, E, ##5, ##8, ##75, of, [MASK], father, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], m, ##ota, ##aw, ##en, @, hot, ##mail, ., com, [SEP], q, ##ais, the, E, ##5, ##8, ##75, of,\n",
            "\t\t[MASK], father, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] disdained death and was not bothered by it challenging the provocations of the imperialist donkeys with full self - control repeating the words there is no god but god and E145 is the messenger of god . [SEP] [[CLS], [MASK], di, ##s, ##dain, ##ed, death, and, was, not, bothered, by, it, challenging, the, pro, ##vocation, ##s, of, the, imperial, ##ist, don, ##key, ##s, with, full, self, -, control, repeating, the, words, there, is, no, god, but, god, and, E, ##14, ##5, is, the, messenger, of, god, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], [MASK], di, ##s, ##dain, ##ed, death, and, was, not, bothered, by, it, challenging, the,\n",
            "\t\tpro, ##vocation, ##s, of, the, imperial, ##ist, don, ##key, ##s, with, full, self, -, control,\n",
            "\t\trepeating, the, words, there, is, no, god, but, god, and, E, ##14, ##5, is, the, messenger, of, god,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] you extended your hands towards them in welcome **** just as you extended them towards them with gifts . [SEP] when the bowels of the earth became too tight to - **** confine your exaltedness after death they made the air your tomb and they exchanged - **** for the shrouds the gown of the sandy winds [SEP] for your greatness in our souls grows and is tended - **** by guards and trustworthy confidents [SEP] E4615 the compassionate rain [MASK] blessings on you - ***** with mercies passing to and fro [SEP] [[CLS], you, extended, your, hands, towards, them, in, welcome, *, *, *, *, just, as, you, extended, them, towards, them, with, gifts, ., [SEP], when, the, bow, ##els, of, the, earth, became, too, tight, to, -, *, *, *, *, con, ##fine, your, ex, ##al, ##ted, ##ness, after, death, they, made, the, air, your, tomb, and, they, exchanged, -, *, *, *, *, for, the, s, ##hr, ##oud, ##s, the, gown, of, the, sandy, winds, [SEP], for, your, great, ##ness, in, our, souls, grows, and, is, tended, -, *, *, *, *, by, guards, and, trust, ##worthy, confident, ##s, [SEP], E, ##46, ##15, the, compassion, ##ate, rain, [MASK], blessing, ##s, on, you, -, *, *, *, *, *, with, me, ##rc, ##ies, passing, to, and, f, ##ro, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 128 with text: \n",
            " \t\t[[CLS], you, extended, your, hands, towards, them, in, welcome, *, *, *, *, just, as, you, extended,\n",
            "\t\tthem, towards, them, with, gifts, ., [SEP], when, the, bow, ##els, of, the, earth, became, too,\n",
            "\t\ttight, to, -, *, *, *, *, con, ##fine, your, ex, ##al, ##ted, ##ness, after, death, they, made, the,\n",
            "\t\tair, your, tomb, and, they, exchanged, -, *, *, *, *, for, the, s, ##hr, ##oud, ##s, the, gown, of,\n",
            "\t\tthe, sandy, winds, [SEP], for, your, great, ##ness, in, our, souls, grows, and, is, tended, -, *, *,\n",
            "\t\t*, *, by, guards, and, trust, ##worthy, confident, ##s, [SEP], E, ##46, ##15, the, compassion,\n",
            "\t\t##ate, rain, [MASK], blessing, ##s, on, you, -, *, *, *, *, *, with, me, ##rc, ##ies, passing, to,\n",
            "\t\tand, f, ##ro, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 107. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the emirati [SEP] E2809 E3964 : i have nothing left but honor and honor can not be bought and sold . [SEP] the collaborator [SEP] my brother qais the E5875 of [MASK] father i thank you for your post and the addition may god take care of you . [SEP] [[CLS], the, em, ##ira, ##ti, [SEP], E, ##28, ##0, ##9, E, ##39, ##64, :, i, have, nothing, left, but, honor, and, honor, can, not, be, bought, and, sold, ., [SEP], the, collaborator, [SEP], my, brother, q, ##ais, the, E, ##5, ##8, ##75, of, [MASK], father, i, thank, you, for, your, post, and, the, addition, may, god, take, care, of, you, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], the, em, ##ira, ##ti, [SEP], E, ##28, ##0, ##9, E, ##39, ##64, :, i, have, nothing, left,\n",
            "\t\tbut, honor, and, honor, can, not, be, bought, and, sold, ., [SEP], the, collaborator, [SEP], my,\n",
            "\t\tbrother, q, ##ais, the, E, ##5, ##8, ##75, of, [MASK], father, i, thank, you, for, your, post, and,\n",
            "\t\tthe, addition, may, god, take, care, of, you, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] motaawen@hotmail.com [SEP] E2809 's last days and what is said [MASK] was like [SEP] [[CLS], m, ##ota, ##aw, ##en, @, hot, ##mail, ., com, [SEP], E, ##28, ##0, ##9, ', s, last, days, and, what, is, said, [MASK], was, like, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 27 with text: \n",
            " \t\t[[CLS], m, ##ota, ##aw, ##en, @, hot, ##mail, ., com, [SEP], E, ##28, ##0, ##9, ', s, last, days,\n",
            "\t\tand, what, is, said, [MASK], was, like, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] bu thari [SEP] what we know about E2809 E3964 in recent years is that [MASK] had changed a lot . [SEP] [[CLS], b, ##u, th, ##ari, [SEP], what, we, know, about, E, ##28, ##0, ##9, E, ##39, ##64, in, recent, years, is, that, [MASK], had, changed, a, lot, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], b, ##u, th, ##ari, [SEP], what, we, know, about, E, ##28, ##0, ##9, E, ##39, ##64, in,\n",
            "\t\trecent, years, is, that, [MASK], had, changed, a, lot, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 22. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] had islamic tendecies and acts which many iraqis mentioned saying that E2809 supported to a great extent the building of mosques centers for memorizing koran and charities and the dancer 's punishment was death . [SEP] [[CLS], [MASK], had, is, ##lam, ##ic, tend, ##ec, ##ies, and, acts, which, many, i, ##ra, ##qi, ##s, mentioned, saying, that, E, ##28, ##0, ##9, supported, to, a, great, extent, the, building, of, mosques, centers, for, me, ##mor, ##izing, k, ##oran, and, charities, and, the, dancer, ', s, punishment, was, death, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 52 with text: \n",
            " \t\t[[CLS], [MASK], had, is, ##lam, ##ic, tend, ##ec, ##ies, and, acts, which, many, i, ##ra, ##qi, ##s,\n",
            "\t\tmentioned, saying, that, E, ##28, ##0, ##9, supported, to, a, great, extent, the, building, of,\n",
            "\t\tmosques, centers, for, me, ##mor, ##izing, k, ##oran, and, charities, and, the, dancer, ', s,\n",
            "\t\tpunishment, was, death, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] had many works . [SEP] [[CLS], [MASK], had, many, works, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 7 with text: \n",
            " \t\t[[CLS], [MASK], had, many, works, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i have followed E2809 E3964 especially in recent years and i noticed about [MASK] was that there was religion but there was injustice and coarseness . [SEP] [[CLS], i, have, followed, E, ##28, ##0, ##9, E, ##39, ##64, especially, in, recent, years, and, i, noticed, about, [MASK], was, that, there, was, religion, but, there, was, injustice, and, coarse, ##ness, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], i, have, followed, E, ##28, ##0, ##9, E, ##39, ##64, especially, in, recent, years, and, i,\n",
            "\t\tnoticed, about, [MASK], was, that, there, was, religion, but, there, was, injustice, and, coarse,\n",
            "\t\t##ness, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] punishment was unjust . [SEP] [[CLS], and, [MASK], punishment, was, un, ##ju, ##st, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 10 with text: \n",
            " \t\t[[CLS], and, [MASK], punishment, was, un, ##ju, ##st, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i mean for example the dancer would be killed after second offence . [SEP] i did not expect anyone to kill the dancer unless good would come of it but like the most idiots [MASK] aim was correct but the method was wrong . [SEP] [[CLS], i, mean, for, example, the, dancer, would, be, killed, after, second, offence, ., [SEP], i, did, not, expect, anyone, to, kill, the, dancer, unless, good, would, come, of, it, but, like, the, most, idiot, ##s, [MASK], aim, was, correct, but, the, method, was, wrong, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], i, mean, for, example, the, dancer, would, be, killed, after, second, offence, ., [SEP], i,\n",
            "\t\tdid, not, expect, anyone, to, kill, the, dancer, unless, good, would, come, of, it, but, like, the,\n",
            "\t\tmost, idiot, ##s, [MASK], aim, was, correct, but, the, method, was, wrong, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but [MASK] was not a scholar nor feared in god . [SEP] [[CLS], but, [MASK], was, not, a, scholar, nor, feared, in, god, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 13 with text: \n",
            " \t\t[[CLS], but, [MASK], was, not, a, scholar, nor, feared, in, god, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i myself heard from some people who were in iraq about the things that E2809 E3964 used to do and could not be reasonably done by anyone other than a good moslem to the extent that many said of [MASK] actions that they were only to gain favor amongst the sunnis and the religious . [SEP] [[CLS], i, myself, heard, from, some, people, who, were, in, i, ##ra, ##q, about, the, things, that, E, ##28, ##0, ##9, E, ##39, ##64, used, to, do, and, could, not, be, reasonably, done, by, anyone, other, than, a, good, m, ##os, ##lem, to, the, extent, that, many, said, of, [MASK], actions, that, they, were, only, to, gain, favor, amongst, the, sun, ##nis, and, the, religious, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 67 with text: \n",
            " \t\t[[CLS], i, myself, heard, from, some, people, who, were, in, i, ##ra, ##q, about, the, things, that,\n",
            "\t\tE, ##28, ##0, ##9, E, ##39, ##64, used, to, do, and, could, not, be, reasonably, done, by, anyone,\n",
            "\t\tother, than, a, good, m, ##os, ##lem, to, the, extent, that, many, said, of, [MASK], actions, that,\n",
            "\t\tthey, were, only, to, gain, favor, amongst, the, sun, ##nis, and, the, religious, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 49. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but we judge by the appearances and [MASK] end was in line with the appearance . [SEP] [[CLS], but, we, judge, by, the, appearances, and, [MASK], end, was, in, line, with, the, appearance, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], but, we, judge, by, the, appearances, and, [MASK], end, was, in, line, with, the,\n",
            "\t\tappearance, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] do we say no this is hypocrisy ? [SEP] E296 in mind that many have said that E2809 prayed regularly especially after [MASK] was put in prison by the rafida and their masters . [SEP] [[CLS], do, we, say, no, this, is, h, ##y, ##po, ##c, ##ris, ##y, ?, [SEP], E, ##29, ##6, in, mind, that, many, have, said, that, E, ##28, ##0, ##9, prayed, regularly, especially, after, [MASK], was, put, in, prison, by, the, r, ##af, ##ida, and, their, masters, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 48 with text: \n",
            " \t\t[[CLS], do, we, say, no, this, is, h, ##y, ##po, ##c, ##ris, ##y, ?, [SEP], E, ##29, ##6, in, mind,\n",
            "\t\tthat, many, have, said, that, E, ##28, ##0, ##9, prayed, regularly, especially, after, [MASK], was,\n",
            "\t\tput, in, prison, by, the, r, ##af, ##ida, and, their, masters, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 33. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so they could show the clip in which there is cursing and insults from E2809 and cut the rest . [SEP] however [MASK] remained steadfast in saying the creed and was killed whilst saying it . [SEP] [[CLS], so, they, could, show, the, clip, in, which, there, is, cursing, and, insults, from, E, ##28, ##0, ##9, and, cut, the, rest, ., [SEP], however, [MASK], remained, s, ##tead, ##fast, in, saying, the, c, ##reed, and, was, killed, whilst, saying, it, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 44 with text: \n",
            " \t\t[[CLS], so, they, could, show, the, clip, in, which, there, is, cursing, and, insults, from, E,\n",
            "\t\t##28, ##0, ##9, and, cut, the, rest, ., [SEP], however, [MASK], remained, s, ##tead, ##fast, in,\n",
            "\t\tsaying, the, c, ##reed, and, was, killed, whilst, saying, it, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 26. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and [MASK] committed himself to the worship of god . and remained loyal to god . [SEP] [[CLS], and, [MASK], committed, himself, to, the, worship, of, god, ., and, remained, loyal, to, god, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], and, [MASK], committed, himself, to, the, worship, of, god, ., and, remained, loyal, to,\n",
            "\t\tgod, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 2. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 978it [00:06, 154.61it/s]\u001b[A[CLS] the newspaper quoted another commander in the mahdi army as saying ; we have taken 10 sunnis hostages we will get a ransom for five of them then we will kill them all . in every major hostage - taking operation we get 50 thousand dollars and [MASK] considered it to be the best business a person could be running in baghdad now ! [SEP] [[CLS], the, newspaper, quoted, another, commander, in, the, ma, ##hdi, army, as, saying, ;, we, have, taken, 10, sun, ##nis, hostages, we, will, get, a, ransom, for, five, of, them, then, we, will, kill, them, all, ., in, every, major, hostage, -, taking, operation, we, get, 50, thousand, dollars, and, [MASK], considered, it, to, be, the, best, business, a, person, could, be, running, in, bag, ##h, ##dad, now, !, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 70 with text: \n",
            " \t\t[[CLS], the, newspaper, quoted, another, commander, in, the, ma, ##hdi, army, as, saying, ;, we,\n",
            "\t\thave, taken, 10, sun, ##nis, hostages, we, will, get, a, ransom, for, five, of, them, then, we,\n",
            "\t\twill, kill, them, all, ., in, every, major, hostage, -, taking, operation, we, get, 50, thousand,\n",
            "\t\tdollars, and, [MASK], considered, it, to, be, the, best, business, a, person, could, be, running,\n",
            "\t\tin, bag, ##h, ##dad, now, !, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 50. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the newspaper described fadil as a valuable commander to this squad because [MASK] is a shiite who grew up in the sunni area south of baghdad . [SEP] [[CLS], the, newspaper, described, f, ##adi, ##l, as, a, valuable, commander, to, this, squad, because, [MASK], is, a, s, ##hi, ##ite, who, grew, up, in, the, sun, ##ni, area, south, of, bag, ##h, ##dad, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 36 with text: \n",
            " \t\t[[CLS], the, newspaper, described, f, ##adi, ##l, as, a, valuable, commander, to, this, squad,\n",
            "\t\tbecause, [MASK], is, a, s, ##hi, ##ite, who, grew, up, in, the, sun, ##ni, area, south, of, bag,\n",
            "\t\t##h, ##dad, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] speaks in the sunni dialect and carries an identity card from the yusufiyah sunni village . [SEP] [[CLS], [MASK], speaks, in, the, sun, ##ni, dialect, and, carries, an, identity, card, from, the, y, ##us, ##uf, ##iya, ##h, sun, ##ni, village, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 25 with text: \n",
            " \t\t[[CLS], [MASK], speaks, in, the, sun, ##ni, dialect, and, carries, an, identity, card, from, the, y,\n",
            "\t\t##us, ##uf, ##iya, ##h, sun, ##ni, village, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] was quoted as saying : i can enter the sunni areas without anyone knowing that i am a shiite . [SEP] [[CLS], [MASK], was, quoted, as, saying, :, i, can, enter, the, sun, ##ni, areas, without, anyone, knowing, that, i, am, a, s, ##hi, ##ite, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], [MASK], was, quoted, as, saying, :, i, can, enter, the, sun, ##ni, areas, without, anyone,\n",
            "\t\tknowing, that, i, am, a, s, ##hi, ##ite, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] fadil was quoted as saying that the mahdi army is resorting in its operations to reinforcements from iraqi army units commanded by its members or sympathizers and confirmed that all the police belong to the shiite militia . [SEP] [MASK] added that the mahdi army is receiving logistic and military support from iran and that they are not encountering any problems in getting weapons through to sadr city because they usually present a letter to each of the checkpoints they pass through . [SEP] [[CLS], f, ##adi, ##l, was, quoted, as, saying, that, the, ma, ##hdi, army, is, resort, ##ing, in, its, operations, to, reinforcements, from, i, ##ra, ##qi, army, units, commanded, by, its, members, or, s, ##ym, ##path, ##izer, ##s, and, confirmed, that, all, the, police, belong, to, the, s, ##hi, ##ite, militia, ., [SEP], [MASK], added, that, the, ma, ##hdi, army, is, receiving, log, ##istic, and, military, support, from, i, ##ran, and, that, they, are, not, encounter, ##ing, any, problems, in, getting, weapons, through, to, sad, ##r, city, because, they, usually, present, a, letter, to, each, of, the, check, ##points, they, pass, through, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 103 with text: \n",
            " \t\t[[CLS], f, ##adi, ##l, was, quoted, as, saying, that, the, ma, ##hdi, army, is, resort, ##ing, in,\n",
            "\t\tits, operations, to, reinforcements, from, i, ##ra, ##qi, army, units, commanded, by, its, members,\n",
            "\t\tor, s, ##ym, ##path, ##izer, ##s, and, confirmed, that, all, the, police, belong, to, the, s, ##hi,\n",
            "\t\t##ite, militia, ., [SEP], [MASK], added, that, the, ma, ##hdi, army, is, receiving, log, ##istic,\n",
            "\t\tand, military, support, from, i, ##ran, and, that, they, are, not, encounter, ##ing, any, problems,\n",
            "\t\tin, getting, weapons, through, to, sad, ##r, city, because, they, usually, present, a, letter, to,\n",
            "\t\teach, of, the, check, ##points, they, pass, through, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 52. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] unique of [MASK] kind [SEP] [[CLS], unique, of, [MASK], kind, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 6 with text: \n",
            " \t\t[[CLS], unique, of, [MASK], kind, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and victory is coming by god 's leave . [SEP] E1870 [SEP] neither E5002 - arabiya nor E5002 - jazeera -- [SEP] how i wish success to the E5002 - majd channels as they are truly the most pure and credible media channels on the face of the earth . [SEP] as for the rest of the channels they serve dubious agendas . [SEP] if they hit the spot once they miss a thousand times . [SEP] E5875 of the interior [SEP] !!\" we have al - razi here ; who is [MASK] equal !!\" [SEP] [[CLS], and, victory, is, coming, by, god, ', s, leave, ., [SEP], E, ##18, ##70, [SEP], neither, E, ##500, ##2, -, a, ##rab, ##iya, nor, E, ##500, ##2, -, j, ##az, ##eera, -, -, [SEP], how, i, wish, success, to, the, E, ##500, ##2, -, ma, ##j, ##d, channels, as, they, are, truly, the, most, pure, and, credible, media, channels, on, the, face, of, the, earth, ., [SEP], as, for, the, rest, of, the, channels, they, serve, dubious, agenda, ##s, ., [SEP], if, they, hit, the, spot, once, they, miss, a, thousand, times, ., [SEP], E, ##5, ##8, ##75, of, the, interior, [SEP], !, !, \", we, have, al, -, r, ##azi, here, ;, who, is, [MASK], equal, !, !, \", [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 122 with text: \n",
            " \t\t[[CLS], and, victory, is, coming, by, god, ', s, leave, ., [SEP], E, ##18, ##70, [SEP], neither, E,\n",
            "\t\t##500, ##2, -, a, ##rab, ##iya, nor, E, ##500, ##2, -, j, ##az, ##eera, -, -, [SEP], how, i, wish,\n",
            "\t\tsuccess, to, the, E, ##500, ##2, -, ma, ##j, ##d, channels, as, they, are, truly, the, most, pure,\n",
            "\t\tand, credible, media, channels, on, the, face, of, the, earth, ., [SEP], as, for, the, rest, of,\n",
            "\t\tthe, channels, they, serve, dubious, agenda, ##s, ., [SEP], if, they, hit, the, spot, once, they,\n",
            "\t\tmiss, a, thousand, times, ., [SEP], E, ##5, ##8, ##75, of, the, interior, [SEP], !, !, \", we, have,\n",
            "\t\tal, -, r, ##azi, here, ;, who, is, [MASK], equal, !, !, \", [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 116. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] however this should not provide an excuse to overthrow it by fair means or foul as [MASK] is our father and the common choice of millions of laboring people . [SEP] [[CLS], however, this, should, not, provide, an, excuse, to, overthrow, it, by, fair, means, or, foul, as, [MASK], is, our, father, and, the, common, choice, of, millions, of, labor, ##ing, people, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], however, this, should, not, provide, an, excuse, to, overthrow, it, by, fair, means, or,\n",
            "\t\tfoul, as, [MASK], is, our, father, and, the, common, choice, of, millions, of, labor, ##ing, people,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 17. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] therefore no matter from the perspective of average citizens or from the standpoint of the founder of the new government we should respect and develop [MASK] on the basis of inheritance innovation and improvement . [SEP] [[CLS], therefore, no, matter, from, the, perspective, of, average, citizens, or, from, the, stand, ##point, of, the, founder, of, the, new, government, we, should, respect, and, develop, [MASK], on, the, basis, of, inheritance, innovation, and, improvement, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 38 with text: \n",
            " \t\t[[CLS], therefore, no, matter, from, the, perspective, of, average, citizens, or, from, the, stand,\n",
            "\t\t##point, of, the, founder, of, the, new, government, we, should, respect, and, develop, [MASK], on,\n",
            "\t\tthe, basis, of, inheritance, innovation, and, improvement, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 27. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] that 's why i want to say to the world and the age by the end of the article : when the father 's toes are injured we should try our best to treat the foot problem and open up a new life and embrace health in the principle of respecting human relations obeying people 's wishes conforming to science and carrying on the legacy -- if the present capitalist followers frantically urge the son to kill [MASK] by all means the result could only be a tragedy ! [SEP] [[CLS], that, ', s, why, i, want, to, say, to, the, world, and, the, age, by, the, end, of, the, article, :, when, the, father, ', s, toes, are, injured, we, should, try, our, best, to, treat, the, foot, problem, and, open, up, a, new, life, and, embrace, health, in, the, principle, of, respect, ##ing, human, relations, obey, ##ing, people, ', s, wishes, conform, ##ing, to, science, and, carrying, on, the, legacy, -, -, if, the, present, capitalist, followers, frantically, urge, the, son, to, kill, [MASK], by, all, means, the, result, could, only, be, a, tragedy, !, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 98 with text: \n",
            " \t\t[[CLS], that, ', s, why, i, want, to, say, to, the, world, and, the, age, by, the, end, of, the,\n",
            "\t\tarticle, :, when, the, father, ', s, toes, are, injured, we, should, try, our, best, to, treat, the,\n",
            "\t\tfoot, problem, and, open, up, a, new, life, and, embrace, health, in, the, principle, of, respect,\n",
            "\t\t##ing, human, relations, obey, ##ing, people, ', s, wishes, conform, ##ing, to, science, and,\n",
            "\t\tcarrying, on, the, legacy, -, -, if, the, present, capitalist, followers, frantically, urge, the,\n",
            "\t\tson, to, kill, [MASK], by, all, means, the, result, could, only, be, a, tragedy, !, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 85. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] although there was only a nodding acquaintance between the neighbor and me we lived in the same apartment building after all and were both chinese . [SEP] so i discussed with my wife back home and lent [MASK] the car as we thought we would not be using the car until the same afternoon . [SEP] [[CLS], although, there, was, only, a, nodding, acquaintance, between, the, neighbor, and, me, we, lived, in, the, same, apartment, building, after, all, and, were, both, chin, ##ese, ., [SEP], so, i, discussed, with, my, wife, back, home, and, lent, [MASK], the, car, as, we, thought, we, would, not, be, using, the, car, until, the, same, afternoon, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 58 with text: \n",
            " \t\t[[CLS], although, there, was, only, a, nodding, acquaintance, between, the, neighbor, and, me, we,\n",
            "\t\tlived, in, the, same, apartment, building, after, all, and, were, both, chin, ##ese, ., [SEP], so,\n",
            "\t\ti, discussed, with, my, wife, back, home, and, lent, [MASK], the, car, as, we, thought, we, would,\n",
            "\t\tnot, be, using, the, car, until, the, same, afternoon, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 39. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i of course agreed and asked them not to rush . [SEP] this wait lasted till 5:30pm when the neighbor told me in a half - crying tone that there was an accident and everyone else was intact except [MASK] father who was in hospital for fractures . [SEP] [[CLS], i, of, course, agreed, and, asked, them, not, to, rush, ., [SEP], this, wait, lasted, till, 5, :, 30, ##pm, when, the, neighbor, told, me, in, a, half, -, crying, tone, that, there, was, an, accident, and, everyone, else, was, intact, except, [MASK], father, who, was, in, hospital, for, fracture, ##s, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 54 with text: \n",
            " \t\t[[CLS], i, of, course, agreed, and, asked, them, not, to, rush, ., [SEP], this, wait, lasted, till,\n",
            "\t\t5, :, 30, ##pm, when, the, neighbor, told, me, in, a, half, -, crying, tone, that, there, was, an,\n",
            "\t\taccident, and, everyone, else, was, intact, except, [MASK], father, who, was, in, hospital, for,\n",
            "\t\tfracture, ##s, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 43. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i comforted [MASK] first and asked my friend to rush me to the hospital . [SEP] [[CLS], i, comfort, ##ed, [MASK], first, and, asked, my, friend, to, rush, me, to, the, hospital, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], i, comfort, ##ed, [MASK], first, and, asked, my, friend, to, rush, me, to, the, hospital, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] my car was completely damaged and had been towed away . [SEP] i forced myself to refrain from the grief of loosing my beloved car -lrb- mine was liability - only coverage -rrb- and asked the friend to send their family home . [SEP] and then the nightmare began . [SEP] the next day my neighbor called me saying [MASK] father needed to undergo an operation in hospital and asked where the money would come from [SEP] [[CLS], my, car, was, completely, damaged, and, had, been, towed, away, ., [SEP], i, forced, myself, to, refrain, from, the, grief, of, lo, ##os, ##ing, my, beloved, car, -, l, ##rb, -, mine, was, liability, -, only, coverage, -, r, ##rb, -, and, asked, the, friend, to, send, their, family, home, ., [SEP], and, then, the, nightmare, began, ., [SEP], the, next, day, my, neighbor, called, me, saying, [MASK], father, needed, to, undergo, an, operation, in, hospital, and, asked, where, the, money, would, come, from, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 86 with text: \n",
            " \t\t[[CLS], my, car, was, completely, damaged, and, had, been, towed, away, ., [SEP], i, forced, myself,\n",
            "\t\tto, refrain, from, the, grief, of, lo, ##os, ##ing, my, beloved, car, -, l, ##rb, -, mine, was,\n",
            "\t\tliability, -, only, coverage, -, r, ##rb, -, and, asked, the, friend, to, send, their, family, home,\n",
            "\t\t., [SEP], and, then, the, nightmare, began, ., [SEP], the, next, day, my, neighbor, called, me,\n",
            "\t\tsaying, [MASK], father, needed, to, undergo, an, operation, in, hospital, and, asked, where, the,\n",
            "\t\tmoney, would, come, from, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 68. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i told [MASK] part would be covered by my car insurance . [SEP] [[CLS], i, told, [MASK], part, would, be, covered, by, my, car, insurance, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], i, told, [MASK], part, would, be, covered, by, my, car, insurance, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] after a while [MASK] called again saying the operation charge was high and it was difficult for them to afford [SEP] [[CLS], after, a, while, [MASK], called, again, saying, the, operation, charge, was, high, and, it, was, difficult, for, them, to, afford, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], after, a, while, [MASK], called, again, saying, the, operation, charge, was, high, and, it,\n",
            "\t\twas, difficult, for, them, to, afford, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 4. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] just then a nearly crazy voice of a woman came from the other end of the line saying [MASK] family had just saved several dozen thousand bucks for the down payment of a house ; where would they live afterwards if they spent this . [SEP] [[CLS], just, then, a, nearly, crazy, voice, of, a, woman, came, from, the, other, end, of, the, line, saying, [MASK], family, had, just, saved, several, dozen, thousand, bucks, for, the, down, payment, of, a, house, ;, where, would, they, live, afterwards, if, they, spent, this, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 47 with text: \n",
            " \t\t[[CLS], just, then, a, nearly, crazy, voice, of, a, woman, came, from, the, other, end, of, the,\n",
            "\t\tline, saying, [MASK], family, had, just, saved, several, dozen, thousand, bucks, for, the, down,\n",
            "\t\tpayment, of, a, house, ;, where, would, they, live, afterwards, if, they, spent, this, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 19. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 995it [00:06, 156.81it/s]\u001b[A[CLS] [MASK] also said my car was a japanese one which had poor safety performance and it was my car to blame for the car accident . [SEP] [[CLS], [MASK], also, said, my, car, was, a, j, ##apa, ##nese, one, which, had, poor, safety, performance, and, it, was, my, car, to, blame, for, the, car, accident, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 30 with text: \n",
            " \t\t[[CLS], [MASK], also, said, my, car, was, a, j, ##apa, ##nese, one, which, had, poor, safety,\n",
            "\t\tperformance, and, it, was, my, car, to, blame, for, the, car, accident, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] added that i should pay the money because the car was mine . [SEP] [[CLS], [MASK], added, that, i, should, pay, the, money, because, the, car, was, mine, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], [MASK], added, that, i, should, pay, the, money, because, the, car, was, mine, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i hung up the phone angrily as the dialog could not go on . [SEP] my wife -lsb- pw% -rsb- had been persuading me aside saying some women would easily lose mind once in fury . [SEP] [MASK] asked me to take no heed of them and let them calm down first . [SEP] [[CLS], i, hung, up, the, phone, angrily, as, the, dial, ##og, could, not, go, on, ., [SEP], my, wife, -, l, ##s, ##b, -, p, ##w, %, -, r, ##s, ##b, -, had, been, per, ##su, ##ading, me, aside, saying, some, women, would, easily, lose, mind, once, in, fury, ., [SEP], [MASK], asked, me, to, take, no, he, ##ed, of, them, and, let, them, calm, down, first, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 69 with text: \n",
            " \t\t[[CLS], i, hung, up, the, phone, angrily, as, the, dial, ##og, could, not, go, on, ., [SEP], my,\n",
            "\t\twife, -, l, ##s, ##b, -, p, ##w, %, -, r, ##s, ##b, -, had, been, per, ##su, ##ading, me, aside,\n",
            "\t\tsaying, some, women, would, easily, lose, mind, once, in, fury, ., [SEP], [MASK], asked, me, to,\n",
            "\t\ttake, no, he, ##ed, of, them, and, let, them, calm, down, first, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 51. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] unexpectedly coming from the other end of the line was a deep and resolute voice of a middle - aged man . [SEP] [MASK] said in an incontestable tone that my neighbor had a car accident as a result of borrowing my car which traumatized them both physically and mentally and harmed their economic interests and as the car owner they would not sue me if i compensated them for the hospital charge and the costs of missing work hours -lrb- three times -rrb- otherwise they would see me in court . [SEP] [[CLS], unexpectedly, coming, from, the, other, end, of, the, line, was, a, deep, and, re, ##sol, ##ute, voice, of, a, middle, -, aged, man, ., [SEP], [MASK], said, in, an, in, ##con, ##test, ##able, tone, that, my, neighbor, had, a, car, accident, as, a, result, of, borrow, ##ing, my, car, which, trauma, ##tized, them, both, physically, and, mentally, and, harm, ##ed, their, economic, interests, and, as, the, car, owner, they, would, not, sue, me, if, i, compensate, ##d, them, for, the, hospital, charge, and, the, costs, of, missing, work, hours, -, l, ##rb, -, three, times, -, r, ##rb, -, otherwise, they, would, see, me, in, court, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 109 with text: \n",
            " \t\t[[CLS], unexpectedly, coming, from, the, other, end, of, the, line, was, a, deep, and, re, ##sol,\n",
            "\t\t##ute, voice, of, a, middle, -, aged, man, ., [SEP], [MASK], said, in, an, in, ##con, ##test,\n",
            "\t\t##able, tone, that, my, neighbor, had, a, car, accident, as, a, result, of, borrow, ##ing, my, car,\n",
            "\t\twhich, trauma, ##tized, them, both, physically, and, mentally, and, harm, ##ed, their, economic,\n",
            "\t\tinterests, and, as, the, car, owner, they, would, not, sue, me, if, i, compensate, ##d, them, for,\n",
            "\t\tthe, hospital, charge, and, the, costs, of, missing, work, hours, -, l, ##rb, -, three, times, -, r,\n",
            "\t\t##rb, -, otherwise, they, would, see, me, in, court, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 26. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i had the interview . [SEP] by then 17 - 18 hours had passed since i 'd gotten up at 4:00 a.m. [SEP] although i was too tired to walk i still had to pump up my spirits so i 'd be in peak form for the interview . [SEP] the interview did n't last long just two hours and it was over . [SEP] i felt the interview went well and started to feel confident . [SEP] but on that day one senior level person was n't there so the next day early i had to go back to talk to [MASK] . [SEP] [[CLS], i, had, the, interview, ., [SEP], by, then, 17, -, 18, hours, had, passed, since, i, ', d, gotten, up, at, 4, :, 00, a, ., m, ., [SEP], although, i, was, too, tired, to, walk, i, still, had, to, pump, up, my, spirits, so, i, ', d, be, in, peak, form, for, the, interview, ., [SEP], the, interview, did, n, ', t, last, long, just, two, hours, and, it, was, over, ., [SEP], i, felt, the, interview, went, well, and, started, to, feel, confident, ., [SEP], but, on, that, day, one, senior, level, person, was, n, ', t, there, so, the, next, day, early, i, had, to, go, back, to, talk, to, [MASK], ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 117 with text: \n",
            " \t\t[[CLS], i, had, the, interview, ., [SEP], by, then, 17, -, 18, hours, had, passed, since, i, ', d,\n",
            "\t\tgotten, up, at, 4, :, 00, a, ., m, ., [SEP], although, i, was, too, tired, to, walk, i, still, had,\n",
            "\t\tto, pump, up, my, spirits, so, i, ', d, be, in, peak, form, for, the, interview, ., [SEP], the,\n",
            "\t\tinterview, did, n, ', t, last, long, just, two, hours, and, it, was, over, ., [SEP], i, felt, the,\n",
            "\t\tinterview, went, well, and, started, to, feel, confident, ., [SEP], but, on, that, day, one, senior,\n",
            "\t\tlevel, person, was, n, ', t, there, so, the, next, day, early, i, had, to, go, back, to, talk, to,\n",
            "\t\t[MASK], ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 114. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but why would a woman wear a man 's shoes ? [SEP] why the bodyguards ? [SEP] and why did the person 's fluid movements seem so familiar ? '' -rrb- [SEP] last week 's real E5548 story ran in the new york daily news under the snappy banner `` creepy dad was root of all clan 's woe singer sez . '' [SEP] the singer was brother E2788 and the claims based on the projected outline for [MASK] abandoned `` tell - all '' family story . [SEP] [[CLS], but, why, would, a, woman, wear, a, man, ', s, shoes, ?, [SEP], why, the, bodyguard, ##s, ?, [SEP], and, why, did, the, person, ', s, fluid, movements, seem, so, familiar, ?, ', ', -, r, ##rb, -, [SEP], last, week, ', s, real, E, ##55, ##48, story, ran, in, the, new, yo, ##rk, daily, news, under, the, snap, ##py, banner, `, `, creepy, dad, was, root, of, all, clan, ', s, w, ##oe, singer, se, ##z, ., ', ', [SEP], the, singer, was, brother, E, ##27, ##8, ##8, and, the, claims, based, on, the, projected, outline, for, [MASK], abandoned, `, `, tell, -, all, ', ', family, story, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 112 with text: \n",
            " \t\t[[CLS], but, why, would, a, woman, wear, a, man, ', s, shoes, ?, [SEP], why, the, bodyguard, ##s, ?,\n",
            "\t\t[SEP], and, why, did, the, person, ', s, fluid, movements, seem, so, familiar, ?, ', ', -, r, ##rb,\n",
            "\t\t-, [SEP], last, week, ', s, real, E, ##55, ##48, story, ran, in, the, new, yo, ##rk, daily, news,\n",
            "\t\tunder, the, snap, ##py, banner, `, `, creepy, dad, was, root, of, all, clan, ', s, w, ##oe, singer,\n",
            "\t\tse, ##z, ., ', ', [SEP], the, singer, was, brother, E, ##27, ##8, ##8, and, the, claims, based, on,\n",
            "\t\tthe, projected, outline, for, [MASK], abandoned, `, `, tell, -, all, ', ', family, story, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 99. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] reading that E6315 was a nightmare of a father and sexually abusive towards [MASK] daughters is not exactly a `` stop the presses '' moment . [SEP] [[CLS], reading, that, E, ##6, ##31, ##5, was, a, nightmare, of, a, father, and, sexually, abusive, towards, [MASK], daughters, is, not, exactly, a, `, `, stop, the, presses, ', ', moment, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], reading, that, E, ##6, ##31, ##5, was, a, nightmare, of, a, father, and, sexually, abusive,\n",
            "\t\ttowards, [MASK], daughters, is, not, exactly, a, `, `, stop, the, presses, ', ', moment, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 17. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] what 's new is this : [SEP] E2788 even suggested [MASK] father may have set up E5324 to be somehow victimized by older men . [SEP] [[CLS], what, ', s, new, is, this, :, [SEP], E, ##27, ##8, ##8, even, suggested, [MASK], father, may, have, set, up, E, ##53, ##24, to, be, somehow, victim, ##ized, by, older, men, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], what, ', s, new, is, this, :, [SEP], E, ##27, ##8, ##8, even, suggested, [MASK], father,\n",
            "\t\tmay, have, set, up, E, ##53, ##24, to, be, somehow, victim, ##ized, by, older, men, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 15. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] `` what was E6315 doing ? '' E2788 wrote . [SEP] this lifts the E5548 family horror show to a whole new order of dysfunction one that sees parents network with power and trade their children for privilege . [SEP] this should be a motif familiar to anyone acquainted with the literature of mind control and ritual abuse survivors : the father and first controller passing [MASK] child - victim up the social ladder of abuse in return for status protection and reward . [SEP] [[CLS], `, `, what, was, E, ##6, ##31, ##5, doing, ?, ', ', E, ##27, ##8, ##8, wrote, ., [SEP], this, lifts, the, E, ##55, ##48, family, horror, show, to, a, whole, new, order, of, d, ##ys, ##function, one, that, sees, parents, network, with, power, and, trade, their, children, for, privilege, ., [SEP], this, should, be, a, motif, familiar, to, anyone, acquainted, with, the, literature, of, mind, control, and, ritual, abuse, survivors, :, the, father, and, first, controller, passing, [MASK], child, -, victim, up, the, social, ladder, of, abuse, in, return, for, status, protection, and, reward, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 98 with text: \n",
            " \t\t[[CLS], `, `, what, was, E, ##6, ##31, ##5, doing, ?, ', ', E, ##27, ##8, ##8, wrote, ., [SEP],\n",
            "\t\tthis, lifts, the, E, ##55, ##48, family, horror, show, to, a, whole, new, order, of, d, ##ys,\n",
            "\t\t##function, one, that, sees, parents, network, with, power, and, trade, their, children, for,\n",
            "\t\tprivilege, ., [SEP], this, should, be, a, motif, familiar, to, anyone, acquainted, with, the,\n",
            "\t\tliterature, of, mind, control, and, ritual, abuse, survivors, :, the, father, and, first,\n",
            "\t\tcontroller, passing, [MASK], child, -, victim, up, the, social, ladder, of, abuse, in, return, for,\n",
            "\t\tstatus, protection, and, reward, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 79. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] perhaps rather the point is that the program exploits pre-existing conditions for abuse which are more common than most suspect . [SEP] -lrb- this is how i tend to think of ponchatoula 's hosanna church . -rrb- [SEP] yet we should n't discount the suspicion either particularly given some of E5324 's paranormal interests and peculiar associations . [SEP] [MASK] close friendship with spectra contactee and channeller of the nine E1040 geller for one instance . [SEP] [[CLS], perhaps, rather, the, point, is, that, the, program, exploits, pre, -, existing, conditions, for, abuse, which, are, more, common, than, most, suspect, ., [SEP], -, l, ##rb, -, this, is, how, i, tend, to, think, of, p, ##on, ##cha, ##to, ##ula, ', s, ho, ##san, ##na, church, ., -, r, ##rb, -, [SEP], yet, we, should, n, ', t, discount, the, suspicion, either, particularly, given, some, of, E, ##53, ##24, ', s, paranormal, interests, and, peculiar, associations, ., [SEP], [MASK], close, friendship, with, s, ##pect, ##ra, contact, ##ee, and, channel, ##ler, of, the, nine, E, ##10, ##40, gel, ##ler, for, one, instance, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 105 with text: \n",
            " \t\t[[CLS], perhaps, rather, the, point, is, that, the, program, exploits, pre, -, existing, conditions,\n",
            "\t\tfor, abuse, which, are, more, common, than, most, suspect, ., [SEP], -, l, ##rb, -, this, is, how,\n",
            "\t\ti, tend, to, think, of, p, ##on, ##cha, ##to, ##ula, ', s, ho, ##san, ##na, church, ., -, r, ##rb,\n",
            "\t\t-, [SEP], yet, we, should, n, ', t, discount, the, suspicion, either, particularly, given, some, of,\n",
            "\t\tE, ##53, ##24, ', s, paranormal, interests, and, peculiar, associations, ., [SEP], [MASK], close,\n",
            "\t\tfriendship, with, s, ##pect, ##ra, contact, ##ee, and, channel, ##ler, of, the, nine, E, ##10, ##40,\n",
            "\t\tgel, ##ler, for, one, instance, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 80. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] entourage including a `` personal magician '' for another . [SEP] [[CLS], [MASK], en, ##tour, ##age, including, a, `, `, personal, magician, ', ', for, another, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], [MASK], en, ##tour, ##age, including, a, `, `, personal, magician, ', ', for, another, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] even if you wanted to you wo n't be reading E2788 's account now . [SEP] like every E5548 sibling who 's tried to break from E1048 [MASK] 's climbed down returned to the fold and scrapped the book apparently on the strength of E5324 's threats to sue . [SEP] [[CLS], even, if, you, wanted, to, you, w, ##o, n, ', t, be, reading, E, ##27, ##8, ##8, ', s, account, now, ., [SEP], like, every, E, ##55, ##48, sibling, who, ', s, tried, to, break, from, E, ##10, ##48, [MASK], ', s, climbed, down, returned, to, the, fold, and, scrapped, the, book, apparently, on, the, strength, of, E, ##53, ##24, ', s, threats, to, sue, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 68 with text: \n",
            " \t\t[[CLS], even, if, you, wanted, to, you, w, ##o, n, ', t, be, reading, E, ##27, ##8, ##8, ', s,\n",
            "\t\taccount, now, ., [SEP], like, every, E, ##55, ##48, sibling, who, ', s, tried, to, break, from, E,\n",
            "\t\t##10, ##48, [MASK], ', s, climbed, down, returned, to, the, fold, and, scrapped, the, book,\n",
            "\t\tapparently, on, the, strength, of, E, ##53, ##24, ', s, threats, to, sue, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 40. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] yet there they were at E5548 's trial last year hand in hand . [SEP] in the big picture E5324 E5548 means not very much . [SEP] but [MASK] little picture may be more newsworthy than the news suggests . [SEP] [[CLS], yet, there, they, were, at, E, ##55, ##48, ', s, trial, last, year, hand, in, hand, ., [SEP], in, the, big, picture, E, ##53, ##24, E, ##55, ##48, means, not, very, much, ., [SEP], but, [MASK], little, picture, may, be, more, news, ##worthy, than, the, news, suggests, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 50 with text: \n",
            " \t\t[[CLS], yet, there, they, were, at, E, ##55, ##48, ', s, trial, last, year, hand, in, hand, .,\n",
            "\t\t[SEP], in, the, big, picture, E, ##53, ##24, E, ##55, ##48, means, not, very, much, ., [SEP], but,\n",
            "\t\t[MASK], little, picture, may, be, more, news, ##worthy, than, the, news, suggests, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 36. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] good stuff . [SEP] having departed last E2265 for a bahrain `` vacation '' from which [MASK] 's yet to emerge . [SEP] [[CLS], good, stuff, ., [SEP], having, departed, last, E, ##22, ##65, for, a, b, ##ah, ##rain, `, `, vacation, ', ', from, which, [MASK], ', s, yet, to, emerge, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 31 with text: \n",
            " \t\t[[CLS], good, stuff, ., [SEP], having, departed, last, E, ##22, ##65, for, a, b, ##ah, ##rain, `, `,\n",
            "\t\tvacation, ', ', from, which, [MASK], ', s, yet, to, emerge, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 23. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the netwerk is as sacred to the state as their mantra `` my way or total annihilation . '' [SEP] bait and switch sets the agenda . [SEP] there is no such thing as an accident . [SEP] it 's saturday morning and the papers say a moon of saturn is spewing life - giving water .... then i see the new r.i. post is about the perrenial child E5324 E5548 . [SEP] this day is named for saturn ... [SEP] ... it was foretold that one day a mighty son of saturn would in turn overthrow [MASK] [SEP] [[CLS], the, net, ##wer, ##k, is, as, sacred, to, the, state, as, their, man, ##tra, `, `, my, way, or, total, an, ##ni, ##hil, ##ation, ., ', ', [SEP], bait, and, switch, sets, the, agenda, ., [SEP], there, is, no, such, thing, as, an, accident, ., [SEP], it, ', s, sat, ##ur, ##day, morning, and, the, papers, say, a, moon, of, sat, ##urn, is, s, ##pe, ##wing, life, -, giving, water, ., ., ., ., then, i, see, the, new, r, ., i, ., post, is, about, the, per, ##ren, ##ial, child, E, ##53, ##24, E, ##55, ##48, ., [SEP], this, day, is, named, for, sat, ##urn, ., ., ., [SEP], ., ., ., it, was, fore, ##to, ##ld, that, one, day, a, mighty, son, of, sat, ##urn, would, in, turn, overthrow, [MASK], [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 134 with text: \n",
            " \t\t[[CLS], the, net, ##wer, ##k, is, as, sacred, to, the, state, as, their, man, ##tra, `, `, my, way,\n",
            "\t\tor, total, an, ##ni, ##hil, ##ation, ., ', ', [SEP], bait, and, switch, sets, the, agenda, ., [SEP],\n",
            "\t\tthere, is, no, such, thing, as, an, accident, ., [SEP], it, ', s, sat, ##ur, ##day, morning, and,\n",
            "\t\tthe, papers, say, a, moon, of, sat, ##urn, is, s, ##pe, ##wing, life, -, giving, water, ., ., ., .,\n",
            "\t\tthen, i, see, the, new, r, ., i, ., post, is, about, the, per, ##ren, ##ial, child, E, ##53, ##24,\n",
            "\t\tE, ##55, ##48, ., [SEP], this, day, is, named, for, sat, ##urn, ., ., ., [SEP], ., ., ., it, was,\n",
            "\t\tfore, ##to, ##ld, that, one, day, a, mighty, son, of, sat, ##urn, would, in, turn, overthrow,\n",
            "\t\t[MASK], [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 132. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and saturn devoured all of [MASK] children when they were born to prevent this ... '' [SEP] [[CLS], and, sat, ##urn, de, ##vour, ##ed, all, of, [MASK], children, when, they, were, born, to, prevent, this, ., ., ., ', ', [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 24 with text: \n",
            " \t\t[[CLS], and, sat, ##urn, de, ##vour, ##ed, all, of, [MASK], children, when, they, were, born, to,\n",
            "\t\tprevent, this, ., ., ., ', ', [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] apparently saturn is also intimately connected with satan masonics and revelation 13 - you know the `` number of the beast '' ... [SEP] nevermind - [SEP] i need a cup of coffee ... [SEP] i looked in vain for the cheney `` a hunting we will go '' to post this out and paste but ... [SEP] and on the subject of cowardice consider this . in a recent hunting escapade E6661 cheney accidentally shot one of [MASK] hunting comrades . [SEP] [[CLS], apparently, sat, ##urn, is, also, intimate, ##ly, connected, with, sat, ##an, ma, ##sonic, ##s, and, revelation, 13, -, you, know, the, `, `, number, of, the, beast, ', ', ., ., ., [SEP], never, ##mind, -, [SEP], i, need, a, cup, of, coffee, ., ., ., [SEP], i, looked, in, vain, for, the, ch, ##ene, ##y, `, `, a, hunting, we, will, go, ', ', to, post, this, out, and, paste, but, ., ., ., [SEP], and, on, the, subject, of, coward, ##ice, consider, this, ., in, a, recent, hunting, es, ##cap, ##ade, E, ##6, ##6, ##6, ##1, ch, ##ene, ##y, accidentally, shot, one, of, [MASK], hunting, comrades, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 111 with text: \n",
            " \t\t[[CLS], apparently, sat, ##urn, is, also, intimate, ##ly, connected, with, sat, ##an, ma, ##sonic,\n",
            "\t\t##s, and, revelation, 13, -, you, know, the, `, `, number, of, the, beast, ', ', ., ., ., [SEP],\n",
            "\t\tnever, ##mind, -, [SEP], i, need, a, cup, of, coffee, ., ., ., [SEP], i, looked, in, vain, for, the,\n",
            "\t\tch, ##ene, ##y, `, `, a, hunting, we, will, go, ', ', to, post, this, out, and, paste, but, ., ., .,\n",
            "\t\t[SEP], and, on, the, subject, of, coward, ##ice, consider, this, ., in, a, recent, hunting, es,\n",
            "\t\t##cap, ##ade, E, ##6, ##6, ##6, ##1, ch, ##ene, ##y, accidentally, shot, one, of, [MASK], hunting,\n",
            "\t\tcomrades, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 106. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 1012it [00:06, 157.81it/s]\u001b[A[CLS] on the mark as always jeff [SEP] but i thought you might cover chile 's inauguration of its first woman president today instead ... [SEP] maybe later as the story develops ? [SEP] E285 boucher 's piece mentions how victims of sexual abuse often have childlike voices almost as if their development was ' frozen ' around the time of their abuse . [SEP] E5548 seems to be of this type . [SEP] or more perversely fakes that [MASK] is : [SEP] [[CLS], on, the, mark, as, always, j, ##ef, ##f, [SEP], but, i, thought, you, might, cover, ch, ##ile, ', s, inauguration, of, its, first, woman, president, today, instead, ., ., ., [SEP], maybe, later, as, the, story, develops, ?, [SEP], E, ##28, ##5, b, ##ou, ##cher, ', s, piece, mentions, how, victims, of, sexual, abuse, often, have, child, ##like, voices, almost, as, if, their, development, was, ', frozen, ', around, the, time, of, their, abuse, ., [SEP], E, ##55, ##48, seems, to, be, of, this, type, ., [SEP], or, more, per, ##verse, ##ly, fake, ##s, that, [MASK], is, :, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 100 with text: \n",
            " \t\t[[CLS], on, the, mark, as, always, j, ##ef, ##f, [SEP], but, i, thought, you, might, cover, ch,\n",
            "\t\t##ile, ', s, inauguration, of, its, first, woman, president, today, instead, ., ., ., [SEP], maybe,\n",
            "\t\tlater, as, the, story, develops, ?, [SEP], E, ##28, ##5, b, ##ou, ##cher, ', s, piece, mentions,\n",
            "\t\thow, victims, of, sexual, abuse, often, have, child, ##like, voices, almost, as, if, their,\n",
            "\t\tdevelopment, was, ', frozen, ', around, the, time, of, their, abuse, ., [SEP], E, ##55, ##48, seems,\n",
            "\t\tto, be, of, this, type, ., [SEP], or, more, per, ##verse, ##ly, fake, ##s, that, [MASK], is, :,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 96. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the witch doctor cursing E5324 E5548 's enemies and blessing the tarnished E6623 of pop himself in a voodoo ritual in switzerland in the summer of 2000 had promised that the 25 people on E5548 's enemies list -lrb- would soon be dead -rrb- ... [SEP] E5548 had ordered [MASK] then business adviser myung - ho E1715 a u.s. - educated korean lawyer based in seoul to wire $ 150 to a bank in mali for a voodoo chief named baba who then had 42 cows ritually sacrificed for the ceremony . [SEP] [[CLS], the, witch, doctor, cursing, E, ##53, ##24, E, ##55, ##48, ', s, enemies, and, blessing, the, ta, ##rn, ##ished, E, ##6, ##6, ##23, of, pop, himself, in, a, v, ##oodoo, ritual, in, s, ##witz, ##erland, in, the, summer, of, 2000, had, promised, that, the, 25, people, on, E, ##55, ##48, ', s, enemies, list, -, l, ##rb, -, would, soon, be, dead, -, r, ##rb, -, ., ., ., [SEP], E, ##55, ##48, had, ordered, [MASK], then, business, adviser, my, ##ung, -, ho, E, ##17, ##15, a, u, ., s, ., -, educated, k, ##ore, ##an, lawyer, based, in, se, ##ou, ##l, to, wire, $, 150, to, a, bank, in, ma, ##li, for, a, v, ##oodoo, chief, named, b, ##aba, who, then, had, 42, cows, ritual, ##ly, sacrificed, for, the, ceremony, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 134 with text: \n",
            " \t\t[[CLS], the, witch, doctor, cursing, E, ##53, ##24, E, ##55, ##48, ', s, enemies, and, blessing,\n",
            "\t\tthe, ta, ##rn, ##ished, E, ##6, ##6, ##23, of, pop, himself, in, a, v, ##oodoo, ritual, in, s,\n",
            "\t\t##witz, ##erland, in, the, summer, of, 2000, had, promised, that, the, 25, people, on, E, ##55,\n",
            "\t\t##48, ', s, enemies, list, -, l, ##rb, -, would, soon, be, dead, -, r, ##rb, -, ., ., ., [SEP], E,\n",
            "\t\t##55, ##48, had, ordered, [MASK], then, business, adviser, my, ##ung, -, ho, E, ##17, ##15, a, u, .,\n",
            "\t\ts, ., -, educated, k, ##ore, ##an, lawyer, based, in, se, ##ou, ##l, to, wire, $, 150, to, a, bank,\n",
            "\t\tin, ma, ##li, for, a, v, ##oodoo, chief, named, b, ##aba, who, then, had, 42, cows, ritual, ##ly,\n",
            "\t\tsacrificed, for, the, ceremony, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 76. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E5548 had already undergone a blood bath ... [SEP] the pop star who is said to be $ 240 million in debt had paid six figures for a ritual cleansing using sheep blood to another voodoo doctor and a mysterious egyptian woman named samia who came to [MASK] with a letter of greeting from a high - ranking saudi prince purportedly nawaf bin abdulaziz E5002 - saud now the chief of intelligence of saudi arabia ... [SEP] [[CLS], E, ##55, ##48, had, already, undergone, a, blood, bath, ., ., ., [SEP], the, pop, star, who, is, said, to, be, $, 240, million, in, debt, had, paid, six, figures, for, a, ritual, clean, ##sing, using, sheep, blood, to, another, v, ##oodoo, doctor, and, a, mysterious, e, ##gy, ##pt, ##ian, woman, named, sa, ##mia, who, came, to, [MASK], with, a, letter, of, greeting, from, a, high, -, ranking, sa, ##udi, prince, pu, ##rp, ##orted, ##ly, na, ##wa, ##f, bin, a, ##b, ##du, ##la, ##zi, ##z, E, ##500, ##2, -, sa, ##ud, now, the, chief, of, intelligence, of, sa, ##udi, a, ##rab, ##ia, ., ., ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 107 with text: \n",
            " \t\t[[CLS], E, ##55, ##48, had, already, undergone, a, blood, bath, ., ., ., [SEP], the, pop, star, who,\n",
            "\t\tis, said, to, be, $, 240, million, in, debt, had, paid, six, figures, for, a, ritual, clean, ##sing,\n",
            "\t\tusing, sheep, blood, to, another, v, ##oodoo, doctor, and, a, mysterious, e, ##gy, ##pt, ##ian,\n",
            "\t\twoman, named, sa, ##mia, who, came, to, [MASK], with, a, letter, of, greeting, from, a, high, -,\n",
            "\t\tranking, sa, ##udi, prince, pu, ##rp, ##orted, ##ly, na, ##wa, ##f, bin, a, ##b, ##du, ##la, ##zi,\n",
            "\t\t##z, E, ##500, ##2, -, sa, ##ud, now, the, chief, of, intelligence, of, sa, ##udi, a, ##rab, ##ia,\n",
            "\t\t., ., ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 58. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] http://www.vanityfair.com/commentary/content/articles/050411roco03c hmm . [SEP] according to that vanity fair article jacko and E1040 geller met through `` that other fellow devotee of children 's charities E86 fayed the chairman of harrods in london who believes that the c.i.a. and E6866 E3619 had princess E1987 and [MASK] son E5316 killed . '' [SEP] [[CLS], http, :, /, /, www, ., van, ##ity, ##fair, ., com, /, commentary, /, content, /, articles, /, 05, ##0, ##41, ##1, ##ro, ##co, ##0, ##3, ##c, h, ##mm, ., [SEP], according, to, that, van, ##ity, fair, article, jack, ##o, and, E, ##10, ##40, gel, ##ler, met, through, `, `, that, other, fellow, devote, ##e, of, children, ', s, charities, E, ##86, f, ##ay, ##ed, the, chairman, of, ha, ##rro, ##ds, in, lo, ##ndon, who, believes, that, the, c, ., i, ., a, ., and, E, ##6, ##86, ##6, E, ##36, ##19, had, princess, E, ##19, ##8, ##7, and, [MASK], son, E, ##53, ##16, killed, ., ', ', [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 110 with text: \n",
            " \t\t[[CLS], http, :, /, /, www, ., van, ##ity, ##fair, ., com, /, commentary, /, content, /, articles,\n",
            "\t\t/, 05, ##0, ##41, ##1, ##ro, ##co, ##0, ##3, ##c, h, ##mm, ., [SEP], according, to, that, van,\n",
            "\t\t##ity, fair, article, jack, ##o, and, E, ##10, ##40, gel, ##ler, met, through, `, `, that, other,\n",
            "\t\tfellow, devote, ##e, of, children, ', s, charities, E, ##86, f, ##ay, ##ed, the, chairman, of, ha,\n",
            "\t\t##rro, ##ds, in, lo, ##ndon, who, believes, that, the, c, ., i, ., a, ., and, E, ##6, ##86, ##6, E,\n",
            "\t\t##36, ##19, had, princess, E, ##19, ##8, ##7, and, [MASK], son, E, ##53, ##16, killed, ., ', ',\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 100. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] could n't agree more . [SEP] i think that pedophilia 's relative commonality is what really makes it frightening . [SEP] my experience happened when i was 15 . [SEP] a friend of mine met this guy who was twice our age & who was more than willing to buy us all the whiskey we could drink . [SEP] we were both enamored with kerouac & E1796 morrison & meeting strange characters & this guy definitely fit the bill . [SEP] [MASK] was muscle for a pittsburgh madam named tex E462 . [SEP] [[CLS], could, n, ', t, agree, more, ., [SEP], i, think, that, p, ##ed, ##op, ##hil, ##ia, ', s, relative, common, ##ality, is, what, really, makes, it, frightening, ., [SEP], my, experience, happened, when, i, was, 15, ., [SEP], a, friend, of, mine, met, this, guy, who, was, twice, our, age, &, who, was, more, than, willing, to, buy, us, all, the, whiskey, we, could, drink, ., [SEP], we, were, both, en, ##amo, ##red, with, k, ##ero, ##ua, ##c, &, E, ##17, ##9, ##6, m, ##or, ##ris, ##on, &, meeting, strange, characters, &, this, guy, definitely, fit, the, bill, ., [SEP], [MASK], was, muscle, for, a, pit, ##ts, ##burgh, mad, ##am, named, te, ##x, E, ##46, ##2, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 119 with text: \n",
            " \t\t[[CLS], could, n, ', t, agree, more, ., [SEP], i, think, that, p, ##ed, ##op, ##hil, ##ia, ', s,\n",
            "\t\trelative, common, ##ality, is, what, really, makes, it, frightening, ., [SEP], my, experience,\n",
            "\t\thappened, when, i, was, 15, ., [SEP], a, friend, of, mine, met, this, guy, who, was, twice, our,\n",
            "\t\tage, &, who, was, more, than, willing, to, buy, us, all, the, whiskey, we, could, drink, ., [SEP],\n",
            "\t\twe, were, both, en, ##amo, ##red, with, k, ##ero, ##ua, ##c, &, E, ##17, ##9, ##6, m, ##or, ##ris,\n",
            "\t\t##on, &, meeting, strange, characters, &, this, guy, definitely, fit, the, bill, ., [SEP], [MASK],\n",
            "\t\twas, muscle, for, a, pit, ##ts, ##burgh, mad, ##am, named, te, ##x, E, ##46, ##2, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 101. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] dressed like a man & ran most of the massage parlors in pgh . [SEP] [[CLS], [MASK], dressed, like, a, man, &, ran, most, of, the, massage, parlor, ##s, in, p, ##gh, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], [MASK], dressed, like, a, man, &, ran, most, of, the, massage, parlor, ##s, in, p, ##gh, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] was into black magick & crowley & booze & drugs & on & on . [SEP] [[CLS], [MASK], was, into, black, magic, ##k, &, crow, ##ley, &, b, ##oo, ##ze, &, drugs, &, on, &, on, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], [MASK], was, into, black, magic, ##k, &, crow, ##ley, &, b, ##oo, ##ze, &, drugs, &, on, &,\n",
            "\t\ton, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] so one night we 're at [MASK] apartment drunk out of our minds [SEP] [[CLS], so, one, night, we, ', re, at, [MASK], apartment, drunk, out, of, our, minds, [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 16 with text: \n",
            " \t\t[[CLS], so, one, night, we, ', re, at, [MASK], apartment, drunk, out, of, our, minds, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] stands up walks into the bedroom & comes back with a porno mag . [SEP] [[CLS], [MASK], stands, up, walks, into, the, bedroom, &, comes, back, with, a, p, ##orn, ##o, ma, ##g, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 20 with text: \n",
            " \t\t[[CLS], [MASK], stands, up, walks, into, the, bedroom, &, comes, back, with, a, p, ##orn, ##o, ma,\n",
            "\t\t##g, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] drops it on the couch next to me . [SEP] [[CLS], [MASK], drops, it, on, the, couch, next, to, me, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 12 with text: \n",
            " \t\t[[CLS], [MASK], drops, it, on, the, couch, next, to, me, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] .... & i broke [MASK] nose with my elbow . [SEP] [[CLS], ., ., ., ., &, i, broke, [MASK], nose, with, my, elbow, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 15 with text: \n",
            " \t\t[[CLS], ., ., ., ., &, i, broke, [MASK], nose, with, my, elbow, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] now for me the interesting part happened years later . [SEP] at the time my buddy & i were selling things . [SEP] my buddy had a real good supplier for these things but it was [MASK] secret . [SEP] [[CLS], now, for, me, the, interesting, part, happened, years, later, ., [SEP], at, the, time, my, buddy, &, i, were, selling, things, ., [SEP], my, buddy, had, a, real, good, supplier, for, these, things, but, it, was, [MASK], secret, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 41 with text: \n",
            " \t\t[[CLS], now, for, me, the, interesting, part, happened, years, later, ., [SEP], at, the, time, my,\n",
            "\t\tbuddy, &, i, were, selling, things, ., [SEP], my, buddy, had, a, real, good, supplier, for, these,\n",
            "\t\tthings, but, it, was, [MASK], secret, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 37. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] friendless [MASK] only company being neighborhood teens looking to party . [SEP] [[CLS], friend, ##less, [MASK], only, company, being, neighborhood, teens, looking, to, party, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], friend, ##less, [MASK], only, company, being, neighborhood, teens, looking, to, party, .,\n",
            "\t\t[SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] [MASK] kept one room empty except for 4 black candles in each of its corners . [SEP] [[CLS], [MASK], kept, one, room, empty, except, for, 4, black, candles, in, each, of, its, corners, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], [MASK], kept, one, room, empty, except, for, 4, black, candles, in, each, of, its, corners,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 1. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] really pathetic . [SEP] about a year later a few of those teens took an intense dislike to E5764 & the beat [MASK] to to death . [SEP] [[CLS], really, pathetic, ., [SEP], about, a, year, later, a, few, of, those, teens, took, an, intense, dislike, to, E, ##5, ##7, ##64, &, the, beat, [MASK], to, to, death, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 32 with text: \n",
            " \t\t[[CLS], really, pathetic, ., [SEP], about, a, year, later, a, few, of, those, teens, took, an,\n",
            "\t\tintense, dislike, to, E, ##5, ##7, ##64, &, the, beat, [MASK], to, to, death, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 26. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] i apologize for continuing my ramble but ... [SEP] ... i do n't know how you folks would classify pedophilia . [SEP] maybe a 30 year old man & a 15 year old boy does n't qualify . [SEP] i wonder about that myself but i have to say that when i was 30 i was n't plying 15 year old girls with booze & copping feels . [SEP] i 'll shut up now . [SEP] speaking of E2515 robertson - [SEP] i remembered something about [MASK] being linked with E6148 bout and the uae two - step currently going on : [SEP] [[CLS], i, apologize, for, continuing, my, ram, ##ble, but, ., ., ., [SEP], ., ., ., i, do, n, ', t, know, how, you, folks, would, class, ##ify, p, ##ed, ##op, ##hil, ##ia, ., [SEP], maybe, a, 30, year, old, man, &, a, 15, year, old, boy, does, n, ', t, qualify, ., [SEP], i, wonder, about, that, myself, but, i, have, to, say, that, when, i, was, 30, i, was, n, ', t, p, ##lying, 15, year, old, girls, with, b, ##oo, ##ze, &, cop, ##ping, feels, ., [SEP], i, ', ll, shut, up, now, ., [SEP], speaking, of, E, ##25, ##15, robe, ##rts, ##on, -, [SEP], i, remembered, something, about, [MASK], being, linked, with, E, ##6, ##14, ##8, bout, and, the, u, ##ae, two, -, step, currently, going, on, :, [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 133 with text: \n",
            " \t\t[[CLS], i, apologize, for, continuing, my, ram, ##ble, but, ., ., ., [SEP], ., ., ., i, do, n, ', t,\n",
            "\t\tknow, how, you, folks, would, class, ##ify, p, ##ed, ##op, ##hil, ##ia, ., [SEP], maybe, a, 30,\n",
            "\t\tyear, old, man, &, a, 15, year, old, boy, does, n, ', t, qualify, ., [SEP], i, wonder, about, that,\n",
            "\t\tmyself, but, i, have, to, say, that, when, i, was, 30, i, was, n, ', t, p, ##lying, 15, year, old,\n",
            "\t\tgirls, with, b, ##oo, ##ze, &, cop, ##ping, feels, ., [SEP], i, ', ll, shut, up, now, ., [SEP],\n",
            "\t\tspeaking, of, E, ##25, ##15, robe, ##rts, ##on, -, [SEP], i, remembered, something, about, [MASK],\n",
            "\t\tbeing, linked, with, E, ##6, ##14, ##8, bout, and, the, u, ##ae, two, -, step, currently, going, on,\n",
            "\t\t:, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 112. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] -lrb- hmmm - [SEP] `` baba '' ... -rrb- [SEP] http://www.analystnewspaper.com/ameu_scam_feb21.html [SEP] http://www.waynemadsenreport.com/ [SEP] http://www.globalpolicy.org/intljustice/wanted/2005/1212ties.htm [SEP] my question is when is a victim guilty of abuse ? [SEP] E5548 is definitely a mind - control sra victim . [SEP] [MASK] 's programmed . [SEP] [[CLS], -, l, ##rb, -, h, ##mm, ##m, -, [SEP], `, `, b, ##aba, ', ', ., ., ., -, r, ##rb, -, [SEP], http, :, /, /, www, ., analyst, ##ne, ##ws, ##paper, ., com, /, am, ##eu, _, s, ##cam, _, f, ##eb, ##21, ., html, [SEP], http, :, /, /, www, ., way, ##ne, ##mad, ##sen, ##re, ##port, ., com, /, [SEP], http, :, /, /, www, ., global, ##poli, ##cy, ., org, /, in, ##tl, ##ju, ##stic, ##e, /, wanted, /, 2005, /, 121, ##2, ##ties, ., h, ##t, ##m, [SEP], my, question, is, when, is, a, victim, guilty, of, abuse, ?, [SEP], E, ##55, ##48, is, definitely, a, mind, -, control, s, ##ra, victim, ., [SEP], [MASK], ', s, programmed, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 127 with text: \n",
            " \t\t[[CLS], -, l, ##rb, -, h, ##mm, ##m, -, [SEP], `, `, b, ##aba, ', ', ., ., ., -, r, ##rb, -, [SEP],\n",
            "\t\thttp, :, /, /, www, ., analyst, ##ne, ##ws, ##paper, ., com, /, am, ##eu, _, s, ##cam, _, f, ##eb,\n",
            "\t\t##21, ., html, [SEP], http, :, /, /, www, ., way, ##ne, ##mad, ##sen, ##re, ##port, ., com, /,\n",
            "\t\t[SEP], http, :, /, /, www, ., global, ##poli, ##cy, ., org, /, in, ##tl, ##ju, ##stic, ##e, /,\n",
            "\t\twanted, /, 2005, /, 121, ##2, ##ties, ., h, ##t, ##m, [SEP], my, question, is, when, is, a, victim,\n",
            "\t\tguilty, of, abuse, ?, [SEP], E, ##55, ##48, is, definitely, a, mind, -, control, s, ##ra, victim, .,\n",
            "\t\t[SEP], [MASK], ', s, programmed, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 121. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 1029it [00:07, 127.71it/s]\u001b[A[CLS] it 's very effective as many ' feel ' that whilst mj is naive [MASK] 's not capable of the evil associated with pedophile activity . [SEP] [[CLS], it, ', s, very, effective, as, many, ', feel, ', that, whilst, m, ##j, is, naive, [MASK], ', s, not, capable, of, the, evil, associated, with, p, ##ed, ##op, ##hil, ##e, activity, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 35 with text: \n",
            " \t\t[[CLS], it, ', s, very, effective, as, many, ', feel, ', that, whilst, m, ##j, is, naive, [MASK], ',\n",
            "\t\ts, not, capable, of, the, evil, associated, with, p, ##ed, ##op, ##hil, ##e, activity, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 17. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] this may not be the case as the public ' persona ' is merely an ' alter ' personality and [MASK] may have many other ' personalities ' which are dark . [SEP] [[CLS], this, may, not, be, the, case, as, the, public, ', persona, ', is, merely, an, ', alter, ', personality, and, [MASK], may, have, many, other, ', personalities, ', which, are, dark, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 34 with text: \n",
            " \t\t[[CLS], this, may, not, be, the, case, as, the, public, ', persona, ', is, merely, an, ', alter, ',\n",
            "\t\tpersonality, and, [MASK], may, have, many, other, ', personalities, ', which, are, dark, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 21. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] then comes the question whether [MASK] programmers were programmed themselves . [SEP] [[CLS], then, comes, the, question, whether, [MASK], programmer, ##s, were, programmed, themselves, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 14 with text: \n",
            " \t\t[[CLS], then, comes, the, question, whether, [MASK], programmer, ##s, were, programmed, themselves,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in an interview with blender -lrb- ca n't remember what issue but E7224 's on the cover -rrb- E7224 talks at length about having mulitiple personalities . [SEP] i ca n't remember all the details but there 's E7224 the normal pop star and among others -lrb- and perhaps the most notable in this context -rrb- one [MASK] calls `` strawberry '' who is geared towards explicit sexual activities . [SEP] [[CLS], in, an, interview, with, blend, ##er, -, l, ##rb, -, ca, n, ', t, remember, what, issue, but, E, ##7, ##22, ##4, ', s, on, the, cover, -, r, ##rb, -, E, ##7, ##22, ##4, talks, at, length, about, having, m, ##uli, ##tip, ##le, personalities, ., [SEP], i, ca, n, ', t, remember, all, the, details, but, there, ', s, E, ##7, ##22, ##4, the, normal, pop, star, and, among, others, -, l, ##rb, -, and, perhaps, the, most, notable, in, this, context, -, r, ##rb, -, one, [MASK], calls, `, `, straw, ##berry, ', ', who, is, geared, towards, explicit, sexual, activities, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 106 with text: \n",
            " \t\t[[CLS], in, an, interview, with, blend, ##er, -, l, ##rb, -, ca, n, ', t, remember, what, issue,\n",
            "\t\tbut, E, ##7, ##22, ##4, ', s, on, the, cover, -, r, ##rb, -, E, ##7, ##22, ##4, talks, at, length,\n",
            "\t\tabout, having, m, ##uli, ##tip, ##le, personalities, ., [SEP], i, ca, n, ', t, remember, all, the,\n",
            "\t\tdetails, but, there, ', s, E, ##7, ##22, ##4, the, normal, pop, star, and, among, others, -, l,\n",
            "\t\t##rb, -, and, perhaps, the, most, notable, in, this, context, -, r, ##rb, -, one, [MASK], calls, `,\n",
            "\t\t`, straw, ##berry, ', ', who, is, geared, towards, explicit, sexual, activities, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 89. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] as i understand this is a classic alters description . [SEP] a real man would at least let a former board member be able to read the reason [MASK] had been banned but i forgot i was dealing with a coward ! [SEP] [[CLS], as, i, understand, this, is, a, classic, alter, ##s, description, ., [SEP], a, real, man, would, at, least, let, a, former, board, member, be, able, to, read, the, reason, [MASK], had, been, banned, but, i, forgot, i, was, dealing, with, a, coward, !, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 45 with text: \n",
            " \t\t[[CLS], as, i, understand, this, is, a, classic, alter, ##s, description, ., [SEP], a, real, man,\n",
            "\t\twould, at, least, let, a, former, board, member, be, able, to, read, the, reason, [MASK], had, been,\n",
            "\t\tbanned, but, i, forgot, i, was, dealing, with, a, coward, !, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 30. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] antiaristo will be disappointed or has [MASK] already been banned too ??? [SEP] [[CLS], anti, ##aris, ##to, will, be, disappointed, or, has, [MASK], already, been, banned, too, ?, ?, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 18 with text: \n",
            " \t\t[[CLS], anti, ##aris, ##to, will, be, disappointed, or, has, [MASK], already, been, banned, too, ?,\n",
            "\t\t?, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 9. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] one mus n't forget [MASK] now infamous superbowl `` nipple '' stunt . [SEP] [[CLS], one, m, ##us, n, ', t, forget, [MASK], now, infamous, superb, ##ow, ##l, `, `, nipple, ', ', stunt, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 22 with text: \n",
            " \t\t[[CLS], one, m, ##us, n, ', t, forget, [MASK], now, infamous, superb, ##ow, ##l, `, `, nipple, ', ',\n",
            "\t\tstunt, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it 's possible that jacksons have been using the occult arts for a long long time . [SEP] i 'd love to hear someone else 's thoughts on this - [SEP] here 's a link to just a wee bit more about E5548 and [MASK] alleged mpd - [SEP] [[CLS], it, ', s, possible, that, jack, ##sons, have, been, using, the, o, ##cc, ##ult, arts, for, a, long, long, time, ., [SEP], i, ', d, love, to, hear, someone, else, ', s, thoughts, on, this, -, [SEP], here, ', s, a, link, to, just, a, we, ##e, bit, more, about, E, ##55, ##48, and, [MASK], alleged, m, ##p, ##d, -, [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 62 with text: \n",
            " \t\t[[CLS], it, ', s, possible, that, jack, ##sons, have, been, using, the, o, ##cc, ##ult, arts, for,\n",
            "\t\ta, long, long, time, ., [SEP], i, ', d, love, to, hear, someone, else, ', s, thoughts, on, this, -,\n",
            "\t\t[SEP], here, ', s, a, link, to, just, a, we, ##e, bit, more, about, E, ##55, ##48, and, [MASK],\n",
            "\t\talleged, m, ##p, ##d, -, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 55. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] but in this case it fits in so well with the ongoing discussion that i 'm tentatively inclined to trust it : [SEP] delay 's career and power became at the same time a central project for all three of the shadow partners E6105 norquist E6876 abramoff and E1082 E874 . [SEP] delay was by no means a random point of interest . [SEP] E6408 delay staggered into E4477 as a freshman congressman in 1984 a terrible drunk [MASK] life dissolving . [SEP] [[CLS], but, in, this, case, it, fits, in, so, well, with, the, ongoing, discussion, that, i, ', m, tentatively, inclined, to, trust, it, :, [SEP], delay, ', s, career, and, power, became, at, the, same, time, a, central, project, for, all, three, of, the, shadow, partners, E, ##6, ##10, ##5, nor, ##quist, E, ##6, ##8, ##7, ##6, a, ##bra, ##mo, ##ff, and, E, ##10, ##8, ##2, E, ##8, ##7, ##4, ., [SEP], delay, was, by, no, means, a, random, point, of, interest, ., [SEP], E, ##64, ##0, ##8, delay, staggered, into, E, ##44, ##7, ##7, as, a, freshman, congress, ##man, in, 1984, a, terrible, drunk, [MASK], life, di, ##sso, ##lving, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 112 with text: \n",
            " \t\t[[CLS], but, in, this, case, it, fits, in, so, well, with, the, ongoing, discussion, that, i, ', m,\n",
            "\t\ttentatively, inclined, to, trust, it, :, [SEP], delay, ', s, career, and, power, became, at, the,\n",
            "\t\tsame, time, a, central, project, for, all, three, of, the, shadow, partners, E, ##6, ##10, ##5, nor,\n",
            "\t\t##quist, E, ##6, ##8, ##7, ##6, a, ##bra, ##mo, ##ff, and, E, ##10, ##8, ##2, E, ##8, ##7, ##4, .,\n",
            "\t\t[SEP], delay, was, by, no, means, a, random, point, of, interest, ., [SEP], E, ##64, ##0, ##8,\n",
            "\t\tdelay, staggered, into, E, ##44, ##7, ##7, as, a, freshman, congress, ##man, in, 1984, a, terrible,\n",
            "\t\tdrunk, [MASK], life, di, ##sso, ##lving, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 105. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] rep. E2204 E3629 -lrb- r - va. -rrb- approached [MASK] with a road to salvation . [SEP] [[CLS], re, ##p, ., E, ##22, ##0, ##4, E, ##36, ##29, -, l, ##rb, -, r, -, v, ##a, ., -, r, ##rb, -, approached, [MASK], with, a, road, to, salvation, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 33 with text: \n",
            " \t\t[[CLS], re, ##p, ., E, ##22, ##0, ##4, E, ##36, ##29, -, l, ##rb, -, r, -, v, ##a, ., -, r, ##rb, -,\n",
            "\t\tapproached, [MASK], with, a, road, to, salvation, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 25. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] colson 's prison fellowship ministries is a subordinate unit of the cult with its tentacles in most prisons in the united states . [SEP] after E6408 delay was zapped E6289 colson became delay 's personal guru . [SEP] colson 's militancy for the end - times war on muslims has made [MASK] a congenial companion to E6876 abramoff in their keeping of delay -- abramoff has repeatedly led E6408 to E7074 to call for all - out war against the arabs . [SEP] [[CLS], co, ##lson, ', s, prison, fellowship, ministries, is, a, subordinate, unit, of, the, cult, with, its, tent, ##acles, in, most, prisons, in, the, united, states, ., [SEP], after, E, ##64, ##0, ##8, delay, was, z, ##ap, ##ped, E, ##6, ##28, ##9, co, ##lson, became, delay, ', s, personal, g, ##uru, ., [SEP], co, ##lson, ', s, mi, ##lit, ##ancy, for, the, end, -, times, war, on, m, ##us, ##lim, ##s, has, made, [MASK], a, con, ##gen, ##ial, companion, to, E, ##6, ##8, ##7, ##6, a, ##bra, ##mo, ##ff, in, their, keeping, of, delay, -, -, a, ##bra, ##mo, ##ff, has, repeatedly, led, E, ##64, ##0, ##8, to, E, ##70, ##7, ##4, to, call, for, all, -, out, war, against, the, a, ##rab, ##s, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 126 with text: \n",
            " \t\t[[CLS], co, ##lson, ', s, prison, fellowship, ministries, is, a, subordinate, unit, of, the, cult,\n",
            "\t\twith, its, tent, ##acles, in, most, prisons, in, the, united, states, ., [SEP], after, E, ##64, ##0,\n",
            "\t\t##8, delay, was, z, ##ap, ##ped, E, ##6, ##28, ##9, co, ##lson, became, delay, ', s, personal, g,\n",
            "\t\t##uru, ., [SEP], co, ##lson, ', s, mi, ##lit, ##ancy, for, the, end, -, times, war, on, m, ##us,\n",
            "\t\t##lim, ##s, has, made, [MASK], a, con, ##gen, ##ial, companion, to, E, ##6, ##8, ##7, ##6, a, ##bra,\n",
            "\t\t##mo, ##ff, in, their, keeping, of, delay, -, -, a, ##bra, ##mo, ##ff, has, repeatedly, led, E,\n",
            "\t\t##64, ##0, ##8, to, E, ##70, ##7, ##4, to, call, for, all, -, out, war, against, the, a, ##rab, ##s,\n",
            "\t\t., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 73. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] E2515 robertson started off as assistant pastor to bredesen whose intelligence partners the full gospel businessmen 's fellowship raised the money to expand robertson 's and bredesen 's E3957 - based E3174 broadcasting network -lrb- cbn -rrb- toward global power status . [SEP] the sentence i put in boldface is *extremely* interesting . [SEP] was n't there a E5324 E5548 video where [MASK] was envisioning himself as ruler of the world or some such ? [SEP] [[CLS], E, ##25, ##15, robe, ##rts, ##on, started, off, as, assistant, pastor, to, bred, ##ese, ##n, whose, intelligence, partners, the, full, gospel, businessmen, ', s, fellowship, raised, the, money, to, expand, robe, ##rts, ##on, ', s, and, bred, ##ese, ##n, ', s, E, ##39, ##5, ##7, -, based, E, ##31, ##7, ##4, broadcasting, network, -, l, ##rb, -, c, ##b, ##n, -, r, ##rb, -, toward, global, power, status, ., [SEP], the, sentence, i, put, in, bold, ##face, is, *, extremely, *, interesting, ., [SEP], was, n, ', t, there, a, E, ##53, ##24, E, ##55, ##48, video, where, [MASK], was, en, ##vision, ##ing, himself, as, ruler, of, the, world, or, some, such, ?, [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 115 with text: \n",
            " \t\t[[CLS], E, ##25, ##15, robe, ##rts, ##on, started, off, as, assistant, pastor, to, bred, ##ese, ##n,\n",
            "\t\twhose, intelligence, partners, the, full, gospel, businessmen, ', s, fellowship, raised, the, money,\n",
            "\t\tto, expand, robe, ##rts, ##on, ', s, and, bred, ##ese, ##n, ', s, E, ##39, ##5, ##7, -, based, E,\n",
            "\t\t##31, ##7, ##4, broadcasting, network, -, l, ##rb, -, c, ##b, ##n, -, r, ##rb, -, toward, global,\n",
            "\t\tpower, status, ., [SEP], the, sentence, i, put, in, bold, ##face, is, *, extremely, *, interesting,\n",
            "\t\t., [SEP], was, n, ', t, there, a, E, ##53, ##24, E, ##55, ##48, video, where, [MASK], was, en,\n",
            "\t\t##vision, ##ing, himself, as, ruler, of, the, world, or, some, such, ?, [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 99. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] the video is a highly disturbing piece of work . [SEP] [MASK] bloated sense of importance is not only chilling it is threatening . [SEP] [[CLS], the, video, is, a, highly, disturbing, piece, of, work, ., [SEP], [MASK], b, ##loat, ##ed, sense, of, importance, is, not, only, chill, ##ing, it, is, threatening, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 29 with text: \n",
            " \t\t[[CLS], the, video, is, a, highly, disturbing, piece, of, work, ., [SEP], [MASK], b, ##loat, ##ed,\n",
            "\t\tsense, of, importance, is, not, only, chill, ##ing, it, is, threatening, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 12. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] suddenly E5324 's perfect world of oz / neverland is forced to be compared with an aryan nation one that substitutes children for hitler 's master race . [SEP] large image of the E5324 E5548 history statue . [SEP] there is some sort of an emblem with a star on [MASK] shoulder but it 's at an angle and not easy to make out clearly . [SEP] [[CLS], suddenly, E, ##53, ##24, ', s, perfect, world, of, oz, /, never, ##land, is, forced, to, be, compared, with, an, a, ##ryan, nation, one, that, substitute, ##s, children, for, hit, ##ler, ', s, master, race, ., [SEP], large, image, of, the, E, ##53, ##24, E, ##55, ##48, history, statue, ., [SEP], there, is, some, sort, of, an, emblem, with, a, star, on, [MASK], shoulder, but, it, ', s, at, an, angle, and, not, easy, to, make, out, clearly, ., [SEP]] his\n",
            "target_ids_list: [his]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 81 with text: \n",
            " \t\t[[CLS], suddenly, E, ##53, ##24, ', s, perfect, world, of, oz, /, never, ##land, is, forced, to, be,\n",
            "\t\tcompared, with, an, a, ##ryan, nation, one, that, substitute, ##s, children, for, hit, ##ler, ', s,\n",
            "\t\tmaster, race, ., [SEP], large, image, of, the, E, ##53, ##24, E, ##55, ##48, history, statue, .,\n",
            "\t\t[SEP], there, is, some, sort, of, an, emblem, with, a, star, on, [MASK], shoulder, but, it, ', s,\n",
            "\t\tat, an, angle, and, not, easy, to, make, out, clearly, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 63. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[his]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] and that wildness is still in [MASK] as it is with all children . [SEP] [[CLS], and, that, wild, ##ness, is, still, in, [MASK], as, it, is, with, all, children, ., [SEP]] him\n",
            "target_ids_list: [him]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 17 with text: \n",
            " \t\t[[CLS], and, that, wild, ##ness, is, still, in, [MASK], as, it, is, with, all, children, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 8. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[him]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "\n",
            "loading instances: 1044it [00:07, 132.05it/s]\u001b[A[CLS] E6997 's only cryptic instructions on character development to E3429 boucicault the first actress to play E3520 was that [MASK] was to remember that E3520 is a bird and is one day old . [SEP] [[CLS], E, ##6, ##9, ##9, ##7, ', s, only, cry, ##ptic, instructions, on, character, development, to, E, ##34, ##29, b, ##ou, ##ci, ##ca, ##ult, the, first, actress, to, play, E, ##35, ##20, was, that, [MASK], was, to, remember, that, E, ##35, ##20, is, a, bird, and, is, one, day, old, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 52 with text: \n",
            " \t\t[[CLS], E, ##6, ##9, ##9, ##7, ', s, only, cry, ##ptic, instructions, on, character, development,\n",
            "\t\tto, E, ##34, ##29, b, ##ou, ##ci, ##ca, ##ult, the, first, actress, to, play, E, ##35, ##20, was,\n",
            "\t\tthat, [MASK], was, to, remember, that, E, ##35, ##20, is, a, bird, and, is, one, day, old, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 34. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] in kensington gardens [MASK] watches out for children who have fallen un-noticed out of their perambulators . [SEP] [[CLS], in, k, ##ens, ##ington, gardens, [MASK], watches, out, for, children, who, have, fallen, un, -, noticed, out, of, their, per, ##am, ##bula, ##tors, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 26 with text: \n",
            " \t\t[[CLS], in, k, ##ens, ##ington, gardens, [MASK], watches, out, for, children, who, have, fallen, un,\n",
            "\t\t-, noticed, out, of, their, per, ##am, ##bula, ##tors, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 6. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] a far cry from the stealing of children . [SEP] E3520 and the lost boys crave a mother to tell them stories . [SEP] it 's ironic that while their own lives are far more exotic than the fairy tales E7046 tells it 's those fictions that they want told by a mother . [SEP] E6997 says quite explicitly in E3520 and E7046 that it is E7046 who does the tempting to keep E3520 with [MASK] not the other way around . [SEP] [[CLS], a, far, cry, from, the, stealing, of, children, ., [SEP], E, ##35, ##20, and, the, lost, boys, c, ##rave, a, mother, to, tell, them, stories, ., [SEP], it, ', s, ironic, that, while, their, own, lives, are, far, more, exotic, than, the, fairy, tales, E, ##70, ##46, tells, it, ', s, those, fiction, ##s, that, they, want, told, by, a, mother, ., [SEP], E, ##6, ##9, ##9, ##7, says, quite, explicitly, in, E, ##35, ##20, and, E, ##70, ##46, that, it, is, E, ##70, ##46, who, does, the, tempting, to, keep, E, ##35, ##20, with, [MASK], not, the, other, way, around, ., [SEP]] her\n",
            "target_ids_list: [her]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 104 with text: \n",
            " \t\t[[CLS], a, far, cry, from, the, stealing, of, children, ., [SEP], E, ##35, ##20, and, the, lost,\n",
            "\t\tboys, c, ##rave, a, mother, to, tell, them, stories, ., [SEP], it, ', s, ironic, that, while, their,\n",
            "\t\town, lives, are, far, more, exotic, than, the, fairy, tales, E, ##70, ##46, tells, it, ', s, those,\n",
            "\t\tfiction, ##s, that, they, want, told, by, a, mother, ., [SEP], E, ##6, ##9, ##9, ##7, says, quite,\n",
            "\t\texplicitly, in, E, ##35, ##20, and, E, ##70, ##46, that, it, is, E, ##70, ##46, who, does, the,\n",
            "\t\ttempting, to, keep, E, ##35, ##20, with, [MASK], not, the, other, way, around, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 96. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[her]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] it is [MASK] who asks if E5324 and E719 could come too . [SEP] [[CLS], it, is, [MASK], who, asks, if, E, ##53, ##24, and, E, ##7, ##19, could, come, too, ., [SEP]] she\n",
            "target_ids_list: [she]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 19 with text: \n",
            " \t\t[[CLS], it, is, [MASK], who, asks, if, E, ##53, ##24, and, E, ##7, ##19, could, come, too, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 3. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[she]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "[CLS] as for the child abuse i have to agree with the post above you can find abuse and ' sick ' individuals in any organization [SEP] but you have too look at the proportion to the rest of society . [SEP] one of my friends is an ' elder ' -lrb- soething like a ' local church leader ' -rrb- [SEP] but [MASK] also said their instructions say that if they hear of any abuse they are to follow local laws which usually requires them to inform the police asap . [SEP] [[CLS], as, for, the, child, abuse, i, have, to, agree, with, the, post, above, you, can, find, abuse, and, ', sick, ', individuals, in, any, organization, [SEP], but, you, have, too, look, at, the, proportion, to, the, rest, of, society, ., [SEP], one, of, my, friends, is, an, ', elder, ', -, l, ##rb, -, so, ##eth, ##ing, like, a, ', local, church, leader, ', -, r, ##rb, -, [SEP], but, [MASK], also, said, their, instructions, say, that, if, they, hear, of, any, abuse, they, are, to, follow, local, laws, which, usually, requires, them, to, inform, the, police, as, ##ap, ., [SEP]] he\n",
            "target_ids_list: [he]\n",
            " target_ids: TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "tokens TextField of length 102 with text: \n",
            " \t\t[[CLS], as, for, the, child, abuse, i, have, to, agree, with, the, post, above, you, can, find,\n",
            "\t\tabuse, and, ', sick, ', individuals, in, any, organization, [SEP], but, you, have, too, look, at,\n",
            "\t\tthe, proportion, to, the, rest, of, society, ., [SEP], one, of, my, friends, is, an, ', elder, ', -,\n",
            "\t\tl, ##rb, -, so, ##eth, ##ing, like, a, ', local, church, leader, ', -, r, ##rb, -, [SEP], but,\n",
            "\t\t[MASK], also, said, their, instructions, say, that, if, they, hear, of, any, abuse, they, are, to,\n",
            "\t\tfollow, local, laws, which, usually, requires, them, to, inform, the, police, as, ##ap, ., [SEP]]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "mask_positions ListField of 1 IndexFields : \n",
            " \t IndexField with index: 71. \n",
            "\n",
            "target_ids TextField of length 1 with text: \n",
            " \t\t[he]\n",
            " \t\tand TokenIndexers : {'transformer': 'PretrainedTransformerIndexer'}\n",
            "loading instances: 1049it [00:07, 146.77it/s]\n",
            "2021-10-08 12:52:37,227 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one\n",
            "2021-10-08 12:52:37,227 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys\n",
            "0it [00:07, ?it/s]\n",
            "2021-10-08 12:52:37,244 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/__main__.py\", line 46, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/__init__.py\", line 122, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 121, in train_model_from_args\n",
            "    file_friendly_logging=args.file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 187, in train_model_from_file\n",
            "    return_model=return_model,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 260, in train_model\n",
            "    file_friendly_logging=file_friendly_logging,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 504, in _train_worker\n",
            "    metrics = train_loop.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/commands/train.py\", line 577, in run\n",
            "    return self.trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 750, in train\n",
            "    metrics, epoch = self._try_train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 773, in _try_train\n",
            "    train_metrics = self._train_epoch(epoch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 490, in _train_epoch\n",
            "    batch_outputs = self.batch_outputs(batch, for_training=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/training/gradient_descent_trainer.py\", line 383, in batch_outputs\n",
            "    output_dict = self._pytorch_model(**batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1071, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp/fairness/adversarial_bias_mitigator.py\", line 121, in forward\n",
            "    predictor_output_dict = self.predictor.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/allennlp_models/lm/models/masked_language_model.py\", line 105, in forward\n",
            "    f\"Number of targets ({targets.size()}) and number of masks \"\n",
            "ValueError: Number of targets (torch.Size([128, 0])) and number of masks (torch.Size([128, 1])) are not equal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eHwK2hUyxpG"
      },
      "source": [
        "# dealing with rows that have more than 1 masked token (removing them)\n",
        "import pandas as pd \n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9MHs8v4y481",
        "outputId": "94b95c5d-92d3-415b-9679-9d11782bdc27"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv ('data_one_mask.csv')\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   text pronouns\n",
            "0     [CLS] welcome both of you to the studio to par...       he\n",
            "1     [CLS] well lots of barricade tape has been str...       he\n",
            "2     [CLS] however the affected area was not as lar...       he\n",
            "3     [CLS] for instance this evening we received an...      him\n",
            "4     [CLS] right . [SEP] hey [MASK] said this was q...       he\n",
            "...                                                 ...      ...\n",
            "1187  [CLS] in kensington gardens [MASK] watches out...       he\n",
            "1188  [CLS] a far cry from the stealing of children ...      her\n",
            "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...      she\n",
            "1190  [CLS] view the video here : http://youtube.com...       he\n",
            "1191  [CLS] as for the child abuse i have to agree w...       he\n",
            "\n",
            "[1192 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UySOPUhzvMZ"
      },
      "source": [
        "mask = df['text'].str.findall('MASK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "WU3akRpT5Gzi",
        "outputId": "b67999a3-0ffd-4284-bdb1-815ad9dae506"
      },
      "source": [
        "mask = pd.DataFrame(mask)\n",
        "mask = mask.rename(columns={'text': 'mask'})\n",
        "mask"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>[MASK, MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1192 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              mask\n",
              "0           [MASK]\n",
              "1           [MASK]\n",
              "2           [MASK]\n",
              "3           [MASK]\n",
              "4           [MASK]\n",
              "...            ...\n",
              "1187        [MASK]\n",
              "1188        [MASK]\n",
              "1189        [MASK]\n",
              "1190  [MASK, MASK]\n",
              "1191        [MASK]\n",
              "\n",
              "[1192 rows x 1 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "E6x_etVX66to",
        "outputId": "498c9e19-3a26-44d5-a13a-0b00fa3466fd"
      },
      "source": [
        "all = pd.concat([df, mask.reindex(df.index)], axis=1)\n",
        "all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pronouns</th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] welcome both of you to the studio to par...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] well lots of barricade tape has been str...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] however the affected area was not as lar...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] for instance this evening we received an...</td>\n",
              "      <td>him</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] right . [SEP] hey [MASK] said this was q...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[CLS] in kensington gardens [MASK] watches out...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[CLS] a far cry from the stealing of children ...</td>\n",
              "      <td>her</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[CLS] it is [MASK] who asks if E5324 and E719 ...</td>\n",
              "      <td>she</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>[CLS] view the video here : http://youtube.com...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK, MASK]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[CLS] as for the child abuse i have to agree w...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1192 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text pronouns          mask\n",
              "0     [CLS] welcome both of you to the studio to par...       he        [MASK]\n",
              "1     [CLS] well lots of barricade tape has been str...       he        [MASK]\n",
              "2     [CLS] however the affected area was not as lar...       he        [MASK]\n",
              "3     [CLS] for instance this evening we received an...      him        [MASK]\n",
              "4     [CLS] right . [SEP] hey [MASK] said this was q...       he        [MASK]\n",
              "...                                                 ...      ...           ...\n",
              "1187  [CLS] in kensington gardens [MASK] watches out...       he        [MASK]\n",
              "1188  [CLS] a far cry from the stealing of children ...      her        [MASK]\n",
              "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...      she        [MASK]\n",
              "1190  [CLS] view the video here : http://youtube.com...       he  [MASK, MASK]\n",
              "1191  [CLS] as for the child abuse i have to agree w...       he        [MASK]\n",
              "\n",
              "[1192 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtbRRjSS7HJ-"
      },
      "source": [
        "all['Len'] = all['mask'].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj4Vb16u9FOp"
      },
      "source": [
        "df = all[all.Len < 2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9KK16oqhltGL",
        "outputId": "8e9e4cda-b843-4ee4-b82a-3e6408639a4a"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pronouns</th>\n",
              "      <th>mask</th>\n",
              "      <th>Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] welcome both of you to the studio to par...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] well lots of barricade tape has been str...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] however the affected area was not as lar...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] for instance this evening we received an...</td>\n",
              "      <td>him</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] right . [SEP] hey [MASK] said this was q...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>[CLS] E6997 's only cryptic instructions on ch...</td>\n",
              "      <td>she</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[CLS] in kensington gardens [MASK] watches out...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[CLS] a far cry from the stealing of children ...</td>\n",
              "      <td>her</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[CLS] it is [MASK] who asks if E5324 and E719 ...</td>\n",
              "      <td>she</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[CLS] as for the child abuse i have to agree w...</td>\n",
              "      <td>he</td>\n",
              "      <td>[MASK]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1049 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text pronouns    mask  Len\n",
              "0     [CLS] welcome both of you to the studio to par...       he  [MASK]    1\n",
              "1     [CLS] well lots of barricade tape has been str...       he  [MASK]    1\n",
              "2     [CLS] however the affected area was not as lar...       he  [MASK]    1\n",
              "3     [CLS] for instance this evening we received an...      him  [MASK]    1\n",
              "4     [CLS] right . [SEP] hey [MASK] said this was q...       he  [MASK]    1\n",
              "...                                                 ...      ...     ...  ...\n",
              "1186  [CLS] E6997 's only cryptic instructions on ch...      she  [MASK]    1\n",
              "1187  [CLS] in kensington gardens [MASK] watches out...       he  [MASK]    1\n",
              "1188  [CLS] a far cry from the stealing of children ...      her  [MASK]    1\n",
              "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...      she  [MASK]    1\n",
              "1191  [CLS] as for the child abuse i have to agree w...       he  [MASK]    1\n",
              "\n",
              "[1049 rows x 4 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsWlndDK-Pz9",
        "outputId": "59479ac0-8834-4504-96d0-6dbb469e597b"
      },
      "source": [
        "df.drop(['mask', 'Len'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1x1BYUty-Rl1",
        "outputId": "a53860cf-b581-48cd-dd29-77965b3a8f9f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pronouns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] welcome both of you to the studio to par...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] well lots of barricade tape has been str...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] however the affected area was not as lar...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] for instance this evening we received an...</td>\n",
              "      <td>him</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] right . [SEP] hey [MASK] said this was q...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>[CLS] E6997 's only cryptic instructions on ch...</td>\n",
              "      <td>she</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[CLS] in kensington gardens [MASK] watches out...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[CLS] a far cry from the stealing of children ...</td>\n",
              "      <td>her</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[CLS] it is [MASK] who asks if E5324 and E719 ...</td>\n",
              "      <td>she</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[CLS] as for the child abuse i have to agree w...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1049 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text pronouns\n",
              "0     [CLS] welcome both of you to the studio to par...       he\n",
              "1     [CLS] well lots of barricade tape has been str...       he\n",
              "2     [CLS] however the affected area was not as lar...       he\n",
              "3     [CLS] for instance this evening we received an...      him\n",
              "4     [CLS] right . [SEP] hey [MASK] said this was q...       he\n",
              "...                                                 ...      ...\n",
              "1186  [CLS] E6997 's only cryptic instructions on ch...      she\n",
              "1187  [CLS] in kensington gardens [MASK] watches out...       he\n",
              "1188  [CLS] a far cry from the stealing of children ...      her\n",
              "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...      she\n",
              "1191  [CLS] as for the child abuse i have to agree w...       he\n",
              "\n",
              "[1049 rows x 2 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p3Wu-at2iWu"
      },
      "source": [
        "# save data to csv \n",
        "df.to_csv('data_final.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bo2wLaWY6ZF"
      },
      "source": [
        "# tokenizing the df \n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "df['tokenized'] = df.apply(lambda row: tokenizer(row['text']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pnx5zRLSZlMG",
        "outputId": "39f2917b-079d-4126-d4fd-dcd661499551"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pronouns</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] welcome both of you to the studio to par...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] well lots of barricade tape has been str...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] however the affected area was not as lar...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] for instance this evening we received an...</td>\n",
              "      <td>him</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] right . [SEP] hey [MASK] said this was q...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>[CLS] E6997 's only cryptic instructions on ch...</td>\n",
              "      <td>she</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[CLS] in kensington gardens [MASK] watches out...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[CLS] a far cry from the stealing of children ...</td>\n",
              "      <td>her</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[CLS] it is [MASK] who asks if E5324 and E719 ...</td>\n",
              "      <td>she</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[CLS] as for the child abuse i have to agree w...</td>\n",
              "      <td>he</td>\n",
              "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1049 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...                                    tokenized\n",
              "0     [CLS] welcome both of you to the studio to par...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "1     [CLS] well lots of barricade tape has been str...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "2     [CLS] however the affected area was not as lar...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "3     [CLS] for instance this evening we received an...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "4     [CLS] right . [SEP] hey [MASK] said this was q...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "...                                                 ...  ...                                          ...\n",
              "1186  [CLS] E6997 's only cryptic instructions on ch...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "1187  [CLS] in kensington gardens [MASK] watches out...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "1188  [CLS] a far cry from the stealing of children ...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "1191  [CLS] as for the child abuse i have to agree w...  ...  [input_ids, token_type_ids, attention_mask]\n",
              "\n",
              "[1049 rows x 3 columns]"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gTwKs5ga6b_"
      },
      "source": [
        "df.to_csv('data_tokenized.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy_mlH1IGUPk"
      },
      "source": [
        "# tokenizing text before (so aligns with config file)\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AIQxeWBdGb3y",
        "outputId": "abdabf51-5cc3-487a-d47f-93d3c4ec6e41"
      },
      "source": [
        "df = pd.read_csv('data_one_mask')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pronouns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] welcome both of you to the studio to par...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] well lots of barricade tape has been str...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] however the affected area was not as lar...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] for instance this evening we received an...</td>\n",
              "      <td>him</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] right . [SEP] hey [MASK] said this was q...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>[CLS] E6997 's only cryptic instructions on ch...</td>\n",
              "      <td>she</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>[CLS] in kensington gardens [MASK] watches out...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>[CLS] a far cry from the stealing of children ...</td>\n",
              "      <td>her</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>[CLS] it is [MASK] who asks if E5324 and E719 ...</td>\n",
              "      <td>she</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>[CLS] as for the child abuse i have to agree w...</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1049 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text pronouns\n",
              "0     [CLS] welcome both of you to the studio to par...       he\n",
              "1     [CLS] well lots of barricade tape has been str...       he\n",
              "2     [CLS] however the affected area was not as lar...       he\n",
              "3     [CLS] for instance this evening we received an...      him\n",
              "4     [CLS] right . [SEP] hey [MASK] said this was q...       he\n",
              "...                                                 ...      ...\n",
              "1186  [CLS] E6997 's only cryptic instructions on ch...      she\n",
              "1187  [CLS] in kensington gardens [MASK] watches out...       he\n",
              "1188  [CLS] a far cry from the stealing of children ...      her\n",
              "1189  [CLS] it is [MASK] who asks if E5324 and E719 ...      she\n",
              "1191  [CLS] as for the child abuse i have to agree w...       he\n",
              "\n",
              "[1049 rows x 2 columns]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URxaA6YAGoNJ"
      },
      "source": [
        "sentences = df.text.values\n",
        "labels = df.pronouns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "478986a2f793407f9e7486f9d964bcf5",
            "4edf7f4a913d4b5b9ccad0b2992f837a",
            "0eb9c572380a4975ab4657f875dd63c7",
            "3b3d9d11c8914f5f82aebe378d03bc4c",
            "6ea636c1dc7149ddb0914893754abff0",
            "27e97d90b55444eba726c8cf8930e15c",
            "8cce9f9bb66d4cdc947b757d7ed28593",
            "d6fd1a84899c4cf8b27e5beee8efd6f5",
            "e4a5268b6a054e55aafcab5332cc3686",
            "256ec5e4b7374fc29b2e2340c5275f77",
            "8e7ed01a44974ad3ac0faf68e539d4da",
            "9392e016ddb54941a5c2f1395e4da688",
            "f65a7d0d411945f98471604d938303a3",
            "5d28c66da01b4d5f8a4972306e65e9d2",
            "d9e2f98ed952487c9a7f42422bef471e",
            "30b3f7e99a144420b6e74e150b0bdda8",
            "c4c558ef28b04b0489d1f5f26e84806d",
            "e5f1fd12642543d0af133455182d156b",
            "3473b35e71944917b266ce6b8e28f678",
            "f4bf074cb42c4ccb8cb66d0290ab13e7",
            "c884bfd71ecc4229be3f124a9ed636a8",
            "19d3ab9089b844d39944ec440e4e7cc9",
            "7bade0270d7642288e6021d508edcbb1",
            "803afacd2c4d40c3894a900e52b216a2",
            "2deb768979be4391af174b041d622d21",
            "9a5cfcce65ba4fc4be389f055f1d51f8",
            "13b7c7e2cb694609b541b2cadfe983b8",
            "bed46b7773da4f37862ab1308ae4708a",
            "b08be842791244d19285ddf8805d967b",
            "f6f26b7f297a4af291ed155b8fe94392",
            "addd5f33fa094cfa973bfeba9fdaf78e",
            "a1aa1d4949c14fffa234cd6ce3e72bff",
            "832e1f25c2c64827bbc01a5fdd441e27"
          ]
        },
        "id": "ydsBPTErGzMj",
        "outputId": "c9b9a52a-1727-4acb-c7c1-ae8e3425cbe5"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "478986a2f793407f9e7486f9d964bcf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9392e016ddb54941a5c2f1395e4da688",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bade0270d7642288e6021d508edcbb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4qqDUc-G7js"
      },
      "source": [
        "Mask tokens:\n",
        "\n",
        "Create a mask which hides all tokens/words which do not correspond to the pronouns we seek to predict. This is used by the BERT model such that only predictions on the pronouns are considered when calculating the training loss. We mask the terms we wish to ignore with the token -100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1x0YXhEHANq",
        "outputId": "2580f515-6265-49cb-a98a-c2fc5a9e7af3"
      },
      "source": [
        "mask_id = tokenizer.convert_tokens_to_ids('[MASK]')\n",
        "mask_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "103"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHXoy_fUHEnD",
        "outputId": "6b186066-cff0-4fa5-e189-8e934aea55df"
      },
      "source": [
        "%%time\n",
        "masked_lm_labels = []\n",
        "for sentence, label in zip(sentences, labels):\n",
        "  sentence_ids =  tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n",
        "  label_id = tokenizer.convert_tokens_to_ids(label)\n",
        "  masked_lm_labels.append([label_id if id == mask_id else -100 for id in sentence_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 921 ms, sys: 0 ns, total: 921 ms\n",
            "Wall time: 918 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ15Ur6NHf7d",
        "outputId": "3af82f0e-42f9-426f-ad1e-553f80b4a656"
      },
      "source": [
        "%%time\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in list(sentences):\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = False, # Adds '[CLS]' and '[SEP]' if True\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  [CLS] welcome both of you to the studio to participate in our program . [SEP] well i especially want to know ha how the two of you found out the news on the day of the accident ? [SEP] ah about 11:00 m. yesterday ah i happened to find out through an sms when i was outside . [SEP] uh-huh . [SEP] uh-huh . [SEP] it happened that i was going to have lunch with a friend um at noon . [SEP] and then the friend first sent me an sms uh-huh . saying [MASK] would come pick me up to go together . [SEP]\n",
            "Token IDs: [101, 6160, 2119, 1997, 2017, 2000, 1996, 2996, 2000, 5589, 1999, 2256, 2565, 1012, 102, 2092, 1045, 2926, 2215, 2000, 2113, 5292, 2129, 1996, 2048, 1997, 2017, 2179, 2041, 1996, 2739, 2006, 1996, 2154, 1997, 1996, 4926, 1029, 102, 6289, 2055, 2340, 1024, 4002, 1049, 1012, 7483, 6289, 1045, 3047, 2000, 2424, 2041, 2083, 2019, 22434, 2043, 1045, 2001, 2648, 1012, 102, 7910, 1011, 9616, 1012, 102, 7910, 1011, 9616, 1012, 102, 2009, 3047, 2008, 1045, 2001, 2183, 2000, 2031, 6265, 2007, 1037, 2767, 8529, 2012, 11501, 1012, 102, 1998, 2059, 1996, 2767, 2034, 2741, 2033, 2019, 22434, 7910, 1011, 9616, 1012, 3038, 103, 2052, 2272, 4060, 2033, 2039, 2000, 2175, 2362, 1012, 102]\n",
            "CPU times: user 961 ms, sys: 0 ns, total: 961 ms\n",
            "Wall time: 958 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUsIGzxmH4Nt",
        "outputId": "20524224-a4c0-4385-d17f-b1d32fd8695f"
      },
      "source": [
        "# padding\n",
        "MAX_LEN = 140\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 150 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 148...\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "masked_lm_labels = pad_sequences(masked_lm_labels, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 140 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OltkjZiXH9ou"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWP_92h4IOEb",
        "outputId": "0b587acf-9657-4934-f566-14c6bf120a13"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 101, 6160, 2119, ...,    0,    0,    0],\n",
              "       [ 101, 2092, 7167, ...,    0,    0,    0],\n",
              "       [ 101, 2174, 1996, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 101, 1037, 2521, ...,    0,    0,    0],\n",
              "       [ 101, 2009, 2003, ...,    0,    0,    0],\n",
              "       [ 101, 2004, 2005, ...,    0,    0,    0]])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QBB_vZ4IQOL",
        "outputId": "3a1325f6-c132-4e3c-e692-c503005c0fe7"
      },
      "source": [
        "masked_lm_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2014, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2016, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2014, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2002, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, 2032, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, 2032, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, 2002, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2014, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2014, -100, -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2014, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2010, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2032, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2010, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2032, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, 2002, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2010, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2032, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2032, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2032, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2032, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2010, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2010, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2032, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, 2014, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, 2010, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2002, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, 2010, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2010, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, 2014, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2010, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, 2002, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2016, -100, -100, -100, -100],\n",
              " [-100, -100, -100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, 2002, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2002, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, 2016, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2014, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, 2016, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, -100, -100, -100, 2016, -100, -100],\n",
              " [-100, -100, -100, 2014, -100, -100, -100],\n",
              " [-100, -100, -100, 2016, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, 2014, -100, -100, -100],\n",
              " [-100, 2016, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2016, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2016, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100, 2016, -100, -100, -100, -100, -100, -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2002, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2016, -100, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2016, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, 2002, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, 2010, -100, -100, -100, -100, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100, -100, -100, 2010, -100, -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2010,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2014,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2016,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2002,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100],\n",
              " [-100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  -100,\n",
              "  2032,\n",
              "  -100,\n",
              "  -100],\n",
              " ...]"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}