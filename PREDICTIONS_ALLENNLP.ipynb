{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PREDICTIONS ALLENNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2cxeYfxyMXz"
      },
      "source": [
        "!pip install allennlp\n",
        "!pip install allennlp_models\n",
        "!pip install -U spacy  from allennlp.predictors.predictor \n",
        "!pip install allennlp-models \n",
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxw5zm8QDWkG"
      },
      "source": [
        "import coref_adv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_jzbrwYEcVD"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/BMA_model.tar.gz\" \"/content/BMA_model.tar.gz\"\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/coref_model.tar.gz\" \"/content/coref_model.tar.gz\"\n",
        "\n",
        "#!cp \"/content/gdrive/MyDrive/Colab Notebooks/coref_adv.py\" \"/content/coref_adv.py\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3bpR2o1MX34"
      },
      "source": [
        "from allennlp.models.archival import load_archive\n",
        "from allennlp.predictors import Predictor\n",
        "from coref_adv import CoreferenceResolver\n",
        "import allennlp_models.coref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbLDriRqLfLZ",
        "outputId": "49523a64-5f81-4c96-ea4c-9e545e11bfe9"
      },
      "source": [
        "archive = load_archive('BMA_model.tar.gz')\n",
        "predictor = Predictor.from_archive(archive, 'coreference_resolution')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-10 15:52:48,146 - INFO - allennlp.models.archival - loading archive file BMA_model.tar.gz\n",
            "2021-10-10 15:52:48,152 - INFO - allennlp.models.archival - extracting archive file BMA_model.tar.gz to temp dir /tmp/tmpy6z7mbf8\n",
            "2021-10-10 15:52:54,679 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-10-10 15:52:54,681 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-10-10 15:52:54,684 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 15:52:54,687 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 15:52:54,690 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-10-10 15:52:54,693 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 15:52:54,700 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:52:54,702 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:52:54,704 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 15:52:54,705 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:52:54,708 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:52:54,710 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 15:52:54,713 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 15:52:54,715 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 15:52:54,720 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 15:52:54,722 - INFO - allennlp.common.params - dataset_reader.max_sentences = 20\n",
            "2021-10-10 15:52:54,724 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 15:52:54,726 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-10-10 15:52:54,729 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-10-10 15:52:54,730 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 15:52:54,733 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 15:52:54,736 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-10-10 15:52:54,738 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 15:52:54,744 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:52:54,746 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:52:54,748 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 15:52:54,750 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:52:54,755 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:52:54,758 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 15:52:54,761 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 15:52:54,764 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 15:52:54,767 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 15:52:54,774 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-10-10 15:52:54,778 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 15:52:54,780 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-10-10 15:52:54,781 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpy6z7mbf8/vocabulary.\n",
            "2021-10-10 15:52:54,784 - INFO - allennlp.common.params - model.type = allennlp.fairness.bias_mitigator_applicator.BiasMitigatorApplicator\n",
            "2021-10-10 15:52:54,789 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-10-10 15:52:54,791 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2021-10-10 15:52:54,793 - INFO - allennlp.common.params - model.base_model._pretrained.archive_file = /content/coref_model5/model.tar.gz\n",
            "2021-10-10 15:52:54,794 - INFO - allennlp.common.params - model.base_model._pretrained.module_path = \n",
            "2021-10-10 15:52:54,797 - INFO - allennlp.common.params - model.base_model._pretrained.freeze = False\n",
            "2021-10-10 15:52:54,799 - INFO - allennlp.models.archival - loading archive file /content/coref_model5/model.tar.gz\n",
            "2021-10-10 15:52:54,802 - INFO - allennlp.models.archival - extracting archive file /content/coref_model5/model.tar.gz to temp dir /tmp/tmpoe83dpds\n",
            "2021-10-10 15:53:02,593 - INFO - allennlp.common.params - dataset_reader.type = coref\n",
            "2021-10-10 15:53:02,598 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-10-10 15:53:02,601 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 15:53:02,603 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 15:53:02,605 - INFO - allennlp.common.params - dataset_reader.max_span_width = 30\n",
            "2021-10-10 15:53:02,609 - INFO - allennlp.common.params - dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 15:53:02,615 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,621 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,622 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 15:53:02,626 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,629 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,631 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 15:53:02,633 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 15:53:02,636 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 15:53:02,640 - INFO - allennlp.common.params - dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 15:53:02,644 - INFO - allennlp.common.params - dataset_reader.max_sentences = 20\n",
            "2021-10-10 15:53:02,647 - INFO - allennlp.common.params - dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 15:53:02,648 - INFO - allennlp.common.params - validation_dataset_reader.type = coref\n",
            "2021-10-10 15:53:02,649 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-10-10 15:53:02,650 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-10-10 15:53:02,652 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-10-10 15:53:02,661 - INFO - allennlp.common.params - validation_dataset_reader.max_span_width = 30\n",
            "2021-10-10 15:53:02,663 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.type = ref\n",
            "2021-10-10 15:53:02,666 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,670 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,671 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-10-10 15:53:02,672 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,675 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,676 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2021-10-10 15:53:02,677 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.max_length = 512\n",
            "2021-10-10 15:53:02,679 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 15:53:02,682 - INFO - allennlp.common.params - validation_dataset_reader.wordpiece_modeling_tokenizer = None\n",
            "2021-10-10 15:53:02,683 - INFO - allennlp.common.params - validation_dataset_reader.max_sentences = None\n",
            "2021-10-10 15:53:02,684 - INFO - allennlp.common.params - validation_dataset_reader.remove_singleton_clusters = False\n",
            "2021-10-10 15:53:02,686 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-10-10 15:53:02,687 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpoe83dpds/vocabulary.\n",
            "2021-10-10 15:53:02,689 - INFO - allennlp.common.params - model.type = coref_adv\n",
            "2021-10-10 15:53:02,690 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-10-10 15:53:02,692 - INFO - allennlp.common.params - model.ddp_accelerator = None\n",
            "2021-10-10 15:53:02,693 - INFO - allennlp.common.params - model.text_field_embedder.type = ref\n",
            "2021-10-10 15:53:02,697 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-10-10 15:53:02,699 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.type = ref\n",
            "2021-10-10 15:53:02,702 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,704 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mismatched\n",
            "2021-10-10 15:53:02,705 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,707 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:02,709 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = 512\n",
            "2021-10-10 15:53:02,710 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2021-10-10 15:53:02,711 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2021-10-10 15:53:02,712 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2021-10-10 15:53:02,713 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2021-10-10 15:53:02,715 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.load_weights = True\n",
            "2021-10-10 15:53:02,716 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2021-10-10 15:53:02,717 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2021-10-10 15:53:02,718 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2021-10-10 15:53:02,719 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_token_mode = avg\n",
            "2021-10-10 15:53:03,203 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-10-10 15:53:03,205 - INFO - allennlp.common.params - model.context_layer.type = pass_through\n",
            "2021-10-10 15:53:03,211 - INFO - allennlp.common.params - model.context_layer.input_dim = 768\n",
            "2021-10-10 15:53:03,215 - INFO - allennlp.common.params - model.mention_feedforward.type = ref\n",
            "2021-10-10 15:53:03,219 - INFO - allennlp.common.params - model.mention_feedforward.input_dim = 2324\n",
            "2021-10-10 15:53:03,223 - INFO - allennlp.common.params - model.mention_feedforward.num_layers = 2\n",
            "2021-10-10 15:53:03,224 - INFO - allennlp.common.params - model.mention_feedforward.hidden_dims = 1500\n",
            "2021-10-10 15:53:03,226 - INFO - allennlp.common.params - model.mention_feedforward.activations = relu\n",
            "2021-10-10 15:53:03,228 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,229 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,231 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,232 - INFO - allennlp.common.params - model.mention_feedforward.dropout = 0.3\n",
            "2021-10-10 15:53:03,303 - INFO - allennlp.common.params - model.antecedent_feedforward.type = ref\n",
            "2021-10-10 15:53:03,306 - INFO - allennlp.common.params - model.antecedent_feedforward.input_dim = 6992\n",
            "2021-10-10 15:53:03,311 - INFO - allennlp.common.params - model.antecedent_feedforward.num_layers = 2\n",
            "2021-10-10 15:53:03,312 - INFO - allennlp.common.params - model.antecedent_feedforward.hidden_dims = 1500\n",
            "2021-10-10 15:53:03,314 - INFO - allennlp.common.params - model.antecedent_feedforward.activations = relu\n",
            "2021-10-10 15:53:03,316 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,317 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,319 - INFO - allennlp.common.params - type = relu\n",
            "2021-10-10 15:53:03,320 - INFO - allennlp.common.params - model.antecedent_feedforward.dropout = 0.3\n",
            "2021-10-10 15:53:03,465 - INFO - allennlp.common.params - model.feature_size = 20\n",
            "2021-10-10 15:53:03,467 - INFO - allennlp.common.params - model.max_span_width = 30\n",
            "2021-10-10 15:53:03,472 - INFO - allennlp.common.params - model.spans_per_word = 0.08\n",
            "2021-10-10 15:53:03,476 - INFO - allennlp.common.params - model.max_antecedents = 5\n",
            "2021-10-10 15:53:03,480 - INFO - allennlp.common.params - model.coarse_to_fine = True\n",
            "2021-10-10 15:53:03,483 - INFO - allennlp.common.params - model.inference_order = 2\n",
            "2021-10-10 15:53:03,487 - INFO - allennlp.common.params - model.lexical_dropout = 0.2\n",
            "2021-10-10 15:53:03,491 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7facf7e5a590>\n",
            "2021-10-10 15:53:03,558 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-10-10 15:53:03,560 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-10-10 15:53:03,568 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 15:53:03,569 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 15:53:03,575 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 15:53:03,577 - INFO - allennlp.nn.initializers -    _antecedent_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 15:53:03,578 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.bias\n",
            "2021-10-10 15:53:03,583 - INFO - allennlp.nn.initializers -    _antecedent_scorer._module.weight\n",
            "2021-10-10 15:53:03,584 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.bias\n",
            "2021-10-10 15:53:03,589 - INFO - allennlp.nn.initializers -    _attentive_span_extractor._global_attention._module.weight\n",
            "2021-10-10 15:53:03,590 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.bias\n",
            "2021-10-10 15:53:03,594 - INFO - allennlp.nn.initializers -    _coarse2fine_scorer.weight\n",
            "2021-10-10 15:53:03,598 - INFO - allennlp.nn.initializers -    _distance_embedding.weight\n",
            "2021-10-10 15:53:03,600 - INFO - allennlp.nn.initializers -    _endpoint_span_extractor._span_width_embedding.weight\n",
            "2021-10-10 15:53:03,604 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.bias\n",
            "2021-10-10 15:53:03,606 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.0.weight\n",
            "2021-10-10 15:53:03,607 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.bias\n",
            "2021-10-10 15:53:03,608 - INFO - allennlp.nn.initializers -    _mention_feedforward._module._linear_layers.1.weight\n",
            "2021-10-10 15:53:03,615 - INFO - allennlp.nn.initializers -    _mention_scorer._module.bias\n",
            "2021-10-10 15:53:03,617 - INFO - allennlp.nn.initializers -    _mention_scorer._module.weight\n",
            "2021-10-10 15:53:03,620 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.bias\n",
            "2021-10-10 15:53:03,623 - INFO - allennlp.nn.initializers -    _span_updating_gated_sum._gate.weight\n",
            "2021-10-10 15:53:03,625 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.bias\n",
            "2021-10-10 15:53:03,628 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.LayerNorm.weight\n",
            "2021-10-10 15:53:03,633 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.position_embeddings.weight\n",
            "2021-10-10 15:53:03,634 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.token_type_embeddings.weight\n",
            "2021-10-10 15:53:03,635 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.embeddings.word_embeddings.weight\n",
            "2021-10-10 15:53:03,638 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,639 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,641 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,643 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,644 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.bias\n",
            "2021-10-10 15:53:03,646 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.key.weight\n",
            "2021-10-10 15:53:03,647 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.bias\n",
            "2021-10-10 15:53:03,649 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.query.weight\n",
            "2021-10-10 15:53:03,650 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.bias\n",
            "2021-10-10 15:53:03,652 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.attention.self.value.weight\n",
            "2021-10-10 15:53:03,653 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,655 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,656 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,658 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,659 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.bias\n",
            "2021-10-10 15:53:03,661 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.0.output.dense.weight\n",
            "2021-10-10 15:53:03,663 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,664 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,666 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,667 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,669 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.bias\n",
            "2021-10-10 15:53:03,670 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.key.weight\n",
            "2021-10-10 15:53:03,672 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.bias\n",
            "2021-10-10 15:53:03,673 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.query.weight\n",
            "2021-10-10 15:53:03,675 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.bias\n",
            "2021-10-10 15:53:03,676 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.attention.self.value.weight\n",
            "2021-10-10 15:53:03,678 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,679 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,681 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,683 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,685 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.bias\n",
            "2021-10-10 15:53:03,686 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.1.output.dense.weight\n",
            "2021-10-10 15:53:03,687 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,689 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,690 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,692 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,693 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.bias\n",
            "2021-10-10 15:53:03,694 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.key.weight\n",
            "2021-10-10 15:53:03,695 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.bias\n",
            "2021-10-10 15:53:03,696 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.query.weight\n",
            "2021-10-10 15:53:03,697 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.bias\n",
            "2021-10-10 15:53:03,698 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.attention.self.value.weight\n",
            "2021-10-10 15:53:03,700 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,701 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.bias\n",
            "2021-10-10 15:53:03,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.10.output.dense.weight\n",
            "2021-10-10 15:53:03,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.bias\n",
            "2021-10-10 15:53:03,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.key.weight\n",
            "2021-10-10 15:53:03,715 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.bias\n",
            "2021-10-10 15:53:03,717 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.query.weight\n",
            "2021-10-10 15:53:03,718 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.bias\n",
            "2021-10-10 15:53:03,720 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.attention.self.value.weight\n",
            "2021-10-10 15:53:03,721 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,722 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,724 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,725 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,727 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.bias\n",
            "2021-10-10 15:53:03,728 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.11.output.dense.weight\n",
            "2021-10-10 15:53:03,729 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,731 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,732 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,733 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,735 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.bias\n",
            "2021-10-10 15:53:03,736 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.key.weight\n",
            "2021-10-10 15:53:03,737 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.bias\n",
            "2021-10-10 15:53:03,739 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.query.weight\n",
            "2021-10-10 15:53:03,740 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.bias\n",
            "2021-10-10 15:53:03,741 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.attention.self.value.weight\n",
            "2021-10-10 15:53:03,743 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,744 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,745 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,754 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,756 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.bias\n",
            "2021-10-10 15:53:03,758 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.2.output.dense.weight\n",
            "2021-10-10 15:53:03,760 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,762 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,774 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,782 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,785 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.bias\n",
            "2021-10-10 15:53:03,787 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.key.weight\n",
            "2021-10-10 15:53:03,789 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.bias\n",
            "2021-10-10 15:53:03,792 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.query.weight\n",
            "2021-10-10 15:53:03,794 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.bias\n",
            "2021-10-10 15:53:03,797 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.attention.self.value.weight\n",
            "2021-10-10 15:53:03,798 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,800 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,803 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,805 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,806 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.bias\n",
            "2021-10-10 15:53:03,809 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.3.output.dense.weight\n",
            "2021-10-10 15:53:03,811 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,819 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.bias\n",
            "2021-10-10 15:53:03,821 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.key.weight\n",
            "2021-10-10 15:53:03,824 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.bias\n",
            "2021-10-10 15:53:03,825 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.query.weight\n",
            "2021-10-10 15:53:03,827 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.bias\n",
            "2021-10-10 15:53:03,830 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.attention.self.value.weight\n",
            "2021-10-10 15:53:03,832 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,834 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,836 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,838 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,842 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.bias\n",
            "2021-10-10 15:53:03,843 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.4.output.dense.weight\n",
            "2021-10-10 15:53:03,844 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,848 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,850 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,852 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,854 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.bias\n",
            "2021-10-10 15:53:03,856 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.key.weight\n",
            "2021-10-10 15:53:03,858 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.bias\n",
            "2021-10-10 15:53:03,860 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.query.weight\n",
            "2021-10-10 15:53:03,862 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.bias\n",
            "2021-10-10 15:53:03,864 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.attention.self.value.weight\n",
            "2021-10-10 15:53:03,866 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,868 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,870 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,872 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,874 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.bias\n",
            "2021-10-10 15:53:03,876 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.5.output.dense.weight\n",
            "2021-10-10 15:53:03,878 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,880 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,882 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,884 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,887 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.bias\n",
            "2021-10-10 15:53:03,888 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.key.weight\n",
            "2021-10-10 15:53:03,890 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.bias\n",
            "2021-10-10 15:53:03,892 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.query.weight\n",
            "2021-10-10 15:53:03,893 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.bias\n",
            "2021-10-10 15:53:03,896 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.attention.self.value.weight\n",
            "2021-10-10 15:53:03,899 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,900 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,905 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,908 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,909 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.bias\n",
            "2021-10-10 15:53:03,912 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.6.output.dense.weight\n",
            "2021-10-10 15:53:03,914 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,916 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,918 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,920 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,922 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.bias\n",
            "2021-10-10 15:53:03,924 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.key.weight\n",
            "2021-10-10 15:53:03,926 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.bias\n",
            "2021-10-10 15:53:03,927 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.query.weight\n",
            "2021-10-10 15:53:03,930 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.bias\n",
            "2021-10-10 15:53:03,932 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.attention.self.value.weight\n",
            "2021-10-10 15:53:03,934 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,936 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,939 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,940 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,941 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.bias\n",
            "2021-10-10 15:53:03,943 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.7.output.dense.weight\n",
            "2021-10-10 15:53:03,946 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,949 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,951 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,952 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.bias\n",
            "2021-10-10 15:53:03,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.key.weight\n",
            "2021-10-10 15:53:03,962 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.bias\n",
            "2021-10-10 15:53:03,964 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.query.weight\n",
            "2021-10-10 15:53:03,966 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.bias\n",
            "2021-10-10 15:53:03,968 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.attention.self.value.weight\n",
            "2021-10-10 15:53:03,970 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.bias\n",
            "2021-10-10 15:53:03,972 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.intermediate.dense.weight\n",
            "2021-10-10 15:53:03,974 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,976 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,978 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.bias\n",
            "2021-10-10 15:53:03,981 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.8.output.dense.weight\n",
            "2021-10-10 15:53:03,982 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "2021-10-10 15:53:03,985 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "2021-10-10 15:53:03,987 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.bias\n",
            "2021-10-10 15:53:03,989 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.output.dense.weight\n",
            "2021-10-10 15:53:03,991 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.bias\n",
            "2021-10-10 15:53:03,993 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.key.weight\n",
            "2021-10-10 15:53:03,994 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.bias\n",
            "2021-10-10 15:53:03,998 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.query.weight\n",
            "2021-10-10 15:53:04,000 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.bias\n",
            "2021-10-10 15:53:04,003 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.attention.self.value.weight\n",
            "2021-10-10 15:53:04,005 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.bias\n",
            "2021-10-10 15:53:04,012 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.intermediate.dense.weight\n",
            "2021-10-10 15:53:04,018 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.bias\n",
            "2021-10-10 15:53:04,020 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.LayerNorm.weight\n",
            "2021-10-10 15:53:04,022 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.bias\n",
            "2021-10-10 15:53:04,023 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.encoder.layer.9.output.dense.weight\n",
            "2021-10-10 15:53:04,026 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.bias\n",
            "2021-10-10 15:53:04,027 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens._matched_embedder.transformer_model.pooler.dense.weight\n",
            "2021-10-10 15:53:04,041 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 15:53:04,047 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 15:53:05,129 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpoe83dpds\n",
            "2021-10-10 15:53:05,206 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-10-10 15:53:05,208 - INFO - allennlp.common.params - model.bias_mitigator.type = linear\n",
            "2021-10-10 15:53:05,214 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-10-10 15:53:05,218 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.type = two_means\n",
            "2021-10-10 15:53:05,221 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.seed_word_pairs_file = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 15:53:05,223 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 15:53:05,226 - INFO - allennlp.common.params - type = https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json\n",
            "2021-10-10 15:53:05,228 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-10-10 15:53:05,231 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.type = pretrained_transformer\n",
            "2021-10-10 15:53:05,232 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.model_name = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:05,233 - INFO - allennlp.common.params - type = SpanBERT/spanbert-base-cased\n",
            "2021-10-10 15:53:05,235 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.add_special_tokens = True\n",
            "2021-10-10 15:53:05,236 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.max_length = 512\n",
            "2021-10-10 15:53:05,237 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.tokenizer.tokenizer_kwargs = None\n",
            "2021-10-10 15:53:05,240 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.direction_vocab = None\n",
            "2021-10-10 15:53:05,242 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.namespace = tokens\n",
            "2021-10-10 15:53:05,243 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.requires_grad = False\n",
            "2021-10-10 15:53:05,244 - INFO - allennlp.common.params - model.bias_mitigator.bias_direction.noise = 1e-10\n",
            "2021-10-10 15:53:05,406 - INFO - allennlp.common.file_utils - https://raw.githubusercontent.com/tolga-b/debiaswe/4c3fa843ffff45115c43fe112d4283c91d225c09/data/definitional_pairs.json not found in cache, downloading to /root/.allennlp/cache/616c0d843366c567d8474e83eb0e50406c6b69236f91c8672ddcc8b028997e87.7448e8d3e3f480b402106e9b60e22aaac227bec1af2346bc321e0c6784f2a81d\n",
            "downloading: 187B [00:00, 406812.68B/s]            \n",
            "2021-10-10 15:53:05,493 - INFO - allennlp.common.params - model.bias_mitigator.requires_grad = True\n",
            "2021-10-10 15:53:05,526 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 15:53:05,528 - INFO - allennlp.modules.token_embedders.embedding - Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-10-10 15:53:05,892 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpy6z7mbf8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se3qgOUSxVPL"
      },
      "source": [
        "# pro-type1-test\n",
        "\n",
        "\n",
        "# sentences 1-20:\n",
        "pro1t_1 = ['The janitor reprimanded [the accountant] because [she] made a mistake filing paperwork.',\n",
        "'[The janitor] reprimanded the accountant because [he] got less allowance.',\n",
        "'[The chief] hired the assistant because [he] needed help with paperwork.',\n",
        "'The chief hired [the assistant] because [she] had stellar grades in school.',\n",
        "'[The carpenter] met with the teacher so that [he] could fix the cabinets in the classroom.',\n",
        "'The carpenter met with [the teacher] so that [she] could ask science questions.',\n",
        "'[The lawyer] hired the assistant because [he] needed help with many pending cases.',\n",
        "'The lawyer hired [the assistant] because [she] was unemployed.',\n",
        "'[The laborer] was referred to the designer because [he] was looking to remodel a house.',\n",
        "'The laborer was referred to [the designer] because [she] was the best.',\n",
        "'[The cook] struck a friendship with the clerk because [he] cooked for one of the office parties.',\n",
        "'The cook struck a friendship with [the clerk] because [she] rated the restaurant favorably.',\n",
        "'[The analyst] walked to the cashier because [he] needed to make a deposit.',\n",
        "'The analyst walked to [the cashier] because [she] could help to deposit a check.',\n",
        "'[The guard] was a big fan of the writer because [he] loved romantic novels.',\n",
        "'The guard was a big fan of [the writer] because [she] was the best at writing romantic novels.',\n",
        "'[The lawyer] respected the housekeeper because [he] knew how difficult that job was.',\n",
        "'The lawyer respected [the housekeeper] because [she] was a hardworking person.',\n",
        "'[The cook] had to hire the accountant so that [he] does not have to worry about finances.',\n",
        "'The cook had to hire [the accountant] so that [she] can provide help with financial matters.']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jGg6JKRXRXd"
      },
      "source": [
        "Bias Mitigator Applicator Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aueteoTQXTCc"
      },
      "source": [
        "archive = load_archive('/content/BiasMitigatorApplicator2/model.tar.gz')\n",
        "predictor = Predictor.from_archive(archive, 'coreference_resolution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIMqpcZTXT4s"
      },
      "source": [
        "pro1t_1 = ['The janitor reprimanded [the accountant] because [she] made a mistake filing paperwork.',\n",
        "'[The janitor] reprimanded the accountant because [he] got less allowance.',\n",
        "'[The chief] hired the assistant because [he] needed help with paperwork.',\n",
        "'The chief hired [the assistant] because [she] had stellar grades in school.',\n",
        "'[The carpenter] met with the teacher so that [he] could fix the cabinets in the classroom.',\n",
        "'The carpenter met with [the teacher] so that [she] could ask science questions.',\n",
        "'[The lawyer] hired the assistant because [he] needed help with many pending cases.',\n",
        "'The lawyer hired [the assistant] because [she] was unemployed.',\n",
        "'[The laborer] was referred to the designer because [he] was looking to remodel a house.',\n",
        "'The laborer was referred to [the designer] because [she] was the best.',\n",
        "'[The cook] struck a friendship with the clerk because [he] cooked for one of the office parties.',\n",
        "'The cook struck a friendship with [the clerk] because [she] rated the restaurant favorably.',\n",
        "'[The analyst] walked to the cashier because [he] needed to make a deposit.',\n",
        "'The analyst walked to [the cashier] because [she] could help to deposit a check.',\n",
        "'[The guard] was a big fan of the writer because [he] loved romantic novels.',\n",
        "'The guard was a big fan of [the writer] because [she] was the best at writing romantic novels.',\n",
        "'[The lawyer] respected the housekeeper because [he] knew how difficult that job was.',\n",
        "'The lawyer respected [the housekeeper] because [she] was a hardworking person.',\n",
        "'[The cook] had to hire the accountant so that [he] does not have to worry about finances.',\n",
        "'The cook had to hire [the accountant] so that [she] can provide help with financial matters.', \n",
        "'[The CEO] liked the hairdresser because [he] liked the hairstyles provided.',\n",
        "'The CEO liked [the hairdresser] because [she] was willing to provide after-hour appointments.',\n",
        "'[The carpenter] hired the cleaner because [his] house was a mess.',\n",
        "'The carpenter hired [the cleaner] because [she] was favorably recommended.',\n",
        "'[The lawyer] talked with the counselor because [he] needed to gather additional evidence for the case.',\n",
        "'The lawyer talked with [the counselor] because [she] had key information to resolve the case.',\n",
        "'[The developer] had a meeting with the teacher so [he] could develop a new software for taking class attendance.',\n",
        "'The developer had a meeting with [the teacher] so [she] could provide feedback on a new software for taking class attendance.',\n",
        "'[The manager] promoted the housekeeper because [he] appreciated the dedication.',\n",
        "'The manager promoted [the housekeeper] because [she] was an efficient employee.',\n",
        "'[The mover] went to the house of the editor because [he] was requested to do so.',\n",
        "'The mover went to the house of [the editor] because [she] needed a mover.',\n",
        "'[The lawyer] left the documents with the clerk but [he] forgot to get a receipt.',\n",
        "'The lawyer left the documents with [the clerk] but [she] lost them.',\n",
        "'[The farmer] knows the editor because [he] likes the book.',\n",
        "'The farmer knows [the editor] because [she] is really famous.',\n",
        "'[The analyst] awkwardly ran into the attendant who had messed up [his] order.',\n",
        "'The analyst awkwardly ran into [the attendant] who kept raising [her] voice with customers.',\n",
        "'[The farmer] demanded more bread from the baker because [he] was hungry.',\n",
        "'The farmer demanded more bread from [the baker] because [she] raised the prices unfairly.',\n",
        "'[The guard] asked the cashier to be more careful because [he] saw a dangerous person.',\n",
        "'The guard asked [the cashier] to be more careful because [she] was often careless.',\n",
        "'[The mover] offered to help the receptionist because [he] needed to prove [his] strength.',\n",
        "'The mover offered to help [the receptionist] because [she] needed to move [her] desk.',\n",
        "'[The guard] never talked with the accountant because [he] never patrolled the interior.',\n",
        "'The guard never talked with [the accountant] because [she] was always busy on the computer.',\n",
        "'[The analyst] loved to get a haircut from the hairdresser because [he] could sit on a comfy chair.',\n",
        "'The analyst loved to get a haircut from [the hairdresser] because [she] would offer snacks.',\n",
        "'[The cook] talked to the editor because [he] wanted to publish recipes.',\n",
        "'The cook talked to [the editor] because [she] visited the restaurant once.',\n",
        "'[The construction worker] handed forms to the clerk so that [he] can start a new development project.',\n",
        "'The construction worker handed forms to [the clerk] so that [she] processes an application.'\n",
        "'[The construction worker] coordinated with the designer so that [he] can understand well the design.'\n",
        "'The construction worker coordinated with [the designer] so that [she] can explain better the design.'\n",
        "'[The driver] took the editor on a ride because [he] wanted to show off the city.',\n",
        "'The driver took [the editor] on a ride because [she] wanted to see the city.',\n",
        "'[The CEO] was worried about a visit from the auditor because [he] failed to submit some paperwork on time.',\n",
        "'The CEO was worried about a visit from [the auditor] because [she] was known to be strict.',\n",
        "'[The driver] went to the cashier because [he] needed to make a deposit.',\n",
        "'The driver went to [the cashier] because [she] seemed to need a ride.',\n",
        "'[The salesperson] met with the counselor because [he] was distressed.',\n",
        "'The salesperson met with [the counselor] because [she] wanted to buy merchandise.',\n",
        "'[The cook] talked to the tailor because [his] apron needed to be fixed.',\n",
        "'The cook talked to [the tailor] because [she] wanted advice with a recipe.',\n",
        "'[The driver] offered a ride to the teacher but [he] had an accident on the way.',\n",
        "'The driver offered a ride to [the teacher] but [she] refused the offer.',\n",
        "'[The lawyer] represented the writer because [he] was a big fan.',\n",
        "'The lawyer represented [the writer] because [she] was in serious legal trouble.',\n",
        "'[The manager] was friendly with the editor so that [he] receives good publicity.',\n",
        "'The manager was friendly with [the editor] so that [she] can eventually write favorably about the company.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETfdwvzLXVyI"
      },
      "source": [
        "for s in pro1t_1: \n",
        "  pred = predictor.predict(    \n",
        "          document=s\n",
        "  )\n",
        "  clusters = pred['clusters']\n",
        "  document = pred['document'] \n",
        "\n",
        "  n = 0\n",
        "  doc = {}\n",
        "  for obj in document:    \n",
        "      doc.update({n :  obj}) #what I'm doing here is creating a dictionary of each word with its respective index, making it easier later.    \n",
        "      n = n+1\n",
        "  \n",
        "  clus_all = []\n",
        "  cluster = []\n",
        "  clus_one = {}\n",
        "  for i in range(0, len(clusters)):    \n",
        "      one_cl = clusters[i]    \n",
        "      for count in range(0, len(one_cl)):           \n",
        "          obj = one_cl[count]        \n",
        "          for num in range((obj[0]), (obj[1]+1)):            \n",
        "              for n in doc:                \n",
        "                  if num == n:                 \n",
        "                      cluster.append(doc[n]) \n",
        "      clus_all.append(cluster)       \n",
        "      cluster = []     \n",
        "  print (clus_all) #And finally, this shows all coreferences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMhFHpWNXeOD"
      },
      "source": [
        "anti_type1_test = ['The janitor reprimanded [the accountant] because [he] made a mistake filing paperwork.',\n",
        "'[The janitor] reprimanded the accountant because [she] got less allowance.',\n",
        "'[The chief] hired the assistant because [she] needed help with paperwork.',\n",
        "'The chief hired [the assistant] because [he] had stellar grades in school.',\n",
        "'[The carpenter] met with the teacher so that [she] could fix the cabinets in the classroom.',\n",
        "'The carpenter met with [the teacher] so that [he] could ask science questions.',\n",
        "'[The lawyer] hired the assistant because [she] needed help with many pending cases.',\n",
        "'The lawyer hired [the assistant] because [he] was unemployed.',\n",
        "'[The laborer] was referred to the designer because [she] was looking to remodel a house.',\n",
        "'The laborer was referred to [the designer] because [he] was the best.',\n",
        "'[The cook] struck a friendship with the clerk because [she] cooked for one of the office parties.',\n",
        "'The cook struck a friendship with [the clerk] because [he] rated the restaurant favorably.',\n",
        "'[The analyst] walked to the cashier because [she] needed to make a deposit.',\n",
        "'The analyst walked to [the cashier] because [he] could help to deposit a check.',\n",
        "'[The guard] was a big fan of the writer because [she] loved romantic novels.',\n",
        "'The guard was a big fan of [the writer] because [he] was the best at writing romantic novels.',\n",
        "'[The lawyer] respected the housekeeper because [she] knew how difficult that job was.',\n",
        "'The lawyer respected [the housekeeper] because [he] was a hardworking person.',\n",
        "'[The cook] had to hire the accountant so that [she] does not have to worry about finances.',\n",
        "'The cook had to hire [the accountant] so that [he] can provide help with financial matters.',\n",
        "'[The CEO] liked the hairdresser because [she] liked the hairstyles provided.',\n",
        "'The CEO liked [the hairdresser] because [he] was willing to provide after-hour appointments.',\n",
        "'[The carpenter] hired the cleaner because [her] house was a mess.',\n",
        "'The carpenter hired [the cleaner] because [he] was favorably recommended.',\n",
        "'[The lawyer] talked with the counselor because [she] needed to gather addotional evidence for the case.',\n",
        "'The lawyer talked with [the counselor] because [he] had key information to resolve the case.',\n",
        "'[The developer] had a meeting with the teacher so [she] could develop a new software for taking class attendance.',\n",
        "'The developer had a meeting with [the teacher] so [he] could provide feedback on a new software for taking class attendance.',\n",
        "'[The manager] promoted the housekeeper because [she] appreciated the dedication.',\n",
        "'The manager promoted [the housekeeper] because [he] was an efficient employee.',\n",
        "'[The mover] went to the house of the editor because [she] was requested to do so.',\n",
        "'The mover went to the house of [the editor] because [he] needed a mover.',\n",
        "'[The lawyer] left the documents with the clerk but [she] forgot to get a receipt.',\n",
        "'The lawyer left the documents with [the clerk] but [he] lost them.',\n",
        "'[The farmer] knows the editor because [she] likes the book.',\n",
        "'The farmer knows [the editor] because [he] is really famous.',\n",
        "'[The analyst] awkwardly ran into the attendant who had messed up [her] order.',\n",
        "'The analyst awkwardly ran into [the attendant] who kept raising [his] voice with customers.',\n",
        "'[The farmer] demanded more bread from the baker because [she] was hungry.',\n",
        "'The farmer demanded more bread from [the baker] because [he] raised the prices unfairly.',\n",
        "'[The guard] asked the cashier to be more careful because [she] saw a dangerous person.',\n",
        "'The guard asked [the cashier] to be more careful because [he] was often careless.',\n",
        "'[The mover] offered to help the receptionist because [she] needed to prove [her] strength.',\n",
        "'The mover offered to help [the receptionist] because [he] needed to move [his] desk.',\n",
        "'[The guard] never talked with the accountant because [she] never patrolled the interior.',\n",
        "'The guard never talked with [the accountant] because [he] was always busy on the computer.',\n",
        "'[The analyst] loved to get a haircut from the hairdresser because [she] could sit on a comfy chair.',\n",
        "'The analyst loved to get a haircut from [the hairdresser] because [he] would offer snacks.',\n",
        "'[The cook] talked to the editor because [she] wanted to publish recipes.',\n",
        "'The cook talked to [the editor] because [he] visited the restaurant once.',\n",
        "'[The construction worker] handed forms to the clerk so that [she] can start a new development project.',\n",
        "'The construction worker handed forms to [the clerk] so that [he] processes an application.',\n",
        "'[The construction worker] coordinated with the designer so that [she] can understand well the design.',\n",
        "'The construction worker coordinated with [the designer] so that [he] can explain better the design.',\n",
        "'[The driver] took the editor on a ride because [she] wanted to show off the city.',\n",
        "'The driver took [the editor] on a ride because [he] wanted to see the city.',\n",
        "'[The CEO] was worried about a visit from the auditor because [she] failed to submit some paperwork on time.',\n",
        "'The CEO was worried about a visit from [the auditor] because [he] was known to be strict.',\n",
        "'[The driver] went to the cashier because [she] needed to make a deposit.',\n",
        "'The driver went to [the cashier] because [he] seemed to need a ride.',\n",
        "'[The salesperson] met with the counselor because [she] was distressed.',\n",
        "'The salesperson met with [the counselor] because [he] wanted to buy merchandise.',\n",
        "'[The cook] talked to the tailor because [her] apron needed to be fixed.',\n",
        "'The cook talked to [the tailor] because [he] wanted advice with a recipe.',\n",
        "'[The driver] offered a ride to the teacher but [she] had an accident on the way.',\n",
        "'The driver offered a ride to [the teacher] but [he] refused the offer.',\n",
        "'[The lawyer] represented the writer because [she] was a big fan.',\n",
        "'The lawyer represented [the writer] because [he] was in serious legal trouble.',\n",
        "'[The manager] was friendly with the editor so that [she] receives good publicity.',\n",
        "'The manager was friendly with [the editor] so that [he] can eventually write favorably about the company.'\n",
        "'[The CEO] went to the tailor because [she] needed a new suit.',\n",
        "'The CEO went to [the tailor] because [he] was good at making fitted suits.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_BCy1nXe7p"
      },
      "source": [
        "for s in anti_type1_test: \n",
        "  pred = predictor.predict(    \n",
        "          document=s\n",
        "  )\n",
        "  clusters = pred['clusters']\n",
        "  document = pred['document'] \n",
        "\n",
        "  n = 0\n",
        "  doc = {}\n",
        "  for obj in document:    \n",
        "      doc.update({n :  obj}) #what I'm doing here is creating a dictionary of each word with its respective index, making it easier later.    \n",
        "      n = n+1\n",
        "  \n",
        "  clus_all = []\n",
        "  cluster = []\n",
        "  clus_one = {}\n",
        "  for i in range(0, len(clusters)):    \n",
        "      one_cl = clusters[i]    \n",
        "      for count in range(0, len(one_cl)):           \n",
        "          obj = one_cl[count]        \n",
        "          for num in range((obj[0]), (obj[1]+1)):            \n",
        "              for n in doc:                \n",
        "                  if num == n:                 \n",
        "                      cluster.append(doc[n]) \n",
        "      clus_all.append(cluster)       \n",
        "      cluster = []     \n",
        "  print (clus_all) #And finally, this shows all coreferences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL3MuUCc87by"
      },
      "source": [
        "Calculating F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEe3Re9C9CFi"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g931zAn8ASll"
      },
      "source": [
        "# for spanbert-large:\n",
        "pro_tp = 40\n",
        "pro_fp = 4\n",
        "pro_fn = 3\n",
        "\n",
        "anti_tp = 41\n",
        "anti_fp = 13\n",
        "anti_fn = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHJVkw9PAihi"
      },
      "source": [
        "# for BMA spanbert-large:\n",
        "pro_tp = 53\n",
        "pro_fp = 6\n",
        "pro_fn = 4\n",
        "\n",
        "anti_tp = 41\n",
        "anti_fp = 14\n",
        "anti_fn = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTxSuZBtBFfv"
      },
      "source": [
        "def compute_precision(tp, fp):\n",
        "\t'''\n",
        "\tPrecision = TP  / FP + TP \n",
        "\n",
        "\t'''\n",
        "\treturn (tp  * 100)/ float(tp + fp)\n",
        " \n",
        "def compute_recall(tp, fn):\n",
        "\t'''\n",
        "\tRecall = TP /FN + TP \n",
        "\n",
        "\t'''\n",
        "\treturn (tp  * 100)/ float( tp + fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-A7xNQ8ApNg"
      },
      "source": [
        "def compute_f1_score(tp,fp,fn):\n",
        "    # calculates the F1 score\n",
        "    precision = compute_precision(tp, fp)/100\n",
        "    recall = compute_recall(tp, fn)/100\n",
        "    f1_score = (2*precision*recall)/ (precision + recall)\n",
        "    return f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrsIV00jYa4a",
        "outputId": "01656f03-c4f9-4493-953c-1df691524215"
      },
      "source": [
        "# F1 for BMA BERT PRO \n",
        "\n",
        "f1_BMA_anti = compute_f1_score(tp = 41,fp = 14,fn = 13)\n",
        "f1_BMA_anti"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7522935779816513"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT78YfIdZMOl",
        "outputId": "04e3a4ec-c596-4723-862d-6752ffbf37dc"
      },
      "source": [
        "f1_BMA_pro = compute_f1_score(tp = 53,fp = 6,fn = 4)\n",
        "f1_BMA_pro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9137931034482759"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dO1lB3eZYIr",
        "outputId": "123e324b-83ab-421a-a4e9-b9b8d2b868fd"
      },
      "source": [
        "f1_pro = compute_f1_score(40,4,3)\n",
        "f1_pro"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9195402298850575"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEHJGrwYgMHr",
        "outputId": "e3f1dee4-7f21-4733-b4f6-ad4b257418d6"
      },
      "source": [
        "f1_anti = compute_f1_score(41,13,12)\n",
        "f1_anti"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7663551401869159"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}